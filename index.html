<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>PCIL-MVP</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
</head>
<body>
  <!-- 主视觉区 -->
  <header class="container my-5 text-center">
    <h1>PCIL-MVP: Perturbation Contrastive Imitation Learning with Multimodal Visual Perception for Active Visual Search</h1>
    <p class="lead">Yuhao Wang, Guohui Tian</p>
    <br><br>
    <img src="images/Figure2.png" alt="Architecture of AVS" class="img-fluid d-block mx-auto" style="max-width:900px;">
  </header>
  <!-- Abstract -->
<section id="abstract" class="container my-5">
  <h2 class="text-center">Abstract</h2>
  <div class="mt-3" style="max-width:900px; margin:auto; font-size:1.1em;">
    
Active visual search (AVS) presents substantial challenges for autonomous agents operating in novel environments, particularly when limited to egocentric RGB-D observations. To address the complexities of target recognition, semantic understanding, and localization amid occlusions and uncertain spatial layouts, we introduce a multi-modal feature fusion framework based on ground projection. Our approach unifies RGB, semantic segmentation, and object detection features into an allocentric grid map, employing CLIP-based vision-language models and cross-attention mechanisms to achieve robust semantic alignment. Localization reliability is enhanced by a hybrid confidence estimation strategy that integrates statistical analysis with learned prediction, facilitating uncertainty-aware navigation. Temporal dependencies are captured using a Transformer-based multi-frame fusion module, enabling resilient and consistent map updates in dynamic scenarios. Additionally, we propose a contrastive imitation learning paradigm featuring adaptive sampling and perturbation trajectory augmentation to improve policy robustness and generalization. Comprehensive experiments demonstrate that our framework significantly outperforms existing baselines on visual localization and search tasks. 
  </div>
</section>

  
   
  <!-- PCIL-MVP -->
  <section id="intro" class="container my-5">
    <h2 class="text-center">Active Environment Perception</h2>
    <img src="images/Figure3.png" alt="Architecture of AEP" class="img-fluid d-block mx-auto" style="max-width:900px;">
  </section>
  <!-- 方法 -->
  <section id="method" class="container my-5">
    <h2 class="text-center">Active Object Search</h2>
    <img src="images/Figure5.png" alt="Architecture of AOS" class="img-fluid d-block mx-auto" style="max-width:500px;">
  </section>
  <!-- Environments in AVDB -->
  <section id="environments" class="container my-5">
    <h2 class="text-center">Environments in AVDB</h2>
    <!-- 图片行 -->
    <div class="row mb-3 justify-content-center">
      <div class="col-2 text-center">
        <img src="Environments/Home_001_2.png" alt="Env 1" class="img-fluid" style="max-width:200px;">
        <div>Home_001_2</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_004_2.png" alt="Env 2" class="img-fluid" style="max-width:200px;">
        <div>Home_004_2</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_005_1.png" alt="Env 3" class="img-fluid" style="max-width:200px;">
        <div>Home_005_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_008_1.png" alt="Env 4" class="img-fluid" style="max-width:200px;">
        <div>Home_008_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_013_1.png" alt="Env 5" class="img-fluid" style="max-width:200px;">
        <div>Home_013_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_015_1.png" alt="Env 6" class="img-fluid" style="max-width:200px;">
        <div>Home_015_1</div>
      </div>
    </div>
    <!-- 视频行 -->
    <div class="row justify-content-center">
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_001_2.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_001_2</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_004_2.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_004_2</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_005_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_005_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_008_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_008_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_013_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_013_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_015_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_015_1</div>
      </div>
    </div>
  </section>
  <!-- Examples -->
  <section id="results" class="container my-5">
    <h2 class="text-center">Examples</h2>
    <div class="row justify-content-center">
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_008_1_12.gif" alt="Example1" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_011_1_6.gif" alt="Example2" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_013_1_6.gif" alt="Example3" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_014_2_0.gif" alt="Example4" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_015_1_5.gif" alt="Example5" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_016_1_27.gif" alt="Example6" class="img-fluid" style="max-width:300px;">
      </div>
    </div>
  </section>
<!-- Code -->
<section id="code" class="container my-5">
  <h2 class="text-center">Code</h2>
  <div class="mt-3 text-center" style="font-size:1.1em;">
	<p>1. Please follow the instructions in <a href="https://github.com/ggeorgak11/mapping_navigation" target="_blank">https://github.com/ggeorgak11/mapping_navigation</a> to set up the environment and download the required data files.</p>

    <p>2. Model files and training scripts are provided. To train the AEP module, execute train_AEP.py directly. For the AOS module, use train_AOS.py to start training and test_AOS.py to perform testing.</p>

	  
	<p>3. Due to the large volume and complexity of the source code files, the complete codebase will be made available after the manuscript is accepted. We will ensure all necessary scripts and documentation are provided for reproducibility and ease of use.</p>
  </div>
  <div class="row mt-4 justify-content-center">
    <!-- 示例：五个代码文件 -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">AEP.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
MapNet with AEP.
Implements visual SLAM with CLIP-based object attention, temporal fusion, and uncertainty quantification.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import os
import numpy as np
import data_helper as dh
import clip


class CLIPFeatureExtractor(nn.Module):
    """
    CLIP-based feature extractor for object-aware visual understanding.
    
    Args:
        clip_model: CLIP model variant
        device: Target device
    
    Returns:
        Object-aware visual features with relevance masks
    """
    def __init__(self, clip_model="ViT-B/32", device="cuda"):
        super().__init__()

        # Handle device parameter
        if isinstance(device, int):
            self.device = f"cuda:{device}"
        elif isinstance(device, str):
            if device.isdigit():
                self.device = f"cuda:{device}"
            else:
                self.device = device
        else:
            self.device = device

        # Load CLIP model
        self.model, self.preprocess = clip.load(clip_model, device=self.device)

        # Freeze CLIP parameters
        for param in self.model.parameters():
            param.requires_grad = False

        # Predefined object categories
        self.object_categories = ['dining_table', 'fridge', 'tv', 'couch', 'microwave']

        # Precompute text features
        self.text_features = self._encode_text_features()
        self.similarity_threshold = 0.3

    def _encode_text_features(self):
        """Precompute text features for all categories."""
        text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in self.object_categories]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_inputs)
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def preprocess_images(self, images):
        """Preprocess images for CLIP input requirements."""
        images = images.to(self.device)

        # Check input dimensions
        if len(images.shape) == 5:
            B, T, C, H, W = images.shape
            images = images.view(B * T, C, H, W)
        elif len(images.shape) == 4:
            B, C, H, W = images.shape
            T = 1
        else:
            raise ValueError(f"Unexpected image shape: {images.shape}")

        # Resize to 224x224
        if H != 224 or W != 224:
            images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)

        # Ensure value range [0,1]
        if images.max() > 1.0:
            images = images / 255.0

        # CLIP normalization
        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=self.device).view(1, 3, 1, 1)
        std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=self.device).view(1, 3, 1, 1)
        images_normalized = (images - mean) / std

        return images_normalized

    def encode_images(self, images):
        """Encode image features using CLIP."""
        model_device = next(self.model.parameters()).device
        images = images.to(model_device)

        # Save original shape information
        original_shape = images.shape
        if len(original_shape) == 5:
            B, T = original_shape[:2]
            images = images.view(B * T, *original_shape[2:])
        elif len(original_shape) == 4:
            B, T = original_shape[0], 1
        else:
            raise ValueError(f"Unexpected image shape: {original_shape}")

        # Preprocess images
        images_processed = self.preprocess_images(images)

        # Ensure data type correctness
        expected_dtype = getattr(self.model, 'dtype', torch.float32)
        images_processed = images_processed.to(dtype=expected_dtype)

        try:
            with torch.no_grad():
                image_features = self.model.encode_image(images_processed)
        except RuntimeError as e:
            print(f"Error during CLIP encoding: {e}")
            raise e

        # Normalize features
        image_features = image_features / image_features.norm(dim=-1, keepdim=True).clamp(min=1e-8)

        # Restore original batch and time dimensions
        if len(original_shape) == 5:
            feature_dim = image_features.shape[-1]
            image_features = image_features.view(B, T, feature_dim)

        return image_features

    def get_relevant_objects(self, images):
        """Extract relevant object features and masks."""
        images = images.to(self.device)
        image_features = self.encode_images(images)

        # Compute similarities
        if len(image_features.shape) == 3:
            B, T, feature_dim = image_features.shape
            image_features_flat = image_features.view(B * T, feature_dim)
        else:
            B, feature_dim = image_features.shape
            T = 1
            image_features_flat = image_features

        similarities = 100.0 * image_features_flat @ self.text_features.T

        if T > 1:
            similarities = similarities.view(B, T, -1)

        relevance_mask = similarities > self.similarity_threshold

        return {
            'image_features': image_features,
            'text_features': self.text_features,
            'similarities': similarities,
            'relevance_mask': relevance_mask,
            'categories': self.object_categories
        }


class ObjectAttentionLayer(nn.Module):
    """
    Object-aware attention mechanism for map feature enhancement.
    
    Args:
        d_model: Feature dimension
        nhead: Number of attention heads
        dropout: Dropout rate
    """
    def __init__(self, d_model, nhead=4, dropout=0.1):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        # FFN
        self.linear1 = nn.Linear(d_model, d_model * 4)
        self.dropout1 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_model * 4, d_model)
        self.dropout2 = nn.Dropout(dropout)
        self.norm2 = nn.LayerNorm(d_model)
        self.activation = F.relu

    def forward_ffn(self, tgt):
        """Apply feed-forward network."""
        tgt2 = self.linear2(self.dropout1(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        return tgt

    def forward(self, map_features, object_features, relevance_mask=None):
        """
        Apply object attention to enhance map features.
        
        Args:
            map_features: [B, H*W, d_model] - query
            object_features: [B, num_obj, d_model] - key/value
            relevance_mask: [B, num_obj] - object relevance mask
        """
        B, H_W, d_model = map_features.shape
        _, num_obj, _ = object_features.shape

        # Adjust dimensions for nn.MultiheadAttention
        q = map_features.permute(1, 0, 2)
        k = object_features.permute(1, 0, 2)
        v = object_features.permute(1, 0, 2)

        # Prepare attention mask
        key_padding_mask = None
        if relevance_mask is not None:
            key_padding_mask = ~relevance_mask

        # Apply attention
        map_features2, _ = self.multihead_attn(
            query=q,
            key=k,
            value=v,
            key_padding_mask=key_padding_mask
        )

        # Restore dimension order
        map_features2 = map_features2.permute(1, 0, 2)
        map_features = map_features + self.dropout(map_features2)
        map_features = self.norm(map_features)

        # Apply feed-forward network
        map_features = self.forward_ffn(map_features)

        return map_features


class TemporalTransformer(nn.Module):
    """
    Temporal transformer for multi-frame feature fusion.
    
    Args:
        d_model: Feature dimension
        num_heads: Number of attention heads
        num_layers: Number of transformer layers
        window_size: Temporal window size
    """
    def __init__(self, d_model, num_heads, num_layers, window_size):
        super(TemporalTransformer, self).__init__()
        self.d_model = d_model
        self.window_size = window_size

        # Position encoding
        self.pos_encoding = nn.Parameter(torch.randn(window_size, d_model))

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=4 * d_model,
            dropout=0.1
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.output_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        """
        Apply temporal fusion to multi-frame features.
        
        Args:
            x: [B, H*W, window_size, d_model]
        
        Returns:
            Temporally fused features [B, H*W, d_model]
        """
        batch_size, spatial_points, seq_len, d_model = x.shape

        x = x.view(batch_size * spatial_points, seq_len, d_model)
        x = x + self.pos_encoding[:seq_len].unsqueeze(0)

        # Convert to seq_first format
        x = x.transpose(0, 1)

        output = self.transformer_encoder(x)

        # Take last timestep output
        fused_output = output[-1, :, :]

        fused_output = self.output_proj(fused_output)
        fused_output = fused_output.view(batch_size, spatial_points, d_model)

        return fused_output


class MapNet_AEP(nn.Module):
    """
    MapNet with transformer-based temporal fusion, uncertainty estimation, and CLIP attention.
    
    Args:
        par: Parameters object
        update_type: Map update mechanism
        input_flags: Input modality flags
    """
    def __init__(self, par, update_type, input_flags):
        super(MapNet_AEP, self).__init__()
        (with_feat, with_sseg, with_dets, use_raw_sseg, use_raw_dets) = input_flags

        # Basic parameter setup
        self.crop_size = par.crop_size
        self.global_map_dim = par.global_map_dim
        self.observation_dim = par.observation_dim
        self.cell_size = par.cell_size
        self.sseg_labels = par.sseg_labels
        self.dets_nClasses = par.dets_nClasses
        self.orientations = par.orientations
        self.pad = par.pad
        self.loss_type = par.loss_type
        self.update_type = par.update_type
        self.window_size = getattr(par, 'window_size', 4)
        self.use_temporal_fusion = getattr(par, 'use_temporal_fusion', True)

        # Dynamic map embedding calculation
        self.img_embedding = 64
        self.sseg_embedding = getattr(par, 'sseg_embedding', 16)
        self.dets_embedding = getattr(par, 'dets_embedding', 16)

        actual_embedding = 0
        if with_feat:
            actual_embedding += self.img_embedding
        if with_sseg:
            actual_embedding += self.sseg_embedding
        if with_dets:
            actual_embedding += self.dets_embedding

        self.map_embedding = actual_embedding
        print(f"Dynamic map_embedding: {self.map_embedding}")

        if hasattr(par, 'map_embedding') and par.map_embedding != actual_embedding:
            print(
                f"Warning: Parameter map_embedding ({par.map_embedding}) differs from computed value ({actual_embedding}), using computed value")
        
        par.map_embedding = actual_embedding

        # Initialize CLIP components
        self.clip_extractor = CLIPFeatureExtractor(device="cuda")
        self.object_attn = ObjectAttentionLayer(d_model=self.img_embedding, nhead=4)
        self.clip_proj = nn.Linear(512, self.img_embedding)
        self.current_rgb_images = None

        # Confidence branch
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(self.map_embedding, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, self.orientations, kernel_size=1),
            nn.Sigmoid()
        )

        # Temporal transformer
        if self.use_temporal_fusion and update_type == "lstm":
            self.temporal_transformer = TemporalTransformer(
                d_model=self.map_embedding,
                num_heads=4,
                num_layers=2,
                window_size=self.window_size
            )

        # Frame buffer initialization
        self.frame_buffer = None
        self.buffer_initialized = False

        # Feature extraction networks
        if with_feat:
            self.resnet_feat_dim = 256
            fnet = models.resnet50(pretrained=True)
            self.ResNet50Truncated = nn.Sequential(*list(fnet.children())[:-5])

            self.small_cnn_img = nn.Sequential(
                nn.Conv2d(in_channels=self.resnet_feat_dim, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.img_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        if with_sseg and not (use_raw_sseg):
            self.small_cnn_sseg = nn.Sequential(
                nn.Conv2d(in_channels=self.sseg_labels, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.sseg_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        if with_dets and not (use_raw_dets):
            self.small_cnn_det = nn.Sequential(
                nn.Conv2d(in_channels=self.dets_nClasses, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.dets_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        # Map update networks
        if update_type == "lstm":
            self.lstm = nn.LSTM(input_size=self.map_embedding, hidden_size=self.map_embedding, num_layers=1)
        elif update_type == "fc":
            self.update_fc = nn.Linear(self.map_embedding * 2, self.map_embedding)
        else:
            self.update_avg = nn.AvgPool1d(kernel_size=2)

        # Loss function
        if self.loss_type != "BCE":
            self.loss_CEL = nn.CrossEntropyLoss(ignore_index=-1)

    def init_frame_buffer(self, batch_size):
        """Initialize historical frame buffer."""
        self.frame_buffer = torch.zeros(
            batch_size,
            self.map_embedding,
            self.global_map_dim[0],
            self.global_map_dim[1],
            self.window_size,
            dtype=torch.float32
        ).cuda()
        self.buffer_initialized = True

    def update_frame_buffer(self, new_observation, batch_size):
        """Update sliding window buffer."""
        if not self.buffer_initialized:
            self.init_frame_buffer(batch_size)

        self.frame_buffer[:, :, :, :, :-1] = self.frame_buffer[:, :, :, :, 1:].clone()
        self.frame_buffer[:, :, :, :, -1] = new_observation

    def get_temporal_features(self, current_frame_idx):
        """Get features for temporal fusion."""
        if current_frame_idx < self.window_size - 1:
            valid_frames = current_frame_idx + 1
            return self.frame_buffer[:, :, :, :, :valid_frames]
        else:
            return self.frame_buffer

    def apply_temporal_fusion(self, features, current_frame_idx):
        """Apply temporal transformer fusion."""
        batch_size, d_model, h, w, valid_frames = features.shape

        if current_frame_idx < self.window_size - 1:
            return features[:, :, :, :, -1]

        # Reorganize data for transformer processing
        temporal_features = features.permute(0, 2, 3, 4, 1).contiguous()
        temporal_features = temporal_features.view(batch_size, h * w, valid_frames, d_model)

        fused_features = self.temporal_transformer(temporal_features)

        fused_features = fused_features.view(batch_size, h, w, d_model)
        fused_features = fused_features.permute(0, 3, 1, 2).contiguous()

        return fused_features

    def extract_img_feat(self, img_data, batch_size):
        """Extract image features with RGB preservation for CLIP."""
        self.current_rgb_images = img_data.clone()

        img_feat = self.ResNet50Truncated(img_data)
        img_feat = F.interpolate(img_feat, size=(self.crop_size[1], self.crop_size[0]), mode='nearest')

        return img_feat

    def init_p(self, batch_size):
        """Initialize agent position distribution on map."""
        p0 = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p0[:, 0, int(self.global_map_dim[0] / 2.0), int(self.global_map_dim[1] / 2.0)] = 1
        return torch.tensor(p0, dtype=torch.float32).cuda()

    def forward(self, local_info, update_type, input_flags, p_gt=None):
        """
        Forward pass for full episode MapNet processing.
        
        Args:
            local_info: Tuple of input modalities
            update_type: Map update mechanism
            input_flags: Input modality flags
            p_gt: Ground truth poses (optional)
        
        Returns:
            p_pred: Predicted position distributions
            map_pred: Predicted map features
            confidence_pred: Prediction confidence
        """
        (img_data, points2D, local3D, sseg, dets) = local_info
        batch_size = img_data.shape[0]
        seq_len = img_data.shape[1]

        # Extract image features
        with_feat = input_flags[0]
        if with_feat:
            original_img_data = img_data.clone()
            img_data_flat = img_data.view(batch_size * seq_len, 3, self.crop_size[1], self.crop_size[0])
            img_feat = self.extract_img_feat(img_data_flat, batch_size)
            img_feat = img_feat.view(batch_size, seq_len, self.resnet_feat_dim, self.crop_size[1], self.crop_size[0])
        else:
            original_img_data = None
            img_feat = torch.zeros(batch_size, seq_len, 1, self.crop_size[1], self.crop_size[0])

        # Ground projection
        grid, map_occ = self.groundProjection(
            img_feat_all=img_feat,
            points2D_all=points2D,
            local3D_all=local3D,
            sseg_all=sseg,
            dets_all=dets,
            batch_size=batch_size,
            seq_len=seq_len,
            input_flags=input_flags,
            original_img_data=original_img_data
        )

        # Rotate grid features to get rotation stack
        grid_packed = grid.view(batch_size * seq_len, self.map_embedding, self.observation_dim[0], self.observation_dim[1])
        rotation_stack_packed = self.rotational_sampler(grid=grid_packed)
        rotation_stack = rotation_stack_packed.view(batch_size, seq_len, self.map_embedding, self.orientations, self.observation_dim[0], self.observation_dim[1])

        # Initialize prediction tensors
        p_pred = np.zeros((batch_size, seq_len, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p_pred = torch.tensor(p_pred, dtype=torch.float32).cuda()
        map_pred = np.zeros((batch_size, seq_len, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        map_pred = torch.tensor(map_pred, dtype=torch.float32).cuda()

        confidence_pred = np.zeros((batch_size, seq_len, self.orientations,
                                    self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        confidence_pred = torch.tensor(confidence_pred, dtype=torch.float32).cuda()

        self.buffer_initialized = False
        for q in range(seq_len):
            rotation_stack_step = rotation_stack[:, q, :, :, :, :]
            if q == 0:
                p_ = self.init_p(batch_size=batch_size).clone()
                map_next = self.register_observation(rotation_stack=rotation_stack_step, p=p_, batch_size=batch_size)
                confidence_ = self.confidence_branch(map_next)
            else:
                map_previous = map_next.clone()
                p_, base_confidence = self.position_prediction(rotation_stack=rotation_stack_step,
                                                               map_previous=map_previous, batch_size=batch_size)

                if p_gt is not None:
                    reg_obsv = self.register_observation(rotation_stack=rotation_stack_step, p=p_gt[:, q, :, :, :], batch_size=batch_size)
                else:
                    reg_obsv = self.register_observation(rotation_stack=rotation_stack_step, p=p_, batch_size=batch_size)

                map_next = self.update_map(reg_obsv, map_previous, batch_size=batch_size,
                                           update_type=update_type, current_frame_idx=q)

                network_confidence = self.confidence_branch(map_next)
                confidence_ = 0.7 * network_confidence + 0.3 * base_confidence

            p_pred[:, q, :, :, :] = p_
            confidence_pred[:, q, :, :, :] = confidence_
            map_pred[:, q, :, :, :] = map_next

        return p_pred, map_pred, confidence_pred

    def build_loss(self, p_pred, confidence_pred, p_gt):
        """
        Build uncertainty-aware loss function.
        
        Args:
            p_pred: Predicted positions
            confidence_pred: Prediction confidence
            p_gt: Ground truth positions
        
        Returns:
            Weighted loss with confidence regularization
        """
        batch_size = p_pred.shape[0]
        seq_len = p_pred.shape[1]

        # Remove first frame
        p_pred = p_pred[:, 1:, :, :, :]
        confidence_pred = confidence_pred[:, 1:, :, :, :]
        p_gt = p_gt[:, 1:, :, :, :]

        if self.loss_type == "BCE":
            # Uncertainty-aware BCE loss
            p_pred_flat = p_pred.contiguous().view(batch_size * (seq_len - 1),
                                                   self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            p_gt_flat = p_gt.contiguous().view(batch_size * (seq_len - 1),
                                               self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            confidence_flat = confidence_pred.contiguous().view(batch_size * (seq_len - 1),
                                                                self.orientations, self.global_map_dim[0], self.global_map_dim[1])

            localization_loss = F.binary_cross_entropy_with_logits(p_pred_flat, p_gt_flat, reduction='none')
            weighted_loss = localization_loss / (confidence_flat + 1e-6)

            confidence_reg = torch.mean(confidence_flat)
            total_loss = torch.mean(weighted_loss) + 0.1 * confidence_reg

        elif self.loss_type == "CEL":
            # Cross-entropy loss with uncertainty awareness
            p_pred_flat = p_pred.contiguous().view(batch_size * (seq_len - 1), -1)
            p_gt_flat = p_gt.contiguous().view(batch_size * (seq_len - 1), -1)
            confidence_flat = confidence_pred.contiguous().view(batch_size * (seq_len - 1),
                                                                self.orientations, self.global_map_dim[0], self.global_map_dim[1])

            p_gt_indices = torch.argmax(p_gt_flat, dim=1)
            ce_loss = F.cross_entropy(p_pred_flat, p_gt_indices, reduction='none')

            confidence_weight = torch.mean(confidence_flat.view(batch_size * (seq_len - 1), -1), dim=1)
            weighted_loss = ce_loss / (confidence_weight + 1e-6)

            confidence_reg = torch.mean(confidence_weight)
            total_loss = torch.mean(weighted_loss) + 0.1 * confidence_reg

        else:
            raise ValueError(f"Unsupported loss type: {self.loss_type}")

        return total_loss

    def register_observation(self, rotation_stack, p, batch_size, confidence=None):
        """Register observation with optional confidence weighting."""
        reg_obsv = np.zeros((batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        reg_obsv = torch.tensor(reg_obsv, dtype=torch.float32).cuda()

        for i in range(batch_size):
            filt = rotation_stack[i, :, :, :, :]
            filt = filt.permute(1, 0, 2, 3)
            p_in = p[i, :, :, :].unsqueeze(0)

            if confidence is not None:
                confidence_weight = confidence[i, :, :, :].unsqueeze(0)
                p_in = p_in * confidence_weight

            reg = F.conv_transpose2d(input=p_in, weight=filt, padding=self.pad)
            reg_obsv[i, :, :, :] = reg

        return reg_obsv

    def position_prediction(self, rotation_stack, map_previous, batch_size):
        """Predict position via cross-correlation with confidence estimation."""
        corr_map = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        corr_map = torch.tensor(corr_map, dtype=torch.float32).cuda()

        for b in range(batch_size):
            map_ = map_previous[b, :, :, :].unsqueeze(0)
            for r in range(self.orientations):
                filt = rotation_stack[b, :, r, :, :].unsqueeze(0)
                corr_tmp = F.conv2d(input=map_, weight=filt, padding=self.pad)
                corr_map[b, r, :, :] = corr_tmp

        p_ = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p_ = torch.tensor(p_, dtype=torch.float32).cuda()

        for i in range(batch_size):
            p_tmp = corr_map[i, :, :, :].view(-1)
            p_tmp = p_tmp.view(self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            p_[i, :, :, :] = p_tmp

        # Confidence estimation
        confidence_map = torch.zeros_like(p_)
        for i in range(batch_size):
            for r in range(self.orientations):
                corr_values = corr_map[i, r, :, :]
                max_response = torch.max(corr_values)
                mean_response = torch.mean(corr_values)
                std_response = torch.std(corr_values)

                confidence = (max_response - mean_response) / (std_response + 1e-6)
                confidence_map[i, r, :, :] = torch.sigmoid(confidence)

        return p_, confidence_map

    def forward_single_step(self, local_info, t, input_flags, map_previous=None, p_given=None, update_type="lstm"):
        """Single-step forward pass for online inference."""
        (img_data, points2D, local3D, sseg, dets) = local_info
        batch_size = img_data.shape[0]

        with_feat = input_flags[0]
        if with_feat:
            self.current_rgb_images = img_data.clone()
            img_feat = self.extract_img_feat(img_data, batch_size)
        else:
            self.current_rgb_images = None
            img_feat = torch.zeros(batch_size, 1, self.crop_size[1], self.crop_size[0])

        # Ground projection processing
        grid = torch.zeros((batch_size, self.map_embedding, self.observation_dim[0], self.observation_dim[1]),
                           dtype=torch.float32).cuda()

        for b in range(batch_size):
            points2D_step = points2D[b]
            local3D_step = local3D[b]
            img_feat_step = img_feat[b, :, :, :].unsqueeze(0)
            sseg_step = sseg[b, :, :, :].unsqueeze(0)
            dets_step = dets[b, :, :, :].unsqueeze(0)

            rgb_image = None
            if self.current_rgb_images is not None:
                rgb_image = self.current_rgb_images[b]

            grid_step, map_occ_step = self.groundProjectionStep(
                img_feat=img_feat_step,
                points2D=points2D_step,
                local3D=local3D_step,
                sseg=sseg_step,
                dets=dets_step,
                input_flags=input_flags,
                rgb_image=rgb_image
            )
            grid[b, :, :, :] = grid_step.squeeze(0)

        rotation_stack = self.rotational_sampler(grid=grid)

        if t == 0:
            self.buffer_initialized = False
            p_ = self.init_p(batch_size=batch_size).clone()
            confidence_ = torch.ones_like(p_) * 0.5
            map_next = self.register_observation(rotation_stack=rotation_stack, p=p_,
                                                 batch_size=batch_size, confidence=confidence_)
        else:
            if p_given is None:
                p_, confidence_ = self.position_prediction(rotation_stack=rotation_stack,
                                                           map_previous=map_previous, batch_size=batch_size)
            else:
                p_ = p_given
                confidence_ = torch.ones_like(p_) * 0.8

            reg_obsv = self.register_observation(rotation_stack=rotation_stack, p=p_,
                                                 batch_size=batch_size, confidence=confidence_)

            map_next = self.update_map(reg_obsv, map_previous, batch_size=batch_size,
                                       update_type=update_type, current_frame_idx=t)

        return p_, confidence_, map_next

    def update_map(self, reg_obsv, map_previous, batch_size, update_type, current_frame_idx=0):
        """Update map with multi-frame temporal fusion support."""
        if update_type == "lstm" and self.use_temporal_fusion:
            self.update_frame_buffer(reg_obsv, batch_size)
            temporal_features = self.get_temporal_features(current_frame_idx)

            if current_frame_idx >= self.window_size - 1:
                fused_reg_obsv = self.apply_temporal_fusion(temporal_features, current_frame_idx)
            else:
                fused_reg_obsv = reg_obsv

            map_next = torch.zeros(
                (batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]),
                dtype=torch.float32
            ).cuda()

            # LSTM update for each spatial location
            for i in range(self.global_map_dim[0]):
                for j in range(self.global_map_dim[1]):
                    emb_in = fused_reg_obsv[:, :, i, j]
                    emb_hidden = map_previous[:, :, i, j]
                    emb_in = emb_in.unsqueeze(0)
                    emb_hidden = emb_hidden.unsqueeze(0)
                    hidden = (emb_hidden.contiguous(), emb_hidden.contiguous())
                    lstm_out, hidden_out = self.lstm(emb_in, hidden)
                    map_next[:, :, i, j] = lstm_out.squeeze(0)

        elif update_type == "fc":
            map2 = torch.cat((map_previous, reg_obsv), 1)
            map_next = self.update_fc(map2.permute(0, 2, 3, 1))
            map_next = torch.tanh(map_next)
            map_next = map_next.permute(0, 3, 1, 2)

        else:
            map_next = torch.zeros(
                (batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]),
                dtype=torch.float32
            ).cuda()
            for i in range(self.global_map_dim[0]):
                for j in range(self.global_map_dim[1]):
                    vec1 = reg_obsv[:, :, i, j].unsqueeze(2)
                    vec2 = map_previous[:, :, i, j].unsqueeze(2)
                    vec = torch.cat((vec1, vec2), 2)
                    avg_out = self.update_avg(vec).squeeze(2)
                    map_next[:, :, i, j] = avg_out
            map_next = torch.tanh(map_next)

        return map_next

    def rotational_sampler(self, grid, rot_init=True):
        """Generate rotational stack for different orientations."""
        if rot_init:
            grid = self.do_rotation(grid, angle=-np.pi/2.0)
        
        rotation_stack = np.zeros((grid.shape[0], self.map_embedding, self.orientations, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32 )
        rotation_stack = torch.tensor(rotation_stack, dtype=torch.float32).cuda()
        for i in range(self.orientations):
            angle = 2*np.pi*(i/self.orientations)
            rotation_stack[:,:,i,:,:] = self.do_rotation(grid, angle)
        return rotation_stack

    def do_rotation(self, grid, angle):
        """Apply rotation transformation to grid."""
        theta = torch.tensor([
            [np.cos(angle), -np.sin(angle), 0],
            [np.sin(angle), np.cos(angle), 0]
        ], dtype=torch.float32).cuda()
        theta = theta.unsqueeze(0).repeat(grid.shape[0], 1, 1)
        grid_affine = F.affine_grid(theta, grid.size(), align_corners=True)
        return F.grid_sample(grid, grid_affine, align_corners=True)

    def groundProjectionStep(self, img_feat, points2D, local3D, sseg, dets, input_flags, rgb_image=None):
        """Ground projection with CLIP-enhanced features."""
        (with_feat, with_sseg, with_dets, use_raw_sseg, use_raw_dets) = input_flags

        # Basic ground projection
        map_coords, valid, map_occ = dh.discretize_coords(
            x=local3D[:, 0], z=local3D[:, 2], map_dim=self.observation_dim, cell_size=self.cell_size)
        map_occ = torch.tensor(map_occ, dtype=torch.float32).cuda()

        points2D = points2D[valid, :]
        map_coords = map_coords[valid, :]
        grids = []

        # CLIP feature interaction
        if with_feat and rgb_image is not None:
            try:
                # Convert ResNet features to map features
                grid_img = self.bin_pooling(img_feat, points2D, map_coords)
                grid_img_in = grid_img.unsqueeze(0)
                map_features = self.small_cnn_img(grid_img_in.cuda())

                # Get CLIP text features
                text_features = self.clip_extractor.text_features
                text_features = text_features.to(dtype=torch.float32)
                text_features_proj = self.clip_proj(text_features)

                # Get CLIP image features
                with torch.no_grad():
                    clip_result = self.clip_extractor.get_relevant_objects(rgb_image.unsqueeze(0))
                    clip_img_feat = clip_result['image_features']
                    relevance_mask = clip_result['relevance_mask']

                    # Handle dimensions
                    if len(clip_img_feat.shape) == 3:
                        clip_img_feat = clip_img_feat.squeeze(1)
                    if len(relevance_mask.shape) == 3:
                        relevance_mask = relevance_mask.squeeze(1)

                    clip_img_feat = clip_img_feat.to(dtype=torch.float32)
                    clip_img_feat_proj = self.clip_proj(clip_img_feat)

                # Process relevant categories
                relevant_indices = torch.where(relevance_mask[0])[0]

                if relevant_indices.numel() > 0:
                    relevant_text_features = text_features_proj[relevant_indices]
                    object_features = torch.cat([
                        relevant_text_features.unsqueeze(0),
                        clip_img_feat_proj.unsqueeze(1)
                    ], dim=1)

                    img_relevance = torch.ones(1, 1, device=relevance_mask.device, dtype=relevance_mask.dtype)
                    extended_relevance_mask = torch.cat([
                        relevance_mask[:, relevant_indices],
                        img_relevance
                    ], dim=1)
                else:
                    object_features = clip_img_feat_proj.unsqueeze(0).unsqueeze(1)
                    extended_relevance_mask = torch.ones(1, 1, device=clip_img_feat_proj.device, dtype=torch.bool)

                # Apply attention mechanism
                B, d_model, H, W = map_features.shape
                map_features_flat = map_features.permute(0, 2, 3, 1).reshape(B, H * W, d_model)

                enhanced_map_features = self.object_attn(
                    map_features_flat,
                    object_features,
                    relevance_mask=extended_relevance_mask
                )

                enhanced_map_features = enhanced_map_features.view(B, H, W, d_model).permute(0, 3, 1, 2)
                grids.append(enhanced_map_features)

            except Exception as e:
                print(f"CLIP processing error, using fallback features: {e}")
                grid_img = self.bin_pooling(img_feat, points2D, map_coords)
                grid_img_in = grid_img.unsqueeze(0)
                map_features = self.small_cnn_img(grid_img_in.cuda())
                grids.append(map_features)

        elif with_feat:
            grid_img = self.bin_pooling(img_feat, points2D, map_coords)
            grid_img_in = grid_img.unsqueeze(0)
            map_features = self.small_cnn_img(grid_img_in.cuda())
            grids.append(map_features)

        # Other feature processing
        if with_sseg:
            grid_sseg = self.label_pooling(sseg, points2D, map_coords)
            grid_sseg_in = grid_sseg.unsqueeze(0)
            if use_raw_sseg:
                grids.append(grid_sseg_in)
            else:
                sseg_processed = self.small_cnn_sseg(grid_sseg_in)
                grids.append(sseg_processed)

        if with_dets:
            grid_det = self.dets_pooling(dets, points2D, map_coords)
            grid_det_in = grid_det.unsqueeze(0)
            if use_raw_dets:
                grids.append(grid_det_in)
            else:
                det_processed = self.small_cnn_det(grid_det_in)
                grids.append(det_processed)

        if len(grids) == 0:
            raise Exception("No input grids!")

        grid_out = torch.cat(grids, 1).cuda()

        if grid_out.shape[1] != self.map_embedding:
            if not hasattr(self, 'feature_adapter'):
                self.feature_adapter = nn.Conv2d(grid_out.shape[1], self.map_embedding, 1).cuda()
            grid_out = self.feature_adapter(grid_out)

        return grid_out, map_occ

    def bin_pooling(self, img_feat, points2D, map_coords):
        """Bin pooling during ground projection of features."""
        grid = np.zeros((img_feat.shape[1], self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        grid = torch.tensor(grid, dtype=torch.float32).cuda()
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_feat = img_feat[0, :, pix_y, pix_x]
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]
            bin_feats = pix_feat[:, ind]
            bin_feat, _ = torch.max(bin_feats, 1)
            grid[:, ucoord[1], ucoord[0]] = bin_feat
        return grid

    def label_pooling(self, sseg, points2D, map_coords):
        """Semantic label pooling for each map grid location."""
        grid = np.zeros((self.sseg_labels, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_lbl = sseg[0, 0, pix_y, pix_x]
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]
            bin_lbls = pix_lbl[ind]
            bin_lbls = bin_lbls.cpu() if bin_lbls.is_cuda else bin_lbls
            hist, bins = np.histogram(bin_lbls.numpy(), bins=list(range(self.sseg_labels + 1)))
            hist = hist / float(bin_lbls.shape[0])
            grid[:, ucoord[1], ucoord[0]] = hist
        grid = torch.tensor(grid, dtype=torch.float32).cuda()
        return grid

    def dets_pooling(self, dets, points2D, map_coords):
        """Detection score pooling via averaging."""
        grid = np.zeros((self.dets_nClasses, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_dets = dets[0, :, pix_y, pix_x]
        if pix_dets.is_cuda:
            pix_dets = pix_dets.cpu()
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]
            bin_dets = pix_dets[:, ind]
            bin_det = bin_dets.mean(1)
            grid[:, ucoord[1], ucoord[0]] = bin_det.numpy()
        grid = torch.tensor(grid, dtype=torch.float32).cuda()

        return grid

    def groundProjection(self, img_feat_all, points2D_all, local3D_all, sseg_all, dets_all, batch_size, seq_len,
                         input_flags, original_img_data=None):
        """Batch ground projection with CLIP enhancement."""
        grid = torch.zeros((batch_size, seq_len, self.map_embedding, self.observation_dim[0], self.observation_dim[1]),
                           dtype=torch.float32).cuda()
        map_occ = torch.zeros((batch_size, seq_len, 1, self.observation_dim[0], self.observation_dim[1]),
                              dtype=torch.float32).cuda()

        for b in range(batch_size):
            points2D_seq = points2D_all[b]
            local3D_seq = local3D_all[b]
            for q in range(seq_len):
                points2D_step = points2D_seq[q]
                local3D_step = local3D_seq[q]
                img_feat_step = img_feat_all[b, q, :, :, :].unsqueeze(0)
                sseg_step = sseg_all[b, q, :, :, :].unsqueeze(0)
                dets_step = dets_all[b, q, :, :, :].unsqueeze(0)

                rgb_image = None
                if original_img_data is not None:
                    rgb_image = original_img_data[b, q]

                grid_step, map_occ_step = self.groundProjectionStep(
                    img_feat=img_feat_step,
                    points2D=points2D_step,
                    local3D=local3D_step,
                    sseg=sseg_step,
                    dets=dets_step,
                    input_flags=input_flags,
                    rgb_image=rgb_image
                )
                grid[b, q, :, :, :] = grid_step.squeeze(0)
                map_occ[b, q, :, :, :] = map_occ_step.squeeze(0)
        return grid, map_occ


</pre>
        </div>
      </div>
    </div>
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">train_AEP.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
Training script for MapNet_AEP.
Outputs predicted poses (p_pred) and maps (map_pred) on training data.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import os
import numpy as np
import random
import data_helper as dh
import helper as hl
from MapNet_AEP import MapNet_AEP
from parameters import ParametersMapNet
from dataloader import AVD
from test_MapNet import evaluate_MapNet


def get_minibatch(par, data, ex_ids, data_index):
    """
    Extract batch samples from dataset and format for model input.
    Args: par (parameters), data (dataset), ex_ids (example IDs), data_index (current index)
    Returns: tuple of batched tensors (imgs, points2D, local3D, pose_gt, sseg, dets)
    """
    imgs_batch = torch.zeros(par.batch_size, par.seq_len, 3, par.crop_size[1], par.crop_size[0])
    pose_gt_batch = np.zeros((par.batch_size, par.seq_len, 3), dtype=np.float32)
    sseg_batch = torch.zeros(par.batch_size, par.seq_len, 1, par.crop_size[1], par.crop_size[0])
    dets_batch = torch.zeros(par.batch_size, par.seq_len, par.dets_nClasses, par.crop_size[1], par.crop_size[0])
    points2D_batch, local3D_batch = [], []
    for k in range(par.batch_size):
        ex = data[ex_ids[data_index + k]]
        imgs_seq = ex["images"]
        points2D_seq = ex["points2D"]
        local3D_seq = ex["local3D"]
        pose_gt_seq = ex["pose"]
        sseg_seq = ex["sseg"]
        dets_seq = ex["dets"]
        imgs_batch[k, :, :, :, :] = imgs_seq
        pose_gt_batch[k, :, :] = pose_gt_seq
        sseg_batch[k, :, :, :, :] = sseg_seq
        dets_batch[k, :, :, :, :] = dets_seq
        points2D_batch.append(points2D_seq)
        local3D_batch.append(local3D_seq)
    return (imgs_batch.cuda(), points2D_batch, local3D_batch, pose_gt_batch, sseg_batch.cuda(), dets_batch.cuda())


if __name__ == '__main__':
    par = ParametersMapNet()
    mapNet_model = MapNet_AEP(par, update_type=par.update_type, input_flags=par.input_flags)
    mapNet_model.cuda()
    print("Model on GPU:", next(mapNet_model.parameters()).is_cuda)
    mapNet_model.train()
    optimizer = optim.Adam(mapNet_model.parameters(), lr=par.lr_rate)
    scheduler = StepLR(optimizer, step_size=par.step_size, gamma=par.gamma)

    print("Loading the training data...")
    avd = AVD(par, seq_len=par.seq_len, nEpisodes=par.epi_per_scene,
              scene_list=par.train_scene_list, action_list=par.action_list, with_shortest_path=par.with_shortest_path)

    log = open(par.model_dir + "train_log_" + par.model_id + ".txt", 'w')
    hl.save_params(par, par.model_dir, name="mapNet")
    loss_list = []

    all_ids = list(range(len(avd)))
    test_ids = all_ids[::100]
    train_ids = list(set(all_ids) - set(test_ids))

    nData = len(train_ids)
    iters_per_epoch = int(nData / float(par.batch_size))
    log.write("Iters_per_epoch:" + str(iters_per_epoch) + "\n")
    print("Iters per epoch:", iters_per_epoch)
    for ep in range(par.nEpochs):
        random.shuffle(train_ids)
        data_index = 0
        for i in range(iters_per_epoch):
            iters = i + ep * iters_per_epoch

            batch = get_minibatch(par, data=avd, ex_ids=train_ids, data_index=data_index)
            (imgs_batch, points2D_batch, local3D_batch, pose_gt_batch, sseg_batch, dets_batch) = batch

            p_gt_batch = dh.build_p_gt(par, pose_gt_batch)
            data_index += par.batch_size

            local_info = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch)
            p_pred, map_pred, confidence_pred = mapNet_model(local_info, update_type=par.update_type,
                                                             input_flags=par.input_flags, p_gt=None)

            loss = mapNet_model.build_loss(p_pred, confidence_pred, p_gt_batch)
            optimizer.zero_grad()
            loss.backward()
            total_norm = 0
            for p in mapNet_model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** (1. / 2)

            if iters % par.show_interval == 0:
                print(f"Gradient norm: {total_norm}")
            optimizer.step()

            if iters % par.show_interval == 0:
                log.write("Epoch:"+str(ep)+" ITER:"+str(iters)+" Loss:"+str(loss.data.item())+"\n")
                print("Epoch:", str(ep), " ITER:", str(iters), " Loss:", str(loss.data.item()))
                log.flush()

            if iters % par.show_interval == 0:
                current_lr = optimizer.param_groups[0]['lr']
                log.write(f"Epoch:{ep} ITER:{iters} Loss:{loss.data.item()} LR:{current_lr}\n")
                print(f"Epoch: {ep} ITER: {iters} Loss: {loss.data.item()} LR: {current_lr}")

            if iters > 0:
                loss_list.append(loss.data.item())
            if iters % par.plot_interval == 0 and iters > 0:
                hl.plot_loss(loss=loss_list, epoch=ep, iteration=iters, step=1, loss_name="NLL", loss_dir=par.model_dir)

            if iters % par.save_interval == 0:
                hl.save_model(model=mapNet_model, model_dir=par.model_dir, model_name="MapNet", train_iter=iters)

            if iters % par.test_interval == 0:
                evaluate_MapNet(par, test_iter=iters, test_ids=test_ids, test_data=avd)
        scheduler.step()



</pre>
        </div>
      </div>
    </div>
    <!-- 继续添加更多代码块，如下 -->
    <!-- code3.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
Imitation Learning Network for Navigation.
Contains ILNet for action prediction and Encoder for egocentric observation feature extraction.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import os
import numpy as np


class ILNet(nn.Module):
    """
    Navigation network for imitation learning.
    Input: mapnet state and egocentric observations
    Output: action costs
    """
    def __init__(self, par, map_in_embedding, map_orient, tvec_dim, nActions, use_ego_obsv, drop_rate=0.2):
        super(ILNet, self).__init__()
        self.map_in_embedding = map_in_embedding
        self.orientations = map_orient
        self.fc_dim = par.fc_dim
        
        # Map feature extraction CNN
        self.map_cnn = nn.Sequential(
            nn.Conv2d(in_channels=map_in_embedding, out_channels=par.conv_embedding, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(num_features=par.conv_embedding),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2)
        )
        self.fc_map = nn.Linear(13*13*8, par.fc_dim)
        self.relu_map = nn.ReLU(inplace=True)
        self.drop_map = nn.Dropout(p=drop_rate)
        
        # Position prediction CNN
        self.p_cnn = nn.Sequential(
            nn.Conv2d(in_channels=map_orient, out_channels=par.conv_embedding, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(num_features=par.conv_embedding),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc_p = nn.Linear(13*13*8, par.fc_dim)
        self.relu_p = nn.ReLU(inplace=True)
        self.drop_p = nn.Dropout(p=drop_rate)
        
        # Target vector embedding
        self.fc_t = nn.Linear(tvec_dim, par.fc_dim)
        self.relu_t = nn.ReLU(inplace=True)
        self.drop_t = nn.Dropout(p=drop_rate)
        state_items = 3
        
        # Egocentric observation processing
        if use_ego_obsv:
            self.fc_ego = nn.Linear(512, par.fc_dim)
            self.relu_ego = nn.ReLU(inplace=True)
            self.drop_ego = nn.Dropout(p=drop_rate)
            state_items = 4
            
        # LSTM for sequence processing
        self.lstm = nn.LSTM(input_size=par.fc_dim*state_items+1, hidden_size=par.fc_dim*state_items+1, num_layers=1)
        
        # Action prediction layer
        self.fc_action = nn.Linear(par.fc_dim*state_items+1, nActions)
        self.hidden = None
        self.cost_loss = nn.L1Loss()

    def init_hidden(self, batch_size, state_items):
        return (torch.zeros(1, batch_size, self.fc_dim*state_items+1).cuda(), 
                torch.zeros(1, batch_size, self.fc_dim*state_items+1).cuda())

    def build_loss(self, cost_pred, cost_gt, loss_weight):
        loss = loss_weight * self.cost_loss(cost_pred, cost_gt)
        return loss

    def forward(self, state, use_ego_obsv):
        if use_ego_obsv:
            (map_pred, p_pred, t_vec, collision, ego_obsv) = state
        else:
            (map_pred, p_pred, t_vec, collision) = state
        if len(map_pred.size()) == 4:
            map_pred = map_pred.unsqueeze(1)
            p_pred = p_pred.unsqueeze(1)

        batch_size = map_pred.shape[0]
        seq_len = map_pred.shape[1]
        global_map_dim = (map_pred.shape[3], map_pred.shape[4])
        
        # Dimension check and adaptation
        actual_map_embedding = map_pred.shape[2]
        if actual_map_embedding != self.map_in_embedding:
            print(f"Dimension mismatch warning: expected map_embedding={self.map_in_embedding}, actual={actual_map_embedding}")
            print(f"map_pred shape: {map_pred.shape}")
            if not hasattr(self, 'map_adapter'):
                print(f"Creating dimension adapter: {actual_map_embedding} -> {self.map_in_embedding}")
                self.map_adapter = nn.Conv2d(actual_map_embedding, self.map_in_embedding, 1).cuda()
            
            map_pred_reshaped = map_pred.view(batch_size*seq_len, actual_map_embedding, global_map_dim[0], global_map_dim[1])
            map_pred = self.map_adapter(map_pred_reshaped)
            map_pred = map_pred.view(batch_size, seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        
        # Map embedding processing
        map_pred = map_pred.view(batch_size*seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        map_cnn_out = self.map_cnn(map_pred)
        map_out = self.fc_map(map_cnn_out.view(map_cnn_out.shape[0], -1))
        map_out = self.relu_map(map_out)
        map_out = self.drop_map(map_out)
        map_out = map_out.view(batch_size, seq_len, -1)
        
        # Position embedding processing
        p_pred = p_pred.view(batch_size*seq_len, self.orientations, global_map_dim[0], global_map_dim[1])
        p_cnn_out = self.p_cnn(p_pred)
        p_out = self.fc_p(p_cnn_out.view(p_cnn_out.shape[0], -1))
        p_out = self.relu_p(p_out)
        p_out = self.drop_p(p_out)
        p_out = p_out.view(batch_size, seq_len, -1)
        
        # Target vector embedding
        t_out = self.fc_t(t_vec)
        t_out = self.relu_t(t_out)
        t_out = self.drop_t(t_out)
        t_out = t_out.unsqueeze(1).repeat(1, seq_len, 1)
        collision = collision.view(batch_size, seq_len, 1)
        
        if use_ego_obsv:
            ego_obsv = ego_obsv.view(batch_size*seq_len, -1)
            ego_out = self.fc_ego(ego_obsv)
            ego_out = self.relu_ego(ego_out)
            ego_out = self.drop_ego(ego_out)
            ego_out = ego_out.view(batch_size, seq_len, -1)
            x = torch.cat((map_out, p_out, t_out, collision, ego_out), 2)
        else:
            x = torch.cat((map_out, p_out, t_out, collision), 2)
            
        x = x.permute(1,0,2)
        lstm_out, self.hidden = self.lstm(x, self.hidden)
        lstm_out = lstm_out.permute(1,0,2)
        action_costs = self.fc_action(lstm_out)
        return action_costs
    
    def get_features(self, state, use_ego_obsv):
        """
        Extract state features (LSTM output) for contrastive learning.
        Returns: feature tensor instead of action costs
        """
        if use_ego_obsv:
            (map_pred, p_pred, t_vec, collision, ego_obsv) = state
        else:
            (map_pred, p_pred, t_vec, collision) = state
        if len(map_pred.size()) == 4:
            map_pred = map_pred.unsqueeze(1)
            p_pred = p_pred.unsqueeze(1)

        batch_size = map_pred.shape[0]
        seq_len = map_pred.shape[1]
        global_map_dim = (map_pred.shape[3], map_pred.shape[4])
        
        actual_map_embedding = map_pred.shape[2]
        if actual_map_embedding != self.map_in_embedding:
            if not hasattr(self, 'map_adapter'):
                self.map_adapter = nn.Conv2d(actual_map_embedding, self.map_in_embedding, 1).cuda()
            map_pred_reshaped = map_pred.view(batch_size*seq_len, actual_map_embedding, global_map_dim[0], global_map_dim[1])
            map_pred = self.map_adapter(map_pred_reshaped)
            map_pred = map_pred.view(batch_size, seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        
        map_pred = map_pred.view(batch_size*seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        map_cnn_out = self.map_cnn(map_pred)
        map_out = self.fc_map(map_cnn_out.view(map_cnn_out.shape[0], -1))
        map_out = self.relu_map(map_out)
        map_out = self.drop_map(map_out)
        map_out = map_out.view(batch_size, seq_len, -1)
        p_pred = p_pred.view(batch_size*seq_len, self.orientations, global_map_dim[0], global_map_dim[1])
        p_cnn_out = self.p_cnn(p_pred)
        p_out = self.fc_p(p_cnn_out.view(p_cnn_out.shape[0], -1))
        p_out = self.relu_p(p_out)
        p_out = self.drop_p(p_out)
        p_out = p_out.view(batch_size, seq_len, -1)
        t_out = self.fc_t(t_vec)
        t_out = self.relu_t(t_out)
        t_out = self.drop_t(t_out)
        t_out = t_out.unsqueeze(1).repeat(1, seq_len, 1)
        collision = collision.view(batch_size, seq_len, 1)
        if use_ego_obsv:
            ego_obsv = ego_obsv.view(batch_size*seq_len, -1)
            ego_out = self.fc_ego(ego_obsv)
            ego_out = self.relu_ego(ego_out)
            ego_out = self.drop_ego(ego_out)
            ego_out = ego_out.view(batch_size, seq_len, -1)
            x = torch.cat((map_out, p_out, t_out, collision, ego_out), 2)
        else:
            x = torch.cat((map_out, p_out, t_out, collision), 2)
        x = x.permute(1,0,2)
        lstm_out, self.hidden = self.lstm(x, self.hidden)
        lstm_out = lstm_out.permute(1,0,2)
        return lstm_out
        

class Encoder(nn.Module): 
    """
    Feature extractor for egocentric observations using modified ResNet-18.
    Input: 4-channel images
    Output: 512-dimensional feature vectors
    """
    def __init__(self):
        super(Encoder, self).__init__()
        fnet = models.resnet18(pretrained=True)
        fnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,bias=False)
        self.img_features = nn.Sequential(*list(fnet.children())[:-1])
        
    def forward(self, img_data):
        img_out = self.img_features(img_data)
        return img_out
</pre>
        </div>
      </div>
    </div>
    <!-- code4.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">train_AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
Navigation network training with perturbation contrast learning.
Combines behavioral cloning with contrastive learning using trajectory perturbations.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import os
import numpy as np
import math
import random
from dataloader_perturbation_contrast import AVD_IL_Perturbed_Contrast
from dataloader import AVD_online
from IL_Net_wo_IPH import ILNet, Encoder
from mapNet import MapNet
from parameters_wo_IPH import Parameters_IL, ParametersMapNet
import helper as hl
import data_helper as dh
from itertools import chain
from test_NavNet import evaluate_NavNet
import time

print("CUDA is available:", torch.cuda.is_available())
print("CUDA device count:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("Current CUDA device:", torch.cuda.current_device())
    print("CUDA device name:", torch.cuda.get_device_name(0))

class InfoNCELoss(nn.Module):
    """InfoNCE contrastive loss function."""
    def __init__(self, temperature=0.07):
        super(InfoNCELoss, self).__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss()
        
    def forward(self, anchor_features, positive_features, negative_features):
        """
        Compute InfoNCE loss.
        
        Args:
            anchor_features: [batch_size, feature_dim]
            positive_features: [batch_size, feature_dim]
            negative_features: [batch_size, neg_count, feature_dim]
        
        Returns:
            Contrastive loss value
        """
        batch_size = anchor_features.size(0)
        neg_count = negative_features.size(1)
        feature_dim = anchor_features.size(1)
        
        # Normalize feature vectors
        anchor_features = F.normalize(anchor_features, p=2, dim=1)
        positive_features = F.normalize(positive_features, p=2, dim=1)
        negative_features = F.normalize(negative_features.view(-1, feature_dim), p=2, dim=1).view(batch_size, neg_count, feature_dim)
        
        # Compute positive similarity [batch_size, 1]
        pos_similarity = torch.bmm(anchor_features.unsqueeze(1), positive_features.unsqueeze(2)).squeeze(-1)
        
        # Compute negative similarity [batch_size, neg_count]
        neg_similarity = torch.bmm(anchor_features.unsqueeze(1), 
                                  negative_features.transpose(1, 2)).squeeze(1)
        
        # Combine positive and negative similarities [batch_size, 1 + neg_count]
        logits = torch.cat([pos_similarity, neg_similarity], dim=1) / self.temperature
        
        # Labels: positive sample index is 0
        labels = torch.zeros(batch_size, dtype=torch.long).to(anchor_features.device)
        
        return self.criterion(logits, labels)

def get_minibatch_with_contrast(batch_size, tvec_dim, seq_len, nActions, data, ex_ids, data_index):
    """Get minibatch data with original and perturbed trajectories for contrastive learning."""
    batch_data = []
    
    for k in range(batch_size):
        batch_data.append(data[ex_ids[data_index + k]])
    
    # Process original trajectory data
    imgs_batch = torch.stack([item['original']['images'] for item in batch_data]).cuda()
    sseg_batch = torch.stack([item['original']['sseg'] for item in batch_data]).cuda()
    dets_batch = torch.stack([item['original']['dets'] for item in batch_data]).cuda()
    imgs_obsv_batch = torch.stack([item['original']['images_obsv'] for item in batch_data]).cuda()
    dets_obsv_batch = torch.stack([item['original']['dets_obsv'] for item in batch_data]).cuda()
    
    points2D_batch = [item['original']['points2D'] for item in batch_data]
    local3D_batch = [item['original']['local3D'] for item in batch_data]
    
    # Target labels
    tvec_batch = torch.zeros(batch_size, tvec_dim).float().cuda()
    for k in range(batch_size):
        tvec_batch[k, batch_data[k]['original']['target_lbl']] = 1
    
    # Collect other necessary data
    pose_gt_batch = np.array([item['original']['pose'] for item in batch_data])
    collisions_batch = torch.stack([item['original']['collisions'] for item in batch_data]).cuda()
    actions = [item['original']['actions'] for item in batch_data]
    costs_batch = torch.stack([item['original']['costs'] for item in batch_data]).cuda()
    image_names = [item['original']['images_names'] for item in batch_data]
    scenes = [item['original']['scene'] for item in batch_data]
    scales = [item['original']['scale'] for item in batch_data]
    
    # Process perturbed trajectory data
    perturbed_imgs_batch = torch.stack([item['perturbed']['images'] for item in batch_data]).cuda()
    perturbed_sseg_batch = torch.stack([item['perturbed']['sseg'] for item in batch_data]).cuda()
    perturbed_dets_batch = torch.stack([item['perturbed']['dets'] for item in batch_data]).cuda()
    perturbed_imgs_obsv_batch = torch.stack([item['perturbed']['images_obsv'] for item in batch_data]).cuda()
    perturbed_dets_obsv_batch = torch.stack([item['perturbed']['dets_obsv'] for item in batch_data]).cuda()
    
    perturbed_points2D_batch = [item['perturbed']['points2D'] for item in batch_data]
    perturbed_local3D_batch = [item['perturbed']['local3D'] for item in batch_data]
    
    perturbed_pose_gt_batch = np.array([item['perturbed']['pose'] for item in batch_data])
    perturbed_collisions_batch = torch.stack([item['perturbed']['collisions'] for item in batch_data]).cuda()
    perturbed_actions = [item['perturbed']['actions'] for item in batch_data]
    perturbed_costs_batch = torch.stack([item['perturbed']['costs'] for item in batch_data]).cuda()
    
    # Contrastive learning information
    contrast_info = {
        "perturb_points": [item['contrast_info']['perturb_point'] for item in batch_data],
        "is_positive": [item['contrast_info']['is_positive'] for item in batch_data],
        "similarity_scores": [item['contrast_info']['similarity_score'] for item in batch_data]
    }
    
    # Organize return format
    mapNet_batch = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch)
    IL_batch = (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, image_names, scenes, scales)
    
    perturbed_mapNet_batch = (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                             perturbed_sseg_batch, perturbed_dets_batch, perturbed_pose_gt_batch)
    perturbed_IL_batch = (perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch, tvec_batch, 
                         perturbed_collisions_batch, perturbed_actions, perturbed_costs_batch, 
                         image_names, scenes, scales)
    
    return mapNet_batch, IL_batch, perturbed_mapNet_batch, perturbed_IL_batch, contrast_info

def run_mapNet(parMapNet, mapNet, start_info, use_p_gt, pose_gt_batch):
    """Run MapNet forward pass."""
    if use_p_gt:
        p_gt_batch = dh.build_p_gt(parMapNet, pose_gt_batch)
        p_, map_, confidence_ = mapNet(local_info=start_info, update_type=parMapNet.update_type, 
                                    input_flags=parMapNet.input_flags, p_gt=p_gt_batch)
        p_ = p_gt_batch.clone() # overwrite the predicted with the ground-truth location
    else:
        p_, map_, confidence_ = mapNet(local_info=start_info, update_type=parMapNet.update_type, input_flags=parMapNet.input_flags)
    return p_, map_

def extract_state_features(parIL, state, policy_net):
    """Extract state features from policy network for contrastive learning."""
    policy_net.hidden = policy_net.init_hidden(batch_size=state[0].size(0), state_items=len(state)-1)
    
    with torch.no_grad():
        state_features = policy_net.get_features(state, parIL.use_ego_obsv)
        
    return state_features

def select_minibatch(par, iters_done):
    """Select between pre-sampled and online generated data."""
    sample = random.random()
    eps_threshold = par.EPS_END + (par.EPS_START-par.EPS_END) * math.exp(-1. * iters_done / par.EPS_DECAY)
    if sample > eps_threshold:
        return 0
    else:
        return 1

def unroll_policy(parIL, parMapNet, policy_net, mapNet, action_list, batch_size, seq_len, graphs):
    """Generate online trajectory data using policy rollout."""
    with torch.no_grad():
        nScenes = 4
        ind = np.random.randint(len(parIL.train_scene_list), size=nScenes)
        scene_list = np.asarray(parIL.train_scene_list)
        sel_scene = scene_list[ind]
        avd_dagger = AVD_online(par=parIL, nStartPos=batch_size/nScenes, 
                                        scene_list=sel_scene, action_list=action_list, graphs_dict=graphs)
        
        # Initialize all arrays to be returned
        imgs_batch = torch.zeros(batch_size, seq_len, 3, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        sseg_batch = torch.zeros(batch_size, seq_len, 1, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        dets_batch = torch.zeros(batch_size, seq_len, avd_dagger.dets_nClasses, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        imgs_obsv_batch = torch.zeros(batch_size, seq_len, 3, avd_dagger.cropSizeObsv[1], avd_dagger.cropSizeObsv[0]).float().cuda()
        dets_obsv_batch = torch.zeros(batch_size, seq_len, 1, avd_dagger.cropSizeObsv[1], avd_dagger.cropSizeObsv[0]).float().cuda()
        tvec_batch = torch.zeros(batch_size, parIL.nTargets).float().cuda()
        pose_gt_batch = np.zeros((batch_size, seq_len, 3), dtype=np.float32)
        collisions_batch = torch.zeros(batch_size, seq_len).float().cuda()
        costs_batch = torch.zeros(batch_size, seq_len, len(action_list)).float().cuda()
        points2D_batch, local3D_batch = [], []
        image_names_batch, scene_batch, scale_batch, actions = [], [], [], []
        
        for i in range(len(avd_dagger)):
            ex = avd_dagger[i]
            img = ex["image"].unsqueeze(0)
            points2D_seq, local3D_seq = [], [] 
            points2D_seq.append(ex["points2D"])
            local3D_seq.append(ex["local3D"])
            sseg = ex["sseg"].unsqueeze(0)
            dets = ex['dets'].unsqueeze(0)
            mapNet_input_start = (img.cuda(), points2D_seq, local3D_seq, sseg.cuda(), dets.cuda())
            
            target_lbl = ex["target_lbl"]
            im_obsv = ex['image_obsv'].cuda()
            dets_obsv = ex['dets_obsv'].cuda()
            tvec = torch.zeros(1, parIL.nTargets).float().cuda()
            tvec[0,target_lbl] = 1
            image_name_seq = []
            image_name_seq.append(ex['image_name'])
            scene = ex['scene']
            scene_batch.append(scene)
            scale = ex['scale']
            scale_batch.append(scale)
            graph = avd_dagger.graphs_dict[target_lbl][scene]
            abs_pose_seq = np.zeros((seq_len, 3), dtype=np.float32)

            annotations, _, im_names_all, world_poses, directions = dh.load_scene_info(parIL.avd_root, scene)
            start_abs_pose = dh.get_image_poses(world_poses, directions, im_names_all, image_name_seq, scale)
            
            p_, confidence_, map_ = mapNet.forward_single_step(local_info=mapNet_input_start, t=0, 
                                                    input_flags=parMapNet.input_flags, update_type=parMapNet.update_type)
            collision_ = torch.tensor([0], dtype=torch.float32).cuda()
            if parIL.use_ego_obsv:
                enc_in = torch.cat((im_obsv, dets_obsv), 0).unsqueeze(0)
                ego_obsv_feat = ego_encoder(enc_in)
                state = (map_, p_, tvec, collision_, ego_obsv_feat)
            else:
                state = (map_, p_, tvec, collision_) 
            current_im = image_name_seq[0]

            imgs_batch[i,0,:,:,:] = img
            sseg_batch[i,0,:,:,:] = sseg
            dets_batch[i,0,:,:,:] = dets
            imgs_obsv_batch[i,0,:,:,:] = im_obsv
            dets_obsv_batch[i,0,:,:,:] = dets_obsv
            tvec_batch[i] = tvec
            collisions_batch[i,0] = collision_
            abs_pose_seq[0,:] = start_abs_pose
            cost = np.asarray(dh.get_state_action_cost(current_im, action_list, annotations, graph), dtype=np.float32)
            costs_batch[i,0,:] = torch.from_numpy(cost).float()

            policy_net.hidden = policy_net.init_hidden(batch_size=1, state_items=len(state)-1)
            for t in range(1, seq_len):
                pred_costs = policy_net(state, parIL.use_ego_obsv)
                pred_costs = pred_costs.view(-1).cpu().numpy()
                pred_label = np.argmin(pred_costs)
                pred_action = action_list[pred_label]
                actions.append(pred_action)

                next_im = avd_dagger.scene_annotations[scene][current_im][pred_action]
                if not(next_im==''):
                    collision = 0
                    batch_next, obsv_batch_next = avd_dagger.get_step_data(next_ims=[next_im], scenes=[scene], scales=[scale])
                    next_im_abs_pose = dh.get_image_poses(world_poses, directions, im_names_all, [next_im], scale)
                    if parIL.use_p_gt:
                        abs_poses = np.concatenate((start_abs_pose, next_im_abs_pose), axis=0)
                        rel_poses = dh.relative_poses(poses=abs_poses)
                        next_im_rel_pose = np.expand_dims(rel_poses[1,:], axis=0)
                        p_gt = dh.build_p_gt(parMapNet, pose_gt_batch=np.expand_dims(next_im_rel_pose, axis=1)).squeeze(1)
                        p_next,  confidence_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, input_flags=parMapNet.input_flags,
                                                                map_previous=state[0], p_given=p_gt, update_type=parMapNet.update_type)
                    else:
                        p_next, confidence_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, 
                                            input_flags=parMapNet.input_flags, map_previous=state[0], update_type=parMapNet.update_type)
                    if parIL.use_ego_obsv:
                        enc_in = torch.cat(obsv_batch_next, 1)
                        ego_obsv_feat = ego_encoder(enc_in)
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda(), ego_obsv_feat)
                    else:
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda())
                    current_im = next_im
                    
                    (imgs_next, points2D_next, local3D_next, sseg_next, dets_next) = batch_next
                    (imgs_obsv_next, dets_obsv_next) = obsv_batch_next
                    imgs_batch[i,t,:,:,:] = imgs_next
                    sseg_batch[i,t,:,:,:] = sseg_next
                    dets_batch[i,t,:,:,:] = dets_next
                    imgs_obsv_batch[i,t,:,:,:] = imgs_obsv_next
                    dets_obsv_batch[i,t,:,:,:] = dets_obsv_next
                    collisions_batch[i,t] = torch.tensor([collision], dtype=torch.float32)
                    abs_pose_seq[t,:] = next_im_abs_pose
                    cost = np.asarray(dh.get_state_action_cost(current_im, action_list, annotations, graph), dtype=np.float32)
                    costs_batch[i,t,:] = torch.from_numpy(cost).float()
                    image_name_seq.append(current_im)
                    points2D_seq.append(points2D_next[0])
                    local3D_seq.append(local3D_next[0])

                else:
                    collision = 1
                    if parIL.stop_on_collision:
                        break
                    if parIL.use_ego_obsv:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda(), state[4])
                    else:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda())
                    
                    imgs_batch[i,t,:,:,:] = imgs_batch[i,t-1,:,:,:]
                    sseg_batch[i,t,:,:,:] = sseg_batch[i,t-1,:,:,:]
                    dets_batch[i,t,:,:,:] = dets_batch[i,t-1,:,:,:]
                    imgs_obsv_batch[i,t,:,:,:] = imgs_obsv_batch[i,t-1,:,:,:]
                    dets_obsv_batch[i,t,:,:,:] = dets_obsv_batch[i,t-1,:,:,:]
                    collisions_batch[i,t] = torch.tensor([collision], dtype=torch.float32)
                    abs_pose_seq[t,:] = abs_pose_seq[t-1,:]
                    costs_batch[i,t,:] = costs_batch[i,t-1,:]
                    image_name_seq.append(current_im)
                    points2D_seq.append(points2D_seq[t-1])
                    local3D_seq.append(local3D_seq[t-1])

            pose_gt_batch[i] = dh.relative_poses(poses=abs_pose_seq)
            image_names_batch.append(image_name_seq)
            points2D_batch.append(points2D_seq)
            local3D_batch.append(local3D_seq)

        actions = np.asarray(actions)
        image_names_batch = np.asarray(image_names_batch)

        mapNet_batch = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch)
        IL_batch = (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, image_names_batch, scene_batch, scale_batch)
        return mapNet_batch, IL_batch

if __name__ == '__main__':
    # Configuration parameters
    parMapNet = ParametersMapNet()
    parIL = Parameters_IL()
    parIL.model_id = "IL_perturb_contrast"
    parIL.model_dir = os.path.join(os.path.dirname(parIL.model_dir), f"{parIL.model_id}/")
    os.makedirs(parIL.model_dir, exist_ok=True)
    
    # Contrastive learning hyperparameters
    contrast_weight = 0.1
    temperature = 0.07
    negative_samples = 32
    
    action_list = np.asarray(parMapNet.action_list)

    # Initialize models
    policy_net = ILNet(parIL, parMapNet.map_embedding, parMapNet.orientations, parIL.nTargets, len(action_list), parIL.use_ego_obsv)
    policy_net.train()
    policy_net.cuda()

    # Load pre-trained MapNet model
    state_model = hl.load_model(model_dir=parIL.mapNet_model_dir, model_name="MapNet",
                                test_iter=parIL.mapNet_iters, eval=not(parIL.finetune_mapNet))
    
    # Initialize optimizer
    if parIL.finetune_mapNet:
        all_params = chain(policy_net.parameters(), state_model.parameters())
    else:
        all_params = policy_net.parameters()
    optimizer = optim.Adam(all_params, lr=parIL.lr_rate)
    scheduler = StepLR(optimizer, step_size=parIL.step_size, gamma=parIL.gamma)
    
    # Initialize InfoNCE loss
    contrast_criterion = InfoNCELoss(temperature=temperature)

    # Load ego observation encoder (if using)
    if parIL.use_ego_obsv:
        ego_encoder = Encoder()
        ego_encoder.cuda()
        ego_encoder.eval()
    else:
        ego_encoder = None

    # Load perturbed contrastive learning dataset
    print("Loading training episodes with perturbation and contrast...")
    img_encoder_path = os.path.join(os.path.dirname(os.path.dirname(parIL.model_dir)), 
                                   "Img_encoder.pth.tar")
    avd = AVD_IL_Perturbed_Contrast(
        par=parIL, 
        seq_len=parIL.seq_len, 
        nEpisodes=parIL.epi_per_scene,
        scene_list=parIL.train_scene_list, 
        action_list=action_list,
        perturb_prob=0.6,
        similarity_threshold=0.6,
        img_encoder_path=img_encoder_path
    )

    # Load validation dataset
    print("Loading validation episodes...")
    avd_test = AVD_online(par=parIL, nStartPos=10, scene_list=parIL.train_scene_list, action_list=action_list)
    
    test_ids = list(range(len(avd_test)))
    train_ids = list(range(len(avd)))
    
    # Save parameters and initialize log
    hl.save_params(parIL, parIL.model_dir, name="IL")
    hl.save_params(parMapNet, parIL.model_dir, name="mapNet")
    log = open(parIL.model_dir + f"train_log_{parIL.model_id}.txt", 'w')
    
    nData = len(train_ids)
    iters_per_epoch = int(nData / float(parIL.batch_size))
    log.write(f"Iters_per_epoch: {iters_per_epoch}\n")
    print(f"Iters per epoch: {iters_per_epoch}")
    
    loss_list = []
    contrast_loss_list = []

    # Start training loop
    for ep in range(parIL.nEpochs):
        epoch_start_time = time.time()
        random.shuffle(train_ids)
        data_index = 0
        
        for i in range(iters_per_epoch):
            iters = i + ep * iters_per_epoch
            
            # Select data source: pre-sampled or online generated
            ch = select_minibatch(par=parIL, iters_done=iters)
            
            if ch:
                # Use preprocessed contrastive learning data
                mapNet_batch, IL_batch, perturbed_mapNet_batch, perturbed_IL_batch, contrast_info = get_minibatch_with_contrast(
                    batch_size=parIL.batch_size,
                    tvec_dim=parIL.nTargets,
                    seq_len=parIL.seq_len,
                    nActions=len(action_list),
                    data=avd,
                    ex_ids=train_ids,
                    data_index=data_index
                )
                
                use_contrast_loss = True
                
            else:
                # Use online generated data
                mapNet_batch, IL_batch = unroll_policy(parIL, parMapNet, policy_net, state_model,
                                                    action_list, batch_size=parIL.batch_size, 
                                                    seq_len=parIL.seq_len, graphs=avd.graphs_dict)
                
                perturbed_mapNet_batch = None
                perturbed_IL_batch = None
                contrast_info = None
                use_contrast_loss = False
            
            data_index += parIL.batch_size
            
            # Unpack data batches
            (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch) = mapNet_batch
            (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, 
             image_names, scenes, scales) = IL_batch
            
            # 1. Process original trajectories
            start_info = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch)
            if parIL.finetune_mapNet:
                p_, map_ = run_mapNet(parMapNet, state_model, start_info, parIL.use_p_gt, pose_gt_batch)
            else:
                with torch.no_grad():
                    p_, map_ = run_mapNet(parMapNet, state_model, start_info, parIL.use_p_gt, pose_gt_batch)
            
            if parIL.use_ego_obsv:
                with torch.no_grad():
                    enc_in = torch.cat((imgs_obsv_batch, dets_obsv_batch), 2)
                    enc_in = enc_in.view(parIL.batch_size*parIL.seq_len, 4, parIL.crop_size_obsv[1], parIL.crop_size_obsv[0])
                    ego_obsv_feat = ego_encoder(enc_in)
                    ego_obsv_feat = ego_obsv_feat.view(parIL.batch_size, parIL.seq_len, ego_obsv_feat.shape[1])
                state = (map_, p_, tvec_batch, collisions_batch, ego_obsv_feat)
            else: 
                state = (map_, p_, tvec_batch, collisions_batch)
            
            # Compute main navigation loss
            policy_net.hidden = policy_net.init_hidden(parIL.batch_size, state_items=len(state)-1)
            pred_costs = policy_net(state, parIL.use_ego_obsv)
            nav_loss = policy_net.build_loss(cost_pred=pred_costs, cost_gt=costs_batch, loss_weight=parIL.loss_weight)
            
            # Initialize total loss
            total_loss = nav_loss
            contrast_loss = torch.tensor(0.0).cuda()
            perturbed_nav_loss = torch.tensor(0.0).cuda()
            
            # 2. Compute additional losses for contrastive learning
            if use_contrast_loss and perturbed_mapNet_batch is not None:
                # Unpack perturbed data
                (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                 perturbed_sseg_batch, perturbed_dets_batch, perturbed_pose_gt_batch) = perturbed_mapNet_batch
                (perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch, _, perturbed_collisions_batch, 
                 perturbed_actions, perturbed_costs_batch, _, _, _) = perturbed_IL_batch
                
                # Process perturbed trajectories
                perturbed_start_info = (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                                       perturbed_sseg_batch, perturbed_dets_batch)
                if parIL.finetune_mapNet:
                    perturbed_p_, perturbed_map_ = run_mapNet(parMapNet, state_model, perturbed_start_info, 
                                                            parIL.use_p_gt, perturbed_pose_gt_batch)
                else:
                    with torch.no_grad():
                        perturbed_p_, perturbed_map_ = run_mapNet(parMapNet, state_model, perturbed_start_info, 
                                                                 parIL.use_p_gt, perturbed_pose_gt_batch)
                
                if parIL.use_ego_obsv:
                    with torch.no_grad():
                        perturbed_enc_in = torch.cat((perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch), 2)
                        perturbed_enc_in = perturbed_enc_in.view(parIL.batch_size*parIL.seq_len, 4, 
                                                               parIL.crop_size_obsv[1], parIL.crop_size_obsv[0])
                        perturbed_ego_obsv_feat = ego_encoder(perturbed_enc_in)
                        perturbed_ego_obsv_feat = perturbed_ego_obsv_feat.view(parIL.batch_size, parIL.seq_len, 
                                                                        perturbed_ego_obsv_feat.shape[1])
                    perturbed_state = (perturbed_map_, perturbed_p_, tvec_batch, perturbed_collisions_batch, perturbed_ego_obsv_feat)
                else: 
                    perturbed_state = (perturbed_map_, perturbed_p_, tvec_batch, perturbed_collisions_batch)
                
                # Compute contrastive learning loss
                orig_features = extract_state_features(parIL, state, policy_net)
                perturbed_features = extract_state_features(parIL, perturbed_state, policy_net)
                
                valid_contrast_samples = 0
                for b in range(parIL.batch_size):
                    perturb_point = contrast_info['perturb_points'][b]
                    
                    if perturb_point > 0:
                        anchor_feat = orig_features[b, perturb_point+1].unsqueeze(0)
                        contrast_feat = perturbed_features[b, perturb_point+1].unsqueeze(0)
                        is_positive = contrast_info['is_positive'][b]
                        
                        neg_indices = [j for j in range(parIL.batch_size) if j != b]
                        if len(neg_indices) > negative_samples:
                            neg_indices = random.sample(neg_indices, negative_samples)
                        
                        if is_positive:
                            neg_feats = torch.stack([perturbed_features[j, perturb_point+1] for j in neg_indices])
                            pos_contrast_loss = contrast_criterion(anchor_feat, contrast_feat, neg_feats.unsqueeze(0))
                            contrast_loss += pos_contrast_loss
                        else:
                            if neg_indices:
                                pos_idx = random.choice(neg_indices)
                                pos_feat = orig_features[pos_idx, perturb_point+1].unsqueeze(0)
                                
                                all_neg_indices = neg_indices.copy()
                                if pos_idx in all_neg_indices:
                                    all_neg_indices.remove(pos_idx)
                                if len(all_neg_indices) > 0:
                                    other_neg_feats = torch.stack([perturbed_features[j, perturb_point+1] for j in all_neg_indices])
                                    neg_feats = torch.cat([contrast_feat, other_neg_feats], dim=0)
                                    
                                    neg_contrast_loss = contrast_criterion(anchor_feat, pos_feat, neg_feats.unsqueeze(0))
                                    contrast_loss += neg_contrast_loss
                                    valid_contrast_samples += 1
                
                if valid_contrast_samples > 0:
                    contrast_loss = contrast_loss / valid_contrast_samples
                
                # Compute navigation loss for perturbed trajectories
                policy_net.hidden = policy_net.init_hidden(parIL.batch_size, state_items=len(perturbed_state)-1)
                perturbed_pred_costs = policy_net(perturbed_state, parIL.use_ego_obsv)
                perturbed_nav_loss = policy_net.build_loss(
                    cost_pred=perturbed_pred_costs, 
                    cost_gt=perturbed_costs_batch, 
                    loss_weight=parIL.loss_weight
                )
                
                # Update total loss
                total_loss = nav_loss + perturbed_nav_loss + contrast_weight * contrast_loss
            
            # Backpropagation and optimization
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # Logging and visualization
            if iters % parIL.show_interval == 0:
                if use_contrast_loss:
                    log_msg = (f"Epoch: {ep} ITER: {iters} "
                              f"Nav_Loss: {nav_loss.item():.4f} "
                              f"Perturb_Loss: {perturbed_nav_loss.item():.4f} "
                              f"Contrast_Loss: {contrast_loss.item():.4f} "
                              f"Total_Loss: {total_loss.item():.4f} [Contrast]")
                else:
                    log_msg = (f"Epoch: {ep} ITER: {iters} "
                              f"Nav_Loss: {nav_loss.item():.4f} "
                              f"Total_Loss: {total_loss.item():.4f} [Online]")
                log.write(log_msg + "\n")
                print(log_msg)
                log.flush()
            
            if iters > 0:
                loss_list.append(total_loss.item())
                contrast_loss_list.append(contrast_loss.item())
                
            if iters % parIL.plot_interval == 0 and iters > 0:
                hl.plot_loss(loss=loss_list, epoch=ep, iteration=iters, 
                            step=1, loss_name="Total_Loss", loss_dir=parIL.model_dir)
                hl.plot_loss(loss=contrast_loss_list, epoch=ep, iteration=iters,
                            step=1, loss_name="Contrast_Loss", loss_dir=parIL.model_dir)
                
            if iters % parIL.save_interval == 0:
                hl.save_model(model=policy_net, model_dir=parIL.model_dir, 
                             model_name="ILNet", train_iter=iters)
                if parIL.finetune_mapNet:
                    hl.save_model(model=state_model, model_dir=parIL.model_dir, 
                                model_name="MapNet", train_iter=iters)
            
            if iters % parIL.test_interval == 0:
                evaluate_NavNet(parIL, parMapNet, mapNet=state_model, ego_encoder=ego_encoder, 
                               test_iter=iters, test_ids=test_ids, test_data=avd_test, 
                               action_list=action_list)
        
        # Update learning rate after each epoch
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch {ep} completed in {epoch_time:.2f} seconds. Updating learning rate...")
        scheduler.step()




    
</pre>
        </div>
      </div>
    </div>   
    <!-- code5.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">Trajectory_Perturbation.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
AVD dataset loader with trajectory perturbation and contrastive learning.
Generates original and perturbed trajectory pairs for contrastive learning.
"""
import torch
from torch.utils.data import Dataset
import numpy as np
from tqdm import tqdm
import random
import os
import data_helper as dh

import torch.nn as nn
from torchvision.models import resnet18 as resnet18_img
from torchvision import transforms
from PIL import Image

class AVD_IL_Perturbed_Contrast(Dataset):
    """
    AVD dataset with trajectory perturbation and contrastive learning capabilities.
    Samples episodes with action costs and generates perturbed trajectories for contrastive learning.
    """
    def __init__(self, par, seq_len, nEpisodes, scene_list, action_list, perturb_prob=0.8, 
                similarity_threshold=0.5, img_encoder_path=None):
        self.datasetPath = par.avd_root
        self.cropSize = par.crop_size
        self.cropSizeObsv = par.crop_size_obsv
        self.orig_res = par.orig_res
        self.normalize = True
        self.pixFormat = "NCHW"
        self.scene_list = scene_list
        self.n_episodes = nEpisodes
        self.seq_len = seq_len
        self.actions = action_list
        self.dets_nClasses = par.dets_nClasses
        
        self.perturb_prob = perturb_prob
        self.similarity_threshold = similarity_threshold
        
        self.sseg_data = np.load(par.sseg_file_path, encoding='bytes', allow_pickle=True).item()
        
        self.detection_data, self.labels_to_cats, self.labels_to_index = dh.load_detections(par, self.scene_list)
        self.cat_dict = par.cat_dict
        
        self.targets_data = np.load(par.targets_file_path, encoding='bytes', allow_pickle=True).item()
        
        self.graphs_dict = dh.get_scene_target_graphs(self.datasetPath, self.cat_dict, self.targets_data, self.actions)
        
        self.torch_device = 'cuda:' + str(par.TORCH_GPU_ID) if hasattr(par, 'TORCH_GPU_ID') and torch.cuda.device_count() > 0 else 'cpu'
        self.feature_dim = par.img_embedding_dim if hasattr(par, 'img_embedding_dim') else 512

        self.img_encoder = None
        if img_encoder_path is None:
            img_encoder_path = os.path.join(os.path.dirname(os.path.dirname(par.model_dir)), 
                                          "Img_encoder.pth.tar")
        self.img_encoder = self.load_img_encoder(self.feature_dim, img_encoder_path)
        
        self.sample_episodes()

    def load_img_encoder(self, feature_dim, ckpt_pth):
        """Load image encoder from checkpoint."""
        img_encoder = resnet18_img(num_classes=feature_dim)
        dim_mlp = img_encoder.fc.weight.shape[1]
        img_encoder.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), img_encoder.fc)
        if not os.path.exists(ckpt_pth):
            print(f"Warning: Encoder file not found: {ckpt_pth}")
            print("Using randomly initialized encoder")
        else:
            try:
                ckpt = torch.load(ckpt_pth, map_location='cpu')
                if 'state_dict' in ckpt:
                    state_dict = ckpt['state_dict']
                    if any(key.startswith('module.encoder_q.') for key in state_dict.keys()):
                        state_dict = {k[len('module.encoder_q.'):]: v for k, v in state_dict.items() if 'module.encoder_q.' in k}
                    elif any(key.startswith('module.') for key in state_dict.keys()):
                        state_dict = {k[len('module.'):]: v for k, v in state_dict.items() if k.startswith('module.')}
                else:
                    state_dict = ckpt
                img_encoder.load_state_dict(state_dict, strict=False)
                print(f"Successfully loaded image encoder: {ckpt_pth}")
            except Exception as e:
                print(f"Failed to load encoder: {e}")
                print("Using randomly initialized encoder")
        img_encoder.eval().to(self.torch_device)
        return img_encoder

    def embed_image(self, img_tensor):
        """Extract image features using pretrained encoder."""
        if self.img_encoder is None:
            return None
        with torch.no_grad():
            if isinstance(img_tensor, np.ndarray):
                if img_tensor.shape[0] != 3 and img_tensor.shape[-1] == 3:
                    img_tensor = np.transpose(img_tensor, (2, 0, 1))
                img_tensor = torch.from_numpy(img_tensor).float()
            if len(img_tensor.shape) == 3:
                img_tensor = img_tensor.unsqueeze(0)
            img_tensor = img_tensor.to(self.torch_device)
            features = self.img_encoder(img_tensor)
            features = nn.functional.normalize(features, dim=1)
            return features.cpu().numpy()

    def is_close(self, img1_features, img2_features, threshold=None):
        """Compute similarity between two image features, return (is_positive, similarity)."""
        if threshold is None:
            threshold = self.similarity_threshold
            
        if img1_features is None or img2_features is None:
            return False, 0.0
            
        img1_features = img1_features.flatten()
        img2_features = img2_features.flatten()
        similarity = np.dot(img1_features, img2_features) / (np.linalg.norm(img1_features) * np.linalg.norm(img2_features))
        
        return similarity > threshold, similarity

    def sample_episodes(self):
        """Sample episodes with original and perturbed trajectories."""
        epi_id = 0
        im_paths, action_paths, cost_paths = {}, {}, {}
        scene_name, scene_scale, target_lbls = {}, {}, {}
        pose_paths, collisions = {}, {}
        
        perturbed_im_paths, perturbed_action_paths = {}, {}
        perturbed_cost_paths, perturbed_pose_paths = {}, {}
        perturbed_collisions = {}
        similarity_scores = {}
        perturb_points = {}
        is_positive_sample = {}

        total_perturbed_episodes = 0
        total_positive_samples = 0
        total_negative_samples = 0
        total_no_perturbation = 0
        similarity_score_list = []
        scene_stats = {}

        for scene in tqdm(self.scene_list, desc="Sampling scenes"):
            annotations, scale, im_names_all, world_poses, directions = dh.load_scene_info(self.datasetPath, scene)
            scene_epi_count = 0
            
            scene_stats[scene] = {
                'total_episodes': 0,
                'perturbed_episodes': 0,
                'positive_samples': 0,
                'negative_samples': 0,
                'no_perturbation': 0,
                'similarity_scores': []
            }
            
            pbar = tqdm(total=self.n_episodes, desc=f"Sampling {scene} episodes", leave=False)
            
            while scene_epi_count < self.n_episodes:
                idx = np.random.randint(len(im_names_all), size=1)
                im_name_0 = im_names_all[idx[0]]
                
                candidates = dh.candidate_targets(scene, self.cat_dict, self.targets_data)
                idx_cat = np.random.randint(len(candidates), size=1)
                cat = candidates[idx_cat[0]]
                target_lbl = self.cat_dict[cat]
                graph = self.graphs_dict[target_lbl][scene]

                choice = np.random.randint(2, size=1)[0]
                im_seq, action_seq, cost_seq = [], [], []
                poses_seq, collision_seq = [], []
                im_seq.append(im_name_0)
                current_im = im_name_0
                
                cost_seq.append(dh.get_state_action_cost(current_im, self.actions, annotations, graph))
                poses_seq.append(dh.get_im_pose(im_names_all, current_im, world_poses, directions, scale))
                collision_seq.append(0)
                
                for i in range(1, self.seq_len):
                    if choice:
                        actions_cost = np.array(cost_seq[i-1])
                        min_cost = np.min(actions_cost)
                        min_ind = np.where(actions_cost==min_cost)[0]
                        if len(min_ind)==1:
                            sel_ind = min_ind[0]
                        else:
                            sel_ind = min_ind[np.random.randint(len(min_ind), size=1)[0]]
                        sel_action = self.actions[sel_ind]
                    else:
                        sel_action = self.actions[np.random.randint(len(self.actions), size=1)[0]]
                    
                    next_im = annotations[current_im][sel_action]
                    if not(next_im==''):
                        current_im = next_im
                        collision_seq.append(0)
                    else:
                        collision_seq.append(1)
                    
                    im_seq.append(current_im)
                    action_seq.append(sel_action)
                    poses_seq.append(dh.get_im_pose(im_names_all, current_im, world_poses, directions, scale))
                    cost_seq.append(dh.get_state_action_cost(current_im, self.actions, annotations, graph))
                
                im_paths[epi_id] = np.asarray(im_seq)
                action_paths[epi_id] = np.asarray(action_seq)
                cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                scene_name[epi_id] = scene
                scene_scale[epi_id] = scale
                target_lbls[epi_id] = target_lbl
                pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                
                scene_stats[scene]['total_episodes'] += 1
                
                apply_perturbation = np.random.rand() < self.perturb_prob
                
                if apply_perturbation:
                    valid_perturb_points = list(range(1, self.seq_len-1))
                    if len(valid_perturb_points) > 0:
                        perturb_idx = random.choice(valid_perturb_points)
                        perturb_points[epi_id] = perturb_idx
                        
                        perturb_im_seq = im_seq[:perturb_idx+1].copy()
                        perturb_action_seq = action_seq[:perturb_idx].copy()
                        perturb_cost_seq = cost_seq[:perturb_idx+1].copy()
                        perturb_poses_seq = poses_seq[:perturb_idx+1].copy()
                        perturb_collision_seq = collision_seq[:perturb_idx+1].copy()
                        
                        available_actions = [a for a in self.actions if a != action_seq[perturb_idx]]
                        if not available_actions:
                            available_actions = self.actions
                            
                        perturb_action = random.choice(available_actions)
                        perturb_action_seq.append(perturb_action)
                        
                        current_im = perturb_im_seq[perturb_idx]
                        next_im = annotations[current_im][perturb_action]
                        
                        if next_im != '':
                            total_perturbed_episodes += 1
                            scene_stats[scene]['perturbed_episodes'] += 1
                            
                            original_img_data, _, _ = dh.getImageData(
                                self.datasetPath, im_seq[perturb_idx+1], 
                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
                                
                            perturbed_img_data, _, _ = dh.getImageData(
                                self.datasetPath, next_im,
                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
                                
                            original_feat = self.embed_image(original_img_data)
                            perturbed_feat = self.embed_image(perturbed_img_data)
                            
                            is_pos, sim_score = self.is_close(original_feat, perturbed_feat)
                            is_positive_sample[epi_id] = is_pos
                            similarity_scores[epi_id] = sim_score
                            
                            similarity_score_list.append(sim_score)
                            scene_stats[scene]['similarity_scores'].append(sim_score)
                            
                            if is_pos:
                                total_positive_samples += 1
                                scene_stats[scene]['positive_samples'] += 1
                            else:
                                total_negative_samples += 1
                                scene_stats[scene]['negative_samples'] += 1
                            
                            current_im = next_im
                            perturb_im_seq.append(current_im)
                            perturb_collision_seq.append(0)
                            
                            perturb_poses_seq.append(dh.get_im_pose(im_names_all, current_im, 
                                                    world_poses, directions, scale))
                            perturb_cost_seq.append(dh.get_state_action_cost(
                                current_im, self.actions, annotations, graph))
                            
                            for i in range(perturb_idx+2, self.seq_len):
                                actions_cost = np.array(perturb_cost_seq[-1])
                                min_cost = np.min(actions_cost)
                                min_ind = np.where(actions_cost==min_cost)[0]
                                if len(min_ind)==1:
                                    sel_ind = min_ind[0]
                                else:
                                    sel_ind = min_ind[np.random.randint(len(min_ind), size=1)[0]]
                                sel_action = self.actions[sel_ind]
                                perturb_action_seq.append(sel_action)
                                
                                next_im = annotations[current_im][sel_action]
                                if not(next_im==''):
                                    current_im = next_im
                                    perturb_collision_seq.append(0)
                                else:
                                    perturb_collision_seq.append(1)
                                
                                perturb_im_seq.append(current_im)
                                perturb_poses_seq.append(dh.get_im_pose(im_names_all, current_im, 
                                                        world_poses, directions, scale))
                                perturb_cost_seq.append(dh.get_state_action_cost(
                                    current_im, self.actions, annotations, graph))
                        else:
                            is_positive_sample[epi_id] = False
                            similarity_scores[epi_id] = 0.0
                            total_no_perturbation += 1
                            scene_stats[scene]['no_perturbation'] += 1
                            perturb_im_seq = im_seq.copy()
                            perturb_action_seq = action_seq.copy()
                            perturb_cost_seq = cost_seq.copy()
                            perturb_poses_seq = poses_seq.copy()
                            perturb_collision_seq = collision_seq.copy()
                        
                        if len(perturb_im_seq) < self.seq_len:
                            last_im = perturb_im_seq[-1]
                            last_pose = perturb_poses_seq[-1]
                            last_cost = perturb_cost_seq[-1]
                            last_collision = perturb_collision_seq[-1]
                            
                            for _ in range(self.seq_len - len(perturb_im_seq)):
                                perturb_im_seq.append(last_im)
                                perturb_poses_seq.append(last_pose)
                                perturb_cost_seq.append(last_cost)
                                perturb_collision_seq.append(last_collision)
                                if len(perturb_action_seq) < self.seq_len - 1:
                                    perturb_action_seq.append(self.actions[0])
                        
                        perturbed_im_paths[epi_id] = np.asarray(perturb_im_seq)
                        perturbed_action_paths[epi_id] = np.asarray(perturb_action_seq)
                        perturbed_cost_paths[epi_id] = np.asarray(perturb_cost_seq, dtype=np.float32)
                        perturbed_pose_paths[epi_id] = np.asarray(perturb_poses_seq, dtype=np.float32)
                        perturbed_collisions[epi_id] = np.asarray(perturb_collision_seq, dtype=np.float32)
                    else:
                        perturbed_im_paths[epi_id] = np.asarray(im_seq)
                        perturbed_action_paths[epi_id] = np.asarray(action_seq)
                        perturbed_cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                        perturbed_pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                        perturbed_collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                        perturb_points[epi_id] = -1
                        is_positive_sample[epi_id] = False
                        similarity_scores[epi_id] = 0.0
                        total_no_perturbation += 1
                        scene_stats[scene]['no_perturbation'] += 1
                else:
                    perturbed_im_paths[epi_id] = np.asarray(im_seq)
                    perturbed_action_paths[epi_id] = np.asarray(action_seq)
                    perturbed_cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                    perturbed_pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                    perturbed_collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                    perturb_points[epi_id] = -1
                    is_positive_sample[epi_id] = False
                    similarity_scores[epi_id] = 0.0
                    total_no_perturbation += 1
                    scene_stats[scene]['no_perturbation'] += 1
                
                epi_id += 1
                scene_epi_count += 1
                pbar.update(1)
            pbar.close()
        
        self.print_sampling_statistics(
            total_episodes=epi_id,
            total_perturbed_episodes=total_perturbed_episodes,
            total_positive_samples=total_positive_samples,
            total_negative_samples=total_negative_samples,
            total_no_perturbation=total_no_perturbation,
            similarity_score_list=similarity_score_list,
            scene_stats=scene_stats
        )
        
        self.sampling_stats = {
            'total_episodes': epi_id,
            'total_perturbed_episodes': total_perturbed_episodes,
            'total_positive_samples': total_positive_samples,
            'total_negative_samples': total_negative_samples,
            'total_no_perturbation': total_no_perturbation,
            'similarity_scores': similarity_score_list,
            'scene_stats': scene_stats
        }
        
        self.im_paths = im_paths
        self.action_paths = action_paths
        self.cost_paths = cost_paths
        self.scene_name = scene_name
        self.scene_scale = scene_scale
        self.target_lbls = target_lbls
        self.pose_paths = pose_paths
        self.collisions = collisions
        
        self.perturbed_im_paths = perturbed_im_paths
        self.perturbed_action_paths = perturbed_action_paths
        self.perturbed_cost_paths = perturbed_cost_paths
        self.perturbed_pose_paths = perturbed_pose_paths
        self.perturbed_collisions = perturbed_collisions
        self.perturb_points = perturb_points
        self.is_positive_sample = is_positive_sample
        self.similarity_scores = similarity_scores
        
        print(f"Total sampled episodes: {epi_id}")
        
    def print_sampling_statistics(self, total_episodes, total_perturbed_episodes, 
                                total_positive_samples, total_negative_samples, 
                                total_no_perturbation, similarity_score_list, scene_stats):
        """Print detailed sampling statistics."""
        print("\n" + "="*80)
        print("Data Collection Statistics Report")
        print("="*80)
        
        print(f"Total episodes: {total_episodes}")
        print(f"Perturbed episodes: {total_perturbed_episodes} ({total_perturbed_episodes/total_episodes*100:.1f}%)")
        print(f"Non-perturbed episodes: {total_no_perturbation} ({total_no_perturbation/total_episodes*100:.1f}%)")
        print()
        
        if total_perturbed_episodes > 0:
            positive_ratio = total_positive_samples / total_perturbed_episodes * 100
            negative_ratio = total_negative_samples / total_perturbed_episodes * 100
            
            print("Contrastive Learning Sample Statistics:")
            print(f"  Positive samples: {total_positive_samples} ({positive_ratio:.1f}%)")
            print(f"  Negative samples: {total_negative_samples} ({negative_ratio:.1f}%)")
            print(f"  Positive:Negative ratio: {total_positive_samples}:{total_negative_samples}")
            
            if len(similarity_score_list) > 0:
                avg_similarity = np.mean(similarity_score_list)
                std_similarity = np.std(similarity_score_list)
                min_similarity = np.min(similarity_score_list)
                max_similarity = np.max(similarity_score_list)
                
                print(f"  Similarity statistics:")
                print(f"    Average similarity: {avg_similarity:.3f}")
                print(f"    Standard deviation: {std_similarity:.3f}")
                print(f"    Min value: {min_similarity:.3f}")
                print(f"    Max value: {max_similarity:.3f}")
                print(f"    Threshold: {self.similarity_threshold}")
            print()
        
        print("Per-scene statistics:")
        print(f"{'Scene Name':<15} {'Total Eps':<10} {'Perturbed Eps':<12} {'Positive':<8} {'Negative':<8} {'Pos Rate':<10}")
        print("-" * 80)
        
        for scene, stats in scene_stats.items():
            if stats['perturbed_episodes'] > 0:
                pos_rate = stats['positive_samples'] / stats['perturbed_episodes'] * 100
            else:
                pos_rate = 0.0
                
            print(f"{scene:<15} {stats['total_episodes']:<10} {stats['perturbed_episodes']:<12} "
                  f"{stats['positive_samples']:<8} {stats['negative_samples']:<8} {pos_rate:<10.1f}%")
        
        print("="*80)
        
        if len(similarity_score_list) > 0:
            print("\nSimilarity distribution:")
            bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
            hist, _ = np.histogram(similarity_score_list, bins=bins)
            
            for i in range(len(bins)-1):
                count = hist[i]
                percentage = count / len(similarity_score_list) * 100
                print(f"  [{bins[i]:.1f}-{bins[i+1]:.1f}): {count} samples ({percentage:.1f}%)")
        
        print("="*80 + "\n")

    def get_sampling_statistics(self):
        """Return sampling statistics for external use."""
        return getattr(self, 'sampling_stats', None)

    def __len__(self):
        return len(self.im_paths)

    def __getitem__(self, index):
        """Return original trajectory, perturbed trajectory, and contrast information."""
        item_original = {}
        item_perturbed = {}
        
        im_path = self.im_paths[index]
        action_path = self.action_paths[index]
        cost_path = self.cost_paths[index]
        scene = self.scene_name[index]
        scale = self.scene_scale[index]
        target_lbl = self.target_lbls[index]
        abs_poses = self.pose_paths[index]
        collision_seq = self.collisions[index]
        
        perturbed_im_path = self.perturbed_im_paths[index]
        perturbed_action_path = self.perturbed_action_paths[index]
        perturbed_cost_path = self.perturbed_cost_paths[index]
        perturbed_abs_poses = self.perturbed_pose_paths[index]
        perturbed_collision_seq = self.perturbed_collisions[index]
        
        perturb_point = self.perturb_points[index]
        is_positive = self.is_positive_sample[index]
        similarity_score = self.similarity_scores[index]
        
        scene_seg = self.sseg_data[scene.encode()]
        scene_dets = self.detection_data[scene]

        imgs = np.zeros((self.seq_len, 3, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        imgs_obsv = np.zeros((self.seq_len, 3, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        ssegs = np.zeros((self.seq_len, 1, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        dets = np.zeros((self.seq_len, self.dets_nClasses, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        dets_obsv = np.zeros((self.seq_len, 1, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        points2D, local3D = [], []
        
        for i in range(len(im_path)):
            im_name = im_path[i]
            imgData, points2D_step, local3D_step = dh.getImageData(self.datasetPath, im_name, 
                                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
            imgs[i,:,:,:] = imgData
            points2D.append(points2D_step)
            local3D.append(local3D_step)
            imgs_obsv[i,:,:,:] = dh.getImageData(self.datasetPath, im_name, scene, self.cropSizeObsv, 
                                            self.orig_res, self.pixFormat, self.normalize, get3d=False)
            ssegs[i,:,:,:] = dh.get_sseg(im_name, scene_seg, self.cropSize)
            dets[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSize, self.dets_nClasses, self.labels_to_index)
            dets_obsv[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSizeObsv, 1, self.labels_to_index)

        rel_poses = dh.relative_poses(poses=abs_poses)

        item_original["images"] = torch.from_numpy(imgs).float()
        item_original["images_names"] = im_path
        item_original["points2D"] = points2D
        item_original["local3D"] = local3D
        item_original["actions"] = action_path
        item_original["costs"] = torch.from_numpy(cost_path).float()
        item_original["target_lbl"] = target_lbl
        item_original["pose"] = rel_poses
        item_original["abs_pose"] = abs_poses
        item_original["collisions"] = torch.from_numpy(collision_seq).float()
        item_original["scene"] = scene
        item_original["scale"] = scale
        item_original['sseg'] = torch.from_numpy(ssegs).float()
        item_original['dets'] = torch.from_numpy(dets).float()
        item_original['images_obsv'] = torch.from_numpy(imgs_obsv).float()
        item_original['dets_obsv'] = torch.from_numpy(dets_obsv).float()
        
        perturbed_imgs = np.zeros((self.seq_len, 3, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_imgs_obsv = np.zeros((self.seq_len, 3, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        perturbed_ssegs = np.zeros((self.seq_len, 1, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_dets = np.zeros((self.seq_len, self.dets_nClasses, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_dets_obsv = np.zeros((self.seq_len, 1, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        perturbed_points2D, perturbed_local3D = [], []
        
        for i in range(len(perturbed_im_path)):
            im_name = perturbed_im_path[i]
            imgData, points2D_step, local3D_step = dh.getImageData(self.datasetPath, im_name, 
                                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
            perturbed_imgs[i,:,:,:] = imgData
            perturbed_points2D.append(points2D_step)
            perturbed_local3D.append(local3D_step)
            perturbed_imgs_obsv[i,:,:,:] = dh.getImageData(self.datasetPath, im_name, scene, self.cropSizeObsv, 
                                                    self.orig_res, self.pixFormat, self.normalize, get3d=False)
            perturbed_ssegs[i,:,:,:] = dh.get_sseg(im_name, scene_seg, self.cropSize)
            perturbed_dets[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSize, self.dets_nClasses, self.labels_to_index)
            perturbed_dets_obsv[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSizeObsv, 1, self.labels_to_index)

        perturbed_rel_poses = dh.relative_poses(poses=perturbed_abs_poses)
        
        item_perturbed["images"] = torch.from_numpy(perturbed_imgs).float()
        item_perturbed["images_names"] = perturbed_im_path
        item_perturbed["points2D"] = perturbed_points2D
        item_perturbed["local3D"] = perturbed_local3D
        item_perturbed["actions"] = perturbed_action_path
        item_perturbed["costs"] = torch.from_numpy(perturbed_cost_path).float()
        item_perturbed["target_lbl"] = target_lbl
        item_perturbed["pose"] = perturbed_rel_poses
        item_perturbed["abs_pose"] = perturbed_abs_poses
        item_perturbed["collisions"] = torch.from_numpy(perturbed_collision_seq).float()
        item_perturbed["scene"] = scene
        item_perturbed["scale"] = scale
        item_perturbed['sseg'] = torch.from_numpy(perturbed_ssegs).float()
        item_perturbed['dets'] = torch.from_numpy(perturbed_dets).float()
        item_perturbed['images_obsv'] = torch.from_numpy(perturbed_imgs_obsv).float()
        item_perturbed['dets_obsv'] = torch.from_numpy(perturbed_dets_obsv).float()
        
        contrast_info = {
            "perturb_point": perturb_point,
            "is_positive": is_positive,
            "similarity_score": similarity_score
        }
        
        return {
            "original": item_original,
            "perturbed": item_perturbed,
            "contrast_info": contrast_info
        }

</pre>     
        </div>
      </div>
    </div>
    <!-- code6.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">test_AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
"""
NavNet evaluation script.
Tests navigation policy using MapNet_AEP and IL_Net_AOS on AVD dataset.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import numpy as np
import random
from dataloader import AVD_online
from mapNet import MapNet
from IL_Net import Encoder
from parameters import ParametersMapNet, Parameters_IL
import helper as hl
import data_helper as dh
import networkx as nx
import pickle

def softmax(x):
	scoreMatExp = np.exp(np.asarray(x))
	return scoreMatExp / scoreMatExp.sum(0)


def prepare_mapNet_input(ex):
    """Prepare input data for MapNet from example."""
    img = ex["image"]
    points2D = ex["points2D"]
    local3D = ex["local3D"]
    sseg = ex["sseg"]
    dets = ex['dets']
    imgs_batch = img.unsqueeze(0)
    sseg_batch = sseg.unsqueeze(0)
    dets_batch = dets.unsqueeze(0)
    points2D_batch, local3D_batch = [], []
    points2D_batch.append(points2D)
    local3D_batch.append(local3D)
    return (imgs_batch.cuda(), points2D_batch, local3D_batch, sseg_batch.cuda(), dets_batch.cuda())


def evaluate_NavNet(parIL, parMapNet, mapNet, ego_encoder, test_iter, test_ids, test_data, action_list):
    """
    Evaluate NavNet performance on test episodes.
    Returns success rate, episode length, path ratio, SPL and APL metrics.
    """
    print("\nRunning validation on NavNet!")
    with torch.no_grad():
        policy_net = hl.load_model(model_dir=parIL.model_dir, model_name="ILNet", test_iter=test_iter)
        acc, epi_length, path_ratio = 0, 0, 0
        spl_sum, apl_sum = 0, 0  
        episode_results, episode_count = {}, 0
        for i in test_ids:
            test_ex = test_data[i]
            mapNet_input_start = prepare_mapNet_input(ex=test_ex)
            target_lbl = test_ex["target_lbl"]
            im_obsv = test_ex['image_obsv'].cuda()
            dets_obsv = test_ex['dets_obsv'].cuda()
            tvec = torch.zeros(1, parIL.nTargets).float().cuda()
            tvec[0,target_lbl] = 1
            image_name, scene, scale = [], [], []
            image_name.append(test_ex['image_name'])
            scene.append(test_ex['scene'])
            scale.append(test_ex['scale'])
            shortest_path_length = test_ex['path_length']

            if parIL.use_p_gt:
                info, annotations, _ = dh.load_scene_info(parIL.avd_root, scene[0])
                im_names_all = info['image_name']
                im_names_all = np.hstack(im_names_all)
                start_abs_pose = dh.get_image_poses(info, im_names_all, image_name, scale[0])

            p_, map_ = mapNet.forward_single_step(local_info=mapNet_input_start, t=0, 
                                                    input_flags=parMapNet.input_flags, update_type=parMapNet.update_type)
            collision_ = torch.tensor([0], dtype=torch.float32).cuda()
            if parIL.use_ego_obsv:
                enc_in = torch.cat((im_obsv, dets_obsv), 0).unsqueeze(0)
                ego_obsv_feat = ego_encoder(enc_in)
                state = (map_, p_, tvec, collision_, ego_obsv_feat)
            else:
                state = (map_, p_, tvec, collision_) 
            current_im = image_name[0]

            done=0
            image_seq, action_seq = [], []
            image_seq.append(current_im)
            policy_net.hidden = policy_net.init_hidden(batch_size=1, state_items=len(state)-1)
            for t in range(1, parIL.max_steps+1):
                pred_costs = policy_net(state, parIL.use_ego_obsv)
                pred_costs = pred_costs.view(-1).cpu().numpy()
                pred_probs = softmax(-pred_costs)
                pred_label = np.random.choice(len(action_list), 1, p=pred_probs)[0]
                pred_action = action_list[pred_label]

                next_im = test_data.scene_annotations[scene[0]][current_im][pred_action]
                if next_im=='':
                    image_seq.append(current_im)
                else:
                    image_seq.append(next_im)
                action_seq.append(pred_action)
                if not(next_im==''):
                    collision = 0
                    path_dist = len(nx.shortest_path(test_data.graphs_dict[target_lbl][scene[0]], next_im, "goal")) - 2
                    if path_dist <= parIL.steps_from_goal:
                        acc += 1
                        epi_length += t
                        path_ratio += t/float(shortest_path_length)
                        spl_sum += shortest_path_length / max(t, shortest_path_length)
                        apl_sum += t
                        done=1
                        break
                    batch_next, obsv_batch_next = test_data.get_step_data(next_ims=[next_im], scenes=scene, scales=scale)
                    if parIL.use_p_gt:
                        next_im_abs_pose = dh.get_image_poses(info, im_names_all, [next_im], scale[0])
                        abs_poses = np.concatenate((start_abs_pose, next_im_abs_pose), axis=0)
                        rel_poses = dh.relative_poses(poses=abs_poses)
                        next_im_rel_pose = np.expand_dims(rel_poses[1,:], axis=0)
                        p_gt = dh.build_p_gt(parMapNet, pose_gt_batch=np.expand_dims(next_im_rel_pose, axis=1)).squeeze(1)
                        p_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, input_flags=parMapNet.input_flags,
                                                                map_previous=state[0], p_given=p_gt, update_type=parMapNet.update_type)
                    else:
                        p_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, 
                                            input_flags=parMapNet.input_flags, map_previous=state[0], update_type=parMapNet.update_type)
                    if parIL.use_ego_obsv:
                        enc_in = torch.cat(obsv_batch_next, 1)
                        ego_obsv_feat = ego_encoder(enc_in)
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda(), ego_obsv_feat)
                    else:
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda())
                    current_im = next_im

                else:
                    collision = 1
                    if parIL.stop_on_collision:
                        break
                    if parIL.use_ego_obsv:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda(), state[4])
                    else:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda())
                
            episode_results[episode_count] = (image_seq, action_seq, parIL.lbl_to_cat[target_lbl], done)
            episode_count+=1
        
        scene_name_for_file = "_".join(parIL.predefined_test_scenes)
        episode_results_path = parIL.model_dir+f'episode_results_eval_{scene_name_for_file}_{test_iter}.pkl'
        
        with open(episode_results_path, 'wb') as f:
            pickle.dump(episode_results, f)
        
        success_rate = acc / float(len(test_ids))
        if acc > 0:
            mean_epi_length = epi_length / float(acc)
            avg_path_length_ratio = path_ratio / float(acc)
            apl = apl_sum / float(acc)
        else:
            mean_epi_length = 0
            avg_path_length_ratio = 0
            apl = 0
        
        spl = spl_sum / float(len(test_ids))
        
        print("Test iter:", test_iter, "Success rate:", success_rate)
        print("Mean epi length:", mean_epi_length, "Avg path length ratio:", avg_path_length_ratio)
        print("SPL (Success weighted by Path Length):", spl)
        print("APL (Average Path Length):", apl)



if __name__ == '__main__':
    parMapNet = ParametersMapNet()
    parIL = Parameters_IL()
    action_list = np.asarray(parMapNet.action_list)

    if parIL.use_predefined_test_set:
        avd = AVD_online(par=parIL, nStartPos=0, scene_list=parIL.predefined_test_scenes, 
                                                action_list=action_list, init_configs=parIL.predefined_confs_file)
    else:
        avd = AVD_online(par=parIL, nStartPos=10, scene_list=["Home_001_1"], action_list=action_list)

    test_ids = list(range(len(avd)))

    if parIL.finetune_mapNet:
        mapNet_model = hl.load_model(model_dir=parIL.model_dir, model_name="MapNet", test_iter=parIL.test_iters)
        print(f"Loaded MapNet model path: {parIL.model_dir}, iterations: {parIL.test_iters}")
    else:
        mapNet_model = hl.load_model(model_dir=parIL.mapNet_model_dir, model_name="MapNet", test_iter=parIL.mapNet_iters)
        print(f"Loaded MapNet model path: {parIL.mapNet_model_dir}, iterations: {parIL.mapNet_iters}")
    
    if parIL.use_ego_obsv:
        ego_encoder = Encoder()
        ego_encoder.cuda()
        ego_encoder.eval()
    else:
        ego_encoder = None

    evaluate_NavNet(parIL, parMapNet, mapNet_model, ego_encoder, test_iter=parIL.test_iters, 
                                            test_ids=test_ids, test_data=avd, action_list=action_list)

</pre>     
        </div>
      </div>
    </div> 
          
</section>



<!-- Real1 and Real2 -->
<section id="intro" class="container my-5">
  <h2 class="text-center">Run our model</h2>
  <div class="mt-3 text-center" style="font-size:1.1em;">
    <p>
      We ran our model on a laptop equipped with an Intel Core i7-10875H CPU and an NVIDIA RTX 2060 GPU. Input images were captured using the Realsense Depth Camera D455. For target detection, we employed YOLO, while semantic segmentation was performed using RedNet. Under powered conditions, the system achieved a frame rate of 7.8 FPS; when running on battery, the frame rate decreased to 1.4 FPS. The motion outputs generated by our model can subsequently be mapped to the motion control system of the mobile robot.
    </p>
  </div>
  <img src="images/real1.png" alt="Run our model" class="img-fluid d-block mx-auto" style="max-width:1300px;">

 
</section>


  <!-- 页脚 -->
  <footer class="bg-light text-center py-3">
    &copy; 2025 Yuhao Wang<br>
    This website is licensed under a
    <a rel="license"
       href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
  </footer>
</body>
</html>



































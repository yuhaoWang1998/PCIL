<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <title>PCIL-MVP</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css">
</head>
<body>
  <!-- 主视觉区 -->
  <header class="container my-5 text-center">
    <h1>PCIL-MVP: Perturbation Contrastive Imitation Learning with Multimodal Visual Perception for Active Visual Search</h1>
    <p class="lead">Yuhao Wang, Guohui Tian</p>
    <br><br>
    <img src="images/Figure2.png" alt="Architecture of AVS" class="img-fluid d-block mx-auto" style="max-width:900px;">
  </header>
  <!-- Abstract -->
<section id="abstract" class="container my-5">
  <h2 class="text-center">Abstract</h2>
  <div class="mt-3" style="max-width:900px; margin:auto; font-size:1.1em;">
    
Active visual search (AVS) presents substantial challenges for autonomous agents operating in novel environments, particularly when limited to egocentric RGB-D observations. To address the complexities of target recognition, semantic understanding, and localization amid occlusions and uncertain spatial layouts, we introduce a multi-modal feature fusion framework based on ground projection. Our approach unifies RGB, semantic segmentation, and object detection features into an allocentric grid map, employing CLIP-based vision-language models and cross-attention mechanisms to achieve robust semantic alignment. Localization reliability is enhanced by a hybrid confidence estimation strategy that integrates statistical analysis with learned prediction, facilitating uncertainty-aware navigation. Temporal dependencies are captured using a Transformer-based multi-frame fusion module, enabling resilient and consistent map updates in dynamic scenarios. Additionally, we propose a contrastive imitation learning paradigm featuring adaptive sampling and perturbation trajectory augmentation to improve policy robustness and generalization. Comprehensive experiments demonstrate that our framework significantly outperforms existing baselines on visual localization and search tasks. 
  </div>
</section>

  
   
  <!-- PCIL-MVP -->
  <section id="intro" class="container my-5">
    <h2 class="text-center">Active Environment Perception</h2>
    <img src="images/Figure3.png" alt="Architecture of AEP" class="img-fluid d-block mx-auto" style="max-width:900px;">
  </section>
  <!-- 方法 -->
  <section id="method" class="container my-5">
    <h2 class="text-center">Active Object Search</h2>
    <img src="images/Figure5.png" alt="Architecture of AOS" class="img-fluid d-block mx-auto" style="max-width:500px;">
  </section>
  <!-- Environments in AVDB -->
  <section id="environments" class="container my-5">
    <h2 class="text-center">Environments in AVDB</h2>
    <!-- 图片行 -->
    <div class="row mb-3 justify-content-center">
      <div class="col-2 text-center">
        <img src="Environments/Home_001_2.png" alt="Env 1" class="img-fluid" style="max-width:200px;">
        <div>Home_001_2</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_004_2.png" alt="Env 2" class="img-fluid" style="max-width:200px;">
        <div>Home_004_2</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_005_1.png" alt="Env 3" class="img-fluid" style="max-width:200px;">
        <div>Home_005_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_008_1.png" alt="Env 4" class="img-fluid" style="max-width:200px;">
        <div>Home_008_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_013_1.png" alt="Env 5" class="img-fluid" style="max-width:200px;">
        <div>Home_013_1</div>
      </div>
      <div class="col-2 text-center">
        <img src="Environments/Home_015_1.png" alt="Env 6" class="img-fluid" style="max-width:200px;">
        <div>Home_015_1</div>
      </div>
    </div>
    <!-- 视频行 -->
    <div class="row justify-content-center">
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_001_2.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_001_2</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_004_2.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_004_2</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_005_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_005_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_008_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_008_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_013_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_013_1</div>
      </div>
      <div class="col-2 text-center">
        <video controls class="img-fluid" style="max-width:200px;" preload="metadata">
          <source src="Environments/Home_015_1.mp4" type="video/mp4">
          您的浏览器不支持视频播放
        </video>
        <div>Home_015_1</div>
      </div>
    </div>
  </section>
  <!-- Examples -->
  <section id="results" class="container my-5">
    <h2 class="text-center">Examples</h2>
    <div class="row justify-content-center">
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_008_1_12.gif" alt="Example1" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_011_1_6.gif" alt="Example2" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_013_1_6.gif" alt="Example3" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_014_2_0.gif" alt="Example4" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_015_1_5.gif" alt="Example5" class="img-fluid" style="max-width:300px;">
      </div>
      <div class="col-12 col-md-4 mb-3 text-center">
        <img src="images/Home_016_1_27.gif" alt="Example6" class="img-fluid" style="max-width:300px;">
      </div>
    </div>
  </section>
<!-- Code -->
<section id="code" class="container my-5">
  <h2 class="text-center">Code</h2>
  <div class="mt-3 text-center" style="font-size:1.1em;">
    We release the training and model code for peer review.
  </div>
  <div class="row mt-4 justify-content-center">
    <!-- 示例：五个代码文件 -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">AEP.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import os
import numpy as np
import data_helper as dh
import clip

class CLIPFeatureExtractor(nn.Module):
    def __init__(self, clip_model="ViT-B/32", device="cuda"):
        super().__init__()

        
        if isinstance(device, int):
            self.device = f"cuda:{device}"
        elif isinstance(device, str):
            if device.isdigit():
                self.device = f"cuda:{device}"
            else:
                self.device = device
        else:
            self.device = device

        # 加载CLIP模型
        self.model, self.preprocess = clip.load(clip_model, device=self.device)

        
        for param in self.model.parameters():
            param.requires_grad = False

        
        self.object_categories = ['dining_table', 'fridge', 'tv', 'couch', 'microwave']

        
        self.text_features = self._encode_text_features()
        self.similarity_threshold = 0.3

    def _encode_text_features(self):
        """预计算所有类别的文本特征"""
        text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in self.object_categories]).to(self.device)
        with torch.no_grad():
            text_features = self.model.encode_text(text_inputs)
        return text_features / text_features.norm(dim=-1, keepdim=True)

    def preprocess_images(self, images):
        """预处理图像以匹配CLIP输入要求"""
        images = images.to(self.device)

       
        if len(images.shape) == 5:  # [B, T, C, H, W]
            B, T, C, H, W = images.shape
            images = images.view(B * T, C, H, W)
        elif len(images.shape) == 4:  # [B, C, H, W]
            B, C, H, W = images.shape
            T = 1
        else:
            raise ValueError(f"Unexpected image shape: {images.shape}")

        
        if H != 224 or W != 224:
            images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)

        # 确保数值范围在[0,1]
        if images.max() > 1.0:
            images = images / 255.0

        # CLIP标准化
        mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=self.device).view(1, 3, 1, 1)
        std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=self.device).view(1, 3, 1, 1)
        images_normalized = (images - mean) / std

        return images_normalized

    def encode_images(self, images):
        """编码图像特征"""
        model_device = next(self.model.parameters()).device
        images = images.to(model_device)

        
        original_shape = images.shape
        if len(original_shape) == 5:  # [B, T, C, H, W]
            B, T = original_shape[:2]
            images = images.view(B * T, *original_shape[2:])
        elif len(original_shape) == 4:  # [B, C, H, W]
            B, T = original_shape[0], 1
        else:
            raise ValueError(f"Unexpected image shape: {original_shape}")

        
        images_processed = self.preprocess_images(images)
        expected_dtype = getattr(self.model, 'dtype', torch.float32)
        images_processed = images_processed.to(dtype=expected_dtype)

        try:
            with torch.no_grad():
                image_features = self.model.encode_image(images_processed)
        except RuntimeError as e:
            print(f"Error during CLIP encoding: {e}")
            raise e

       
        image_features = image_features / image_features.norm(dim=-1, keepdim=True).clamp(min=1e-8)

        
        if len(original_shape) == 5:
            feature_dim = image_features.shape[-1]
            image_features = image_features.view(B, T, feature_dim)

        return image_features

    def get_relevant_objects(self, images):
        """获取相关物品的特征和掩码"""
        images = images.to(self.device)
        image_features = self.encode_images(images)

        # 计算相似度
        if len(image_features.shape) == 3:  # [B, T, feature_dim]
            B, T, feature_dim = image_features.shape
            image_features_flat = image_features.view(B * T, feature_dim)
        else:  # [B, feature_dim]
            B, feature_dim = image_features.shape
            T = 1
            image_features_flat = image_features

        similarities = 100.0 * image_features_flat @ self.text_features.T

        if T > 1:
            similarities = similarities.view(B, T, -1)

        relevance_mask = similarities > self.similarity_threshold

        return {
            'image_features': image_features,
            'text_features': self.text_features,
            'similarities': similarities,
            'relevance_mask': relevance_mask,
            'categories': self.object_categories
        }


class ObjectAttentionLayer(nn.Module):
    def __init__(self, d_model, nhead=4, dropout=0.1):
        super().__init__()
        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        # FFN
        self.linear1 = nn.Linear(d_model, d_model * 4)
        self.dropout1 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_model * 4, d_model)
        self.dropout2 = nn.Dropout(dropout)
        self.norm2 = nn.LayerNorm(d_model)
        self.activation = F.relu

    def forward_ffn(self, tgt):
        tgt2 = self.linear2(self.dropout1(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        return tgt

    def forward(self, map_features, object_features, relevance_mask=None):
   
        B, H_W, d_model = map_features.shape
        _, num_obj, _ = object_features.shape

       
        q = map_features.permute(1, 0, 2)  # [H*W, B, d]
        k = object_features.permute(1, 0, 2)  # [num_obj, B, d]
        v = object_features.permute(1, 0, 2)  # [num_obj, B, d]

        
        key_padding_mask = None
        if relevance_mask is not None:
            key_padding_mask = ~relevance_mask

        
        map_features2, _ = self.multihead_attn(
            query=q,
            key=k,
            value=v,
            key_padding_mask=key_padding_mask
        )

      
        map_features2 = map_features2.permute(1, 0, 2)  # [B, H*W, d]
        map_features = map_features + self.dropout(map_features2)
        map_features = self.norm(map_features)

        
        map_features = self.forward_ffn(map_features)

        return map_features



class TemporalTransformer(nn.Module):
  def __init__(self, d_model, num_heads, num_layers, window_size):
      super(TemporalTransformer, self).__init__()
      self.d_model = d_model
      self.window_size = window_size

     
      self.pos_encoding = nn.Parameter(torch.randn(window_size, d_model))

      
      encoder_layer = nn.TransformerEncoderLayer(
          d_model=d_model,
          nhead=num_heads,
          dim_feedforward=4 * d_model,
          dropout=0.1

      )
      self.transformer_encoder = nn.TransformerEncoder(
          encoder_layer,
          num_layers=num_layers
      )

      
      self.output_proj = nn.Linear(d_model, d_model)

  def forward(self, x):

      batch_size, spatial_points, seq_len, d_model = x.shape

     
      x = x.view(batch_size * spatial_points, seq_len, d_model)

      
      x = x + self.pos_encoding[:seq_len].unsqueeze(0)

    
      x = x.transpose(0, 1)

      # 通过Transformer (现在输入是 [seq_len, B*H*W, d_model])
      output = self.transformer_encoder(x)  # 输出: [seq_len, B*H*W, d_model]

      # 取最后一个时间步的输出作为融合结果
      fused_output = output[-1, :, :]  # [B*H*W, d_model]

      # 应用输出投影
      fused_output = self.output_proj(fused_output)

      # 恢复空间维度: [B*H*W, d_model] -> [B, H*W, d_model]
      fused_output = fused_output.view(batch_size, spatial_points, d_model)

      return fused_output


class MapNet_transformer_uncertain_clip_1(nn.Module):
    def __init__(self, par, update_type, input_flags):
        super(MapNet_transformer_uncertain_clip_1, self).__init__()
        (with_feat, with_sseg, with_dets, use_raw_sseg, use_raw_dets) = input_flags

        self.crop_size = par.crop_size
        self.global_map_dim = par.global_map_dim
        self.observation_dim = par.observation_dim
        self.cell_size = par.cell_size
        self.sseg_labels = par.sseg_labels
        self.dets_nClasses = par.dets_nClasses
        self.orientations = par.orientations
        self.pad = par.pad
        self.loss_type = par.loss_type
        self.update_type = par.update_type
        self.window_size = getattr(par, 'window_size', 4)
        self.use_temporal_fusion = getattr(par, 'use_temporal_fusion', True)


        self.img_embedding = 64  # CLIP增强后的图像特征维度
        self.sseg_embedding = getattr(par, 'sseg_embedding', 16)
        self.dets_embedding = getattr(par, 'dets_embedding', 16)


        actual_embedding = 0
        if with_feat:
            actual_embedding += self.img_embedding
        if with_sseg:
            actual_embedding += self.sseg_embedding
        if with_dets:
            actual_embedding += self.dets_embedding

        
        self.map_embedding = actual_embedding
        print(f"动态计算的 map_embedding: {self.map_embedding}")

        # 如果原始参数设置了 map_embedding，给出警告
        if hasattr(par, 'map_embedding') and par.map_embedding != actual_embedding:
            print(
                f"警告: 参数中的 map_embedding ({par.map_embedding}) 与实际计算值 ({actual_embedding}) 不匹配，使用实际计算值")
        
    
        par.map_embedding = actual_embedding


        self.clip_extractor = CLIPFeatureExtractor(device="cuda")
        self.object_attn = ObjectAttentionLayer(d_model=self.img_embedding, nhead=4)  # 注意这里用img_embedding
        self.clip_proj = nn.Linear(512, self.img_embedding)  # CLIP特征投影到图像特征维度
        self.current_rgb_images = None

        # 置信度分支
        self.confidence_branch = nn.Sequential(
            nn.Conv2d(self.map_embedding, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, self.orientations, kernel_size=1),
            nn.Sigmoid()
        )

        # 时序Transformer
        if self.use_temporal_fusion and update_type == "lstm":
            self.temporal_transformer = TemporalTransformer(
                d_model=self.map_embedding,
                num_heads=4,
                num_layers=2,
                window_size=self.window_size
            )

        # 历史帧缓冲区初始化标志
        self.frame_buffer = None
        self.buffer_initialized = False

       
        if with_feat:
            self.resnet_feat_dim = 256
            fnet = models.resnet50(pretrained=True)
            self.ResNet50Truncated = nn.Sequential(*list(fnet.children())[:-5])

            self.small_cnn_img = nn.Sequential(
                nn.Conv2d(in_channels=self.resnet_feat_dim, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.img_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        if with_sseg and not (use_raw_sseg):
            self.small_cnn_sseg = nn.Sequential(
                nn.Conv2d(in_channels=self.sseg_labels, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.sseg_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        if with_dets and not (use_raw_dets):
            self.small_cnn_det = nn.Sequential(
                nn.Conv2d(in_channels=self.dets_nClasses, out_channels=64, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(in_channels=64, out_channels=self.dets_embedding, kernel_size=3, stride=1, padding=1),
                nn.ReLU(inplace=True)
            )

        
        if update_type == "lstm":
            self.lstm = nn.LSTM(input_size=self.map_embedding, hidden_size=self.map_embedding, num_layers=1)
        elif update_type == "fc":
            self.update_fc = nn.Linear(self.map_embedding * 2, self.map_embedding)
        else:  # 'avg'
            self.update_avg = nn.AvgPool1d(kernel_size=2)

        
        if self.loss_type != "BCE":
            self.loss_CEL = nn.CrossEntropyLoss(ignore_index=-1)


    def init_frame_buffer(self, batch_size):
        """初始化历史帧缓冲区"""
        self.frame_buffer = torch.zeros(
            batch_size,
            self.map_embedding,
            self.global_map_dim[0],
            self.global_map_dim[1],
            self.window_size,
            dtype=torch.float32
        ).cuda()
        self.buffer_initialized = True


    def update_frame_buffer(self, new_observation, batch_size):
        """更新滑动窗口缓冲区"""
        if not self.buffer_initialized:
            self.init_frame_buffer(batch_size)

        # 滑动窗口：移除最旧的帧，添加新帧
        self.frame_buffer[:, :, :, :, :-1] = self.frame_buffer[:, :, :, :, 1:].clone()
        self.frame_buffer[:, :, :, :, -1] = new_observation

    def get_temporal_features(self, current_frame_idx):
        """获取用于时序融合的特征"""
        if current_frame_idx < self.window_size - 1:
          
            valid_frames = current_frame_idx + 1
            return self.frame_buffer[:, :, :, :, :valid_frames]
        else:
           
            return self.frame_buffer

    def apply_temporal_fusion(self, features, current_frame_idx):
        """应用时序Transformer融合"""
        # features 的形状应该是 [B, d_model, H, W, valid_frames]
        batch_size, d_model, h, w, valid_frames = features.shape

        if current_frame_idx < self.window_size - 1:
            # 前几帧直接返回最新的特征（最后一帧）
            return features[:, :, :, :, -1]

        # 重组数据用于Transformer处理
        # [B, d_model, H, W, window_size] -> [B, H*W, window_size, d_model]
        temporal_features = features.permute(0, 2, 3, 4, 1).contiguous()
        temporal_features = temporal_features.view(batch_size, h * w, valid_frames, d_model)

        # 通过Transformer融合
        fused_features = self.temporal_transformer(temporal_features)

      
        fused_features = fused_features.view(batch_size, h, w, d_model)
        fused_features = fused_features.permute(0, 3, 1, 2).contiguous()

        return fused_features

    def extract_img_feat(self, img_data, batch_size):
        """修正后的特征提取函数"""
        # 存储原始RGB图像用于CLIP处理
        self.current_rgb_images = img_data.clone()

        
        img_feat = self.ResNet50Truncated(img_data)
        img_feat = F.interpolate(img_feat, size=(self.crop_size[1], self.crop_size[0]), mode='nearest')

        return img_feat

    # 初始化智能体在地图上的位置分布
    def init_p(self, batch_size):
        # Initialize the position (p0) in the center of the map at angle 0 (i.e. orientation index 0).
        p0 = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p0[:, 0, int(self.global_map_dim[0] / 2.0), int(self.global_map_dim[1] / 2.0)] = 1  # 14,14
        return torch.tensor(p0, dtype=torch.float32).cuda()

    def forward(self, local_info, update_type, input_flags, p_gt=None):
        # 为整个episod运行 MapNet
        (img_data, points2D, local3D, sseg, dets) = local_info
        batch_size = img_data.shape[0]
        seq_len = img_data.shape[1]

        # 提取图片特征
        with_feat = input_flags[0]
        if with_feat:
            # *** 关键修正：保持原始形状用于CLIP处理 ***
            original_img_data = img_data.clone()  # [B, T, 3, H, W]

            img_data_flat = img_data.view(batch_size * seq_len, 3, self.crop_size[1], self.crop_size[0])
            img_feat = self.extract_img_feat(img_data_flat, batch_size)
            img_feat = img_feat.view(batch_size, seq_len, self.resnet_feat_dim, self.crop_size[1], self.crop_size[0])
        else:
            original_img_data = None
            img_feat = torch.zeros(batch_size, seq_len, 1, self.crop_size[1], self.crop_size[0])

        # 地面投影 - 传递原始图像
        grid, map_occ = self.groundProjection(
            img_feat_all=img_feat,
            points2D_all=points2D,
            local3D_all=local3D,
            sseg_all=sseg,
            dets_all=dets,
            batch_size=batch_size,
            seq_len=seq_len,
            input_flags=input_flags,
            original_img_data=original_img_data
        )

        # 旋转网格的特征通道以获取旋转堆栈
        grid_packed = grid.view(batch_size * seq_len, self.map_embedding, self.observation_dim[0], self.observation_dim[1])
        rotation_stack_packed = self.rotational_sampler(grid=grid_packed)
        rotation_stack = rotation_stack_packed.view(batch_size, seq_len, self.map_embedding, self.orientations, self.observation_dim[0], self.observation_dim[1])

        # p_pred 是预测的位姿分布；map_pred用于存储地图特征的预测值
        p_pred = np.zeros((batch_size, seq_len, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p_pred = torch.tensor(p_pred, dtype=torch.float32).cuda()
        map_pred = np.zeros((batch_size, seq_len, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        map_pred = torch.tensor(map_pred, dtype=torch.float32).cuda()

        confidence_pred = np.zeros((batch_size, seq_len, self.orientations,
                                    self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        confidence_pred = torch.tensor(confidence_pred, dtype=torch.float32).cuda()

        self.buffer_initialized = False
        for q in range(seq_len):
            rotation_stack_step = rotation_stack[:, q, :, :, :, :]
            if q == 0:
                p_ = self.init_p(batch_size=batch_size).clone()
                map_next = self.register_observation(rotation_stack=rotation_stack_step, p=p_, batch_size=batch_size)

                # 使用confidence_branch计算置信度
                confidence_ = self.confidence_branch(map_next)
            else:
                map_previous = map_next.clone()
                p_, base_confidence = self.position_prediction(rotation_stack=rotation_stack_step,
                                                               map_previous=map_previous, batch_size=batch_size)

                if p_gt is not None:
                    reg_obsv = self.register_observation(rotation_stack=rotation_stack_step, p=p_gt[:, q, :, :, :], batch_size=batch_size)
                else:
                    reg_obsv = self.register_observation(rotation_stack=rotation_stack_step, p=p_, batch_size=batch_size)

                # 传递当前帧索引用于时序融合
                map_next = self.update_map(reg_obsv, map_previous, batch_size=batch_size,
                                           update_type=update_type, current_frame_idx=q)

                # 使用confidence_branch计算置信度，结合基础置信度
                network_confidence = self.confidence_branch(map_next)
                confidence_ = 0.7 * network_confidence + 0.3 * base_confidence

            p_pred[:, q, :, :, :] = p_
            confidence_pred[:, q, :, :, :] = confidence_
            map_pred[:, q, :, :, :] = map_next

        return p_pred, map_pred, confidence_pred

    def build_loss(self, p_pred, confidence_pred, p_gt):
        batch_size = p_pred.shape[0]
        seq_len = p_pred.shape[1]

        # 移除第一帧
        p_pred = p_pred[:, 1:, :, :, :]
        confidence_pred = confidence_pred[:, 1:, :, :, :]
        p_gt = p_gt[:, 1:, :, :, :]

        if self.loss_type == "BCE":
            # 不确定性感知的BCE loss
            p_pred_flat = p_pred.contiguous().view(batch_size * (seq_len - 1),
                                                   self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            p_gt_flat = p_gt.contiguous().view(batch_size * (seq_len - 1),
                                               self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            confidence_flat = confidence_pred.contiguous().view(batch_size * (seq_len - 1),
                                                                self.orientations, self.global_map_dim[0], self.global_map_dim[1])

            # 主要定位损失（由置信度加权）
            localization_loss = F.binary_cross_entropy_with_logits(p_pred_flat, p_gt_flat, reduction='none')
            weighted_loss = localization_loss / (confidence_flat + 1e-6)  # 高置信度 = 高权重

            # 置信度正则化损失（防止过度自信）
            confidence_reg = torch.mean(confidence_flat)  # 鼓励适度的置信度

            # 总损失
            total_loss = torch.mean(weighted_loss) + 0.1 * confidence_reg

        elif self.loss_type == "CEL":
            # 交叉熵损失的不确定性感知版本
            p_pred_flat = p_pred.contiguous().view(batch_size * (seq_len - 1), -1)
            p_gt_flat = p_gt.contiguous().view(batch_size * (seq_len - 1), -1)
            confidence_flat = confidence_pred.contiguous().view(batch_size * (seq_len - 1),
                                                                self.orientations, self.global_map_dim[0], self.global_map_dim[1])

            # 将真值转换为类别索引
            p_gt_indices = torch.argmax(p_gt_flat, dim=1)

            # 计算交叉熵损失
            ce_loss = F.cross_entropy(p_pred_flat, p_gt_indices, reduction='none')

            # 置信度加权（取每个样本的平均置信度）
            confidence_weight = torch.mean(confidence_flat.view(batch_size * (seq_len - 1), -1), dim=1)
            weighted_loss = ce_loss / (confidence_weight + 1e-6)

            # 置信度正则化
            confidence_reg = torch.mean(confidence_weight)

            # 总损失
            total_loss = torch.mean(weighted_loss) + 0.1 * confidence_reg

        else:
            raise ValueError(f"Unsupported loss type: {self.loss_type}")

        return total_loss

    def register_observation(self, rotation_stack, p, batch_size, confidence=None):
        reg_obsv = np.zeros((batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        reg_obsv = torch.tensor(reg_obsv, dtype=torch.float32).cuda()

        for i in range(batch_size):
            filt = rotation_stack[i, :, :, :, :]
            filt = filt.permute(1, 0, 2, 3)
            p_in = p[i, :, :, :].unsqueeze(0)

            # 如果提供了置信度，则进行加权
            if confidence is not None:
                confidence_weight = confidence[i, :, :, :].unsqueeze(0)
                p_in = p_in * confidence_weight  # 置信度加权

            reg = F.conv_transpose2d(input=p_in, weight=filt, padding=self.pad)
            reg_obsv[i, :, :, :] = reg

        return reg_obsv

    def position_prediction(self, rotation_stack, map_previous, batch_size):
        # Do the cross correlation with the existing map
        corr_map = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        corr_map = torch.tensor(corr_map, dtype=torch.float32).cuda()

        for b in range(batch_size):
            map_ = map_previous[b, :, :, :].unsqueeze(0)  # 1 x n x h x w
            for r in range(self.orientations):
                # convolve each filter with the previous map
                filt = rotation_stack[b, :, r, :, :].unsqueeze(0)  # 1 x n x s x s
                corr_tmp = F.conv2d(input=map_, weight=filt, padding=self.pad)
                corr_map[b, r, :, :] = corr_tmp

        p_ = np.zeros((batch_size, self.orientations, self.global_map_dim[0], self.global_map_dim[1]), dtype=np.float32)
        p_ = torch.tensor(p_, dtype=torch.float32).cuda()

        for i in range(batch_size):
            p_tmp = corr_map[i, :, :, :].view(-1)
            p_tmp = p_tmp.view(self.orientations, self.global_map_dim[0], self.global_map_dim[1])
            p_[i, :, :, :] = p_tmp

        # 置信度估计
        confidence_map = torch.zeros_like(p_)
        for i in range(batch_size):
            for r in range(self.orientations):
                corr_values = corr_map[i, r, :, :]
                # 计算每个位置的置信度
                max_response = torch.max(corr_values)
                mean_response = torch.mean(corr_values)
                std_response = torch.std(corr_values)

                # 置信度公式：结合峰值强度和分布集中度
                confidence = (max_response - mean_response) / (std_response + 1e-6)
                confidence_map[i, r, :, :] = torch.sigmoid(confidence)

        return p_, confidence_map
    
    def forward_single_step(self, local_info, t, input_flags, map_previous=None, p_given=None, update_type="lstm"):
        (img_data, points2D, local3D, sseg, dets) = local_info
        batch_size = img_data.shape[0]

        with_feat = input_flags[0]
        if with_feat:
            # *** 保存原始RGB图像用于CLIP ***
            self.current_rgb_images = img_data.clone()
            img_feat = self.extract_img_feat(img_data, batch_size)
        else:
            self.current_rgb_images = None
            img_feat = torch.zeros(batch_size, 1, self.crop_size[1], self.crop_size[0])

        # 地面投影处理
        grid = torch.zeros((batch_size, self.map_embedding, self.observation_dim[0], self.observation_dim[1]),
                           dtype=torch.float32).cuda()

        for b in range(batch_size):
            points2D_step = points2D[b]
            local3D_step = local3D[b]
            img_feat_step = img_feat[b, :, :, :].unsqueeze(0)
            sseg_step = sseg[b, :, :, :].unsqueeze(0)
            dets_step = dets[b, :, :, :].unsqueeze(0)


            rgb_image = None
            if self.current_rgb_images is not None:
                rgb_image = self.current_rgb_images[b]  # [3, H, W]

            grid_step, map_occ_step = self.groundProjectionStep(
                img_feat=img_feat_step,
                points2D=points2D_step,
                local3D=local3D_step,
                sseg=sseg_step,
                dets=dets_step,
                input_flags=input_flags,
                rgb_image=rgb_image  
            )
            grid[b, :, :, :] = grid_step.squeeze(0)



        rotation_stack = self.rotational_sampler(grid=grid)

        if t == 0:
            # 第一帧：重置缓冲区
            self.buffer_initialized = False
            p_ = self.init_p(batch_size=batch_size).clone()
            confidence_ = torch.ones_like(p_) * 0.5
            map_next = self.register_observation(rotation_stack=rotation_stack, p=p_,
                                                 batch_size=batch_size, confidence=confidence_)
        else:
            if p_given is None:
                p_, confidence_ = self.position_prediction(rotation_stack=rotation_stack,
                                                           map_previous=map_previous, batch_size=batch_size)
            else:
                p_ = p_given
                confidence_ = torch.ones_like(p_) * 0.8

            reg_obsv = self.register_observation(rotation_stack=rotation_stack, p=p_,
                                                 batch_size=batch_size, confidence=confidence_)

            # 传递时间步索引用于时序融合
            map_next = self.update_map(reg_obsv, map_previous, batch_size=batch_size,
                                       update_type=update_type, current_frame_idx=t)


        return p_, confidence_, map_next  # 现在返回三个值：位置、置信度、地图

    def update_map(self, reg_obsv, map_previous, batch_size, update_type, current_frame_idx=0):
        """更新地图，支持多帧时序融合"""

        if update_type == "lstm" and self.use_temporal_fusion:
     
            self.update_frame_buffer(reg_obsv, batch_size)

            # 获取时序特征并应用融合
            temporal_features = self.get_temporal_features(current_frame_idx)

            if current_frame_idx >= self.window_size - 1:
                
                fused_reg_obsv = self.apply_temporal_fusion(temporal_features, current_frame_idx)
            else:
                
                fused_reg_obsv = reg_obsv

            # 使用融合后的特征进行LSTM更新
            map_next = torch.zeros(
                (batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]),
                dtype=torch.float32
            ).cuda()

            # LSTM更新每个空间位置
            for i in range(self.global_map_dim[0]):
                for j in range(self.global_map_dim[1]):
                    emb_in = fused_reg_obsv[:, :, i, j]  # b x n
                    emb_hidden = map_previous[:, :, i, j]  # b x n
                    emb_in = emb_in.unsqueeze(0)  # 添加序列长度维度
                    emb_hidden = emb_hidden.unsqueeze(0)  # 添加LSTM层数维度
                    hidden = (emb_hidden.contiguous(), emb_hidden.contiguous())
                    lstm_out, hidden_out = self.lstm(emb_in, hidden)
                    map_next[:, :, i, j] = lstm_out.squeeze(0)

        elif update_type == "fc":
            # FC更新保持不变
            map2 = torch.cat((map_previous, reg_obsv), 1)
            map_next = self.update_fc(map2.permute(0, 2, 3, 1))
            map_next = torch.tanh(map_next)
            map_next = map_next.permute(0, 3, 1, 2)

        else:  # 'avg'
            # 平均池化更新保持不变
            map_next = torch.zeros(
                (batch_size, self.map_embedding, self.global_map_dim[0], self.global_map_dim[1]),
                dtype=torch.float32
            ).cuda()
            for i in range(self.global_map_dim[0]):
                for j in range(self.global_map_dim[1]):
                    vec1 = reg_obsv[:, :, i, j].unsqueeze(2)
                    vec2 = map_previous[:, :, i, j].unsqueeze(2)
                    vec = torch.cat((vec1, vec2), 2)
                    avg_out = self.update_avg(vec).squeeze(2)
                    map_next[:, :, i, j] = avg_out
            map_next = torch.tanh(map_next)

        return map_next


    def rotational_sampler(self, grid, rot_init=True):
        # The grid (after the groundProjection) is facing up
        # We need to rotate it so as to face to the right (angle 0)
        if rot_init:
            grid = self.do_rotation(grid, angle=-np.pi/2.0)
        # Rotate the grid's feature channels to obtain the rotational stack
        rotation_stack = np.zeros((grid.shape[0], self.map_embedding, self.orientations, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32 )
        rotation_stack = torch.tensor(rotation_stack, dtype=torch.float32).cuda()
        for i in range(self.orientations):
            angle = 2*np.pi*(i/self.orientations)
            rotation_stack[:,:,i,:,:] = self.do_rotation(grid, angle) #grid_trans
        return rotation_stack


    def do_rotation(self, grid, angle):
        theta = torch.tensor([
            [np.cos(angle), -np.sin(angle), 0],
            [np.sin(angle), np.cos(angle), 0]
        ], dtype=torch.float32).cuda()
        theta = theta.unsqueeze(0).repeat(grid.shape[0], 1, 1)
        grid_affine = F.affine_grid(theta, grid.size(), align_corners=True)  # Set align_corners=True
        return F.grid_sample(grid, grid_affine, align_corners=True)  # Set align_corners=True

    def groundProjectionStep(self, img_feat, points2D, local3D, sseg, dets, input_flags, rgb_image=None):
        """修正后的地面投影函数"""
        (with_feat, with_sseg, with_dets, use_raw_sseg, use_raw_dets) = input_flags

        # 地面投影基础处理
        map_coords, valid, map_occ = dh.discretize_coords(
            x=local3D[:, 0], z=local3D[:, 2], map_dim=self.observation_dim, cell_size=self.cell_size)
        map_occ = torch.tensor(map_occ, dtype=torch.float32).cuda()

        points2D = points2D[valid, :]
        map_coords = map_coords[valid, :]
        grids = []

        # ========== CLIP特征交互 ==========
        if with_feat and rgb_image is not None:
            try:
                # 1. 获取ResNet特征并转换为地图特征
                grid_img = self.bin_pooling(img_feat, points2D, map_coords)
                grid_img_in = grid_img.unsqueeze(0)
                map_features = self.small_cnn_img(grid_img_in.cuda())  # [1, img_embedding, H, W]

                # 2. 获取CLIP文本特征
                text_features = self.clip_extractor.text_features  # [num_obj, 512]
                text_features = text_features.to(dtype=torch.float32)
                text_features_proj = self.clip_proj(text_features)  # [num_obj, img_embedding]

                # 3. 获取CLIP图像特征
                with torch.no_grad():
                    clip_result = self.clip_extractor.get_relevant_objects(rgb_image.unsqueeze(0))
                    clip_img_feat = clip_result['image_features']  # [1, 512] 或 [1, 1, 512]
                    relevance_mask = clip_result['relevance_mask']  # [1, num_obj] 或 [1, 1, num_obj]

                    # 处理维度
                    if len(clip_img_feat.shape) == 3:
                        clip_img_feat = clip_img_feat.squeeze(1)
                    if len(relevance_mask.shape) == 3:
                        relevance_mask = relevance_mask.squeeze(1)

                    clip_img_feat = clip_img_feat.to(dtype=torch.float32)
                    clip_img_feat_proj = self.clip_proj(clip_img_feat)  # [1, img_embedding]

                # 4. 处理相关类别
                relevant_indices = torch.where(relevance_mask[0])[0]

                if relevant_indices.numel() > 0:
                    relevant_text_features = text_features_proj[relevant_indices]
                    object_features = torch.cat([
                        relevant_text_features.unsqueeze(0),
                        clip_img_feat_proj.unsqueeze(1)
                    ], dim=1)

                    img_relevance = torch.ones(1, 1, device=relevance_mask.device, dtype=relevance_mask.dtype)
                    extended_relevance_mask = torch.cat([
                        relevance_mask[:, relevant_indices],
                        img_relevance
                    ], dim=1)
                else:
                    object_features = clip_img_feat_proj.unsqueeze(0).unsqueeze(1)
                    extended_relevance_mask = torch.ones(1, 1, device=clip_img_feat_proj.device, dtype=torch.bool)

                # 5. 应用注意力机制
                B, d_model, H, W = map_features.shape
                map_features_flat = map_features.permute(0, 2, 3, 1).reshape(B, H * W, d_model)

                enhanced_map_features = self.object_attn(
                    map_features_flat,
                    object_features,
                    relevance_mask=extended_relevance_mask
                )

                enhanced_map_features = enhanced_map_features.view(B, H, W, d_model).permute(0, 3, 1, 2)
                grids.append(enhanced_map_features)

            except Exception as e:
                print(f"CLIP处理出错，使用普通特征: {e}")
                # Fallback到普通特征处理
                grid_img = self.bin_pooling(img_feat, points2D, map_coords)
                grid_img_in = grid_img.unsqueeze(0)
                map_features = self.small_cnn_img(grid_img_in.cuda())
                grids.append(map_features)

        elif with_feat:
            # 没有RGB图像时的处理
            grid_img = self.bin_pooling(img_feat, points2D, map_coords)
            grid_img_in = grid_img.unsqueeze(0)
            map_features = self.small_cnn_img(grid_img_in.cuda())
            grids.append(map_features)

        # ========== 其他特征处理 ==========
        if with_sseg:
            grid_sseg = self.label_pooling(sseg, points2D, map_coords)
            grid_sseg_in = grid_sseg.unsqueeze(0)
            if use_raw_sseg:
                grids.append(grid_sseg_in)
            else:
                sseg_processed = self.small_cnn_sseg(grid_sseg_in)
                grids.append(sseg_processed)

        if with_dets:
            grid_det = self.dets_pooling(dets, points2D, map_coords)
            grid_det_in = grid_det.unsqueeze(0)
            if use_raw_dets:
                grids.append(grid_det_in)
            else:
                det_processed = self.small_cnn_det(grid_det_in)
                grids.append(det_processed)

        if len(grids) == 0:
            raise Exception("No input grids!")

        # 拼接特征
        grid_out = torch.cat(grids, 1).cuda()



        # 如果维度仍然不匹配，添加一个适配层
        if grid_out.shape[1] != self.map_embedding:
            if not hasattr(self, 'feature_adapter'):
                self.feature_adapter = nn.Conv2d(grid_out.shape[1], self.map_embedding, 1).cuda()
            grid_out = self.feature_adapter(grid_out)
            #print(f"通过适配层调整后的维度: {grid_out.shape}")

        return grid_out, map_occ



    def bin_pooling(self, img_feat, points2D, map_coords):
        # Bin pooling during ground projection of the features
        grid = np.zeros((img_feat.shape[1], self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        grid = torch.tensor(grid, dtype=torch.float32).cuda()
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_feat = img_feat[0, :, pix_y, pix_x]
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]  # indices of where ucoord can be found in map_coords
            # Features indices in the ind array belong to the same bin and have to be max-pooled
            bin_feats = pix_feat[:, ind]  # [d x n] n:number of feature vectors projected, d:feat_dim
            bin_feat, _ = torch.max(bin_feats, 1)  # [d]
            grid[:, ucoord[1], ucoord[0]] = bin_feat
        return grid

        # 对所有投影到同一个地图格点的像素标签，统计各类别出现的频率，得到该格点的语义概率分布
    def label_pooling(self, sseg, points2D, map_coords):
        # Similar to bin_pooling() but instead features we pool the semantic labels
        # For each bin get the frequencies of the class labels based on the labels projected
        # Each grid location will hold a probability distribution over the semantic labels
        grid = np.zeros((self.sseg_labels, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_lbl = sseg[0, 0, pix_y, pix_x]
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]  # indices of where ucoord can be found in map_coords
            bin_lbls = pix_lbl[ind]
            # Labels are from 0-39 where 0:wall ... 39:other_prop
            # hist, bins = np.histogram(bin_lbls, bins=list(range(self.sseg_labels+1)))
            bin_lbls = bin_lbls.cpu() if bin_lbls.is_cuda else bin_lbls
            hist, bins = np.histogram(bin_lbls.numpy(), bins=list(range(self.sseg_labels + 1)))
            # 修改后
            hist = hist / float(bin_lbls.shape[0])
            grid[:, ucoord[1], ucoord[0]] = hist
        grid = torch.tensor(grid, dtype=torch.float32).cuda()
        return grid

    # 对所有投影到同一个地图格点的检测分数（每个类别），进行均值池化（average pooling），得到该格点的检测分数分布
    def dets_pooling(self, dets, points2D, map_coords):
        # Bin pooling of the detection masks. Detection scores in the same bin are averaged.
        grid = np.zeros((self.dets_nClasses, self.observation_dim[0], self.observation_dim[1]), dtype=np.float32)
        pix_x, pix_y = points2D[:, 0], points2D[:, 1]
        pix_dets = dets[0, :, pix_y, pix_x]
        if pix_dets.is_cuda:
            pix_dets = pix_dets.cpu()  # 确保张量在 CPU 上
        uniq_rows = np.unique(map_coords, axis=0)
        for i in range(uniq_rows.shape[0]):
            ucoord = uniq_rows[i, :]
            ind = np.where((map_coords == ucoord).all(axis=1))[0]  # 寻找坐标在 map_coords 中的索引
            bin_dets = pix_dets[:, ind]  # 所有在同一格中的检测结果
            bin_det = bin_dets.mean(1)
            grid[:, ucoord[1], ucoord[0]] = bin_det.numpy()  # 确保使用.numpy()之前，bin_det在 CPU 上
        grid = torch.tensor(grid, dtype=torch.float32).cuda()  # 将结果转回 GPU

        return grid

    def groundProjection(self, img_feat_all, points2D_all, local3D_all, sseg_all, dets_all, batch_size, seq_len,
                         input_flags, original_img_data=None):
        """修正后的批量地面投影"""
        grid = torch.zeros((batch_size, seq_len, self.map_embedding, self.observation_dim[0], self.observation_dim[1]),
                           dtype=torch.float32).cuda()
        map_occ = torch.zeros((batch_size, seq_len, 1, self.observation_dim[0], self.observation_dim[1]),
                              dtype=torch.float32).cuda()

        for b in range(batch_size):
            points2D_seq = points2D_all[b]
            local3D_seq = local3D_all[b]
            for q in range(seq_len):
                points2D_step = points2D_seq[q]
                local3D_step = local3D_seq[q]
                img_feat_step = img_feat_all[b, q, :, :, :].unsqueeze(0)
                sseg_step = sseg_all[b, q, :, :, :].unsqueeze(0)
                dets_step = dets_all[b, q, :, :, :].unsqueeze(0)

          
                rgb_image = None
                if original_img_data is not None:
                    rgb_image = original_img_data[b, q]  # [3, H, W]

                grid_step, map_occ_step = self.groundProjectionStep(
                    img_feat=img_feat_step,
                    points2D=points2D_step,
                    local3D=local3D_step,
                    sseg=sseg_step,
                    dets=dets_step,
                    input_flags=input_flags,
                    rgb_image=rgb_image
                )
                grid[b, q, :, :, :] = grid_step.squeeze(0)
                map_occ[b, q, :, :, :] = map_occ_step.squeeze(0)
        return grid, map_occ

</pre>
        </div>
      </div>
    </div>
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">train_AEP.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
# 代码最后的输出是模型在训练数据上预测的姿态 p_pred 和地图 map_pred
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import os
import numpy as np
import random
import data_helper as dh
import helper as hl
from MapNet_transformer_uncertain_clip_1 import MapNet_transformer_uncertain_clip_1
from parameters import ParametersMapNet
from dataloader import AVD
from test_MapNet import evaluate_MapNet


# 将数据集中一批样本抽取出来，整理成模型需要的格式，并放到GPU上，为后续训练或推理做准备。
def get_minibatch(par, data, ex_ids, data_index):
    imgs_batch = torch.zeros(par.batch_size, par.seq_len, 3, par.crop_size[1], par.crop_size[0])
    pose_gt_batch = np.zeros((par.batch_size, par.seq_len, 3), dtype=np.float32)
    sseg_batch = torch.zeros(par.batch_size, par.seq_len, 1, par.crop_size[1], par.crop_size[0])
    dets_batch = torch.zeros(par.batch_size, par.seq_len, par.dets_nClasses, par.crop_size[1], par.crop_size[0])
    points2D_batch, local3D_batch = [], []
    for k in range(par.batch_size):
        ex = data[ex_ids[data_index + k]]
        imgs_seq = ex["images"]
        points2D_seq = ex["points2D"]
        local3D_seq = ex["local3D"]
        pose_gt_seq = ex["pose"]
        sseg_seq = ex["sseg"]
        dets_seq = ex["dets"]
        imgs_batch[k, :, :, :, :] = imgs_seq
        pose_gt_batch[k, :, :] = pose_gt_seq
        sseg_batch[k, :, :, :, :] = sseg_seq
        dets_batch[k, :, :, :, :] = dets_seq
        points2D_batch.append(points2D_seq)  # nested list of batch_size x seq_len x n_points x 2
        local3D_batch.append(local3D_seq)  # nested list of batch_size x seq_len x n_points x 3
    return (imgs_batch.cuda(), points2D_batch, local3D_batch, pose_gt_batch, sseg_batch.cuda(), dets_batch.cuda())


if __name__ == '__main__':
    par = ParametersMapNet()
    mapNet_model = MapNet_transformer_uncertain_clip_1(par, update_type=par.update_type, input_flags=par.input_flags)
    mapNet_model.cuda()
    print("Model on GPU:", next(mapNet_model.parameters()).is_cuda)
    mapNet_model.train()
    optimizer = optim.Adam(mapNet_model.parameters(), lr=par.lr_rate)
    scheduler = StepLR(optimizer, step_size=par.step_size, gamma=par.gamma)  

    print("Loading the training data...")
    avd = AVD(par, seq_len=par.seq_len, nEpisodes=par.epi_per_scene,
              scene_list=par.train_scene_list, action_list=par.action_list, with_shortest_path=par.with_shortest_path)

    log = open(par.model_dir + "train_log_" + par.model_id + ".txt", 'w')
    hl.save_params(par, par.model_dir, name="mapNet")
    loss_list = []

    all_ids = list(range(len(avd)))
    test_ids = all_ids[::100]
    train_ids = list(set(all_ids) - set(test_ids))

    nData = len(train_ids)
    iters_per_epoch = int(nData / float(par.batch_size))
    log.write("Iters_per_epoch:" + str(iters_per_epoch) + "\n")
    print("Iters per epoch:", iters_per_epoch)
    for ep in range(par.nEpochs):
        # 随机打乱训练数据并获取小批量数据进行训练
        random.shuffle(train_ids)
        data_index = 0
        for i in range(iters_per_epoch):
            iters = i + ep * iters_per_epoch

            # 对训练小批量进行采样
            batch = get_minibatch(par, data=avd, ex_ids=train_ids, data_index=data_index)
            (imgs_batch, points2D_batch, local3D_batch, pose_gt_batch, sseg_batch, dets_batch) = batch

            p_gt_batch = dh.build_p_gt(par, pose_gt_batch)
            data_index += par.batch_size

            # 执行 mapNet 的前向forward
            local_info = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch)
            p_pred, map_pred, confidence_pred = mapNet_model(local_info, update_type=par.update_type,
                                                             input_flags=par.input_flags, p_gt=None)

            loss = mapNet_model.build_loss(p_pred, confidence_pred, p_gt_batch)
            optimizer.zero_grad()
            loss.backward()
            total_norm = 0
            for p in mapNet_model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** (1. / 2)

            if iters % par.show_interval == 0:
                print(f"Gradient norm: {total_norm}")
            optimizer.step()



            if iters % par.show_interval == 0:
                current_lr = optimizer.param_groups[0]['lr']
                log.write(f"Epoch:{ep} ITER:{iters} Loss:{loss.data.item()} LR:{current_lr}\n")
                print(f"Epoch: {ep} ITER: {iters} Loss: {loss.data.item()} LR: {current_lr}")

            if iters > 0:
                loss_list.append(loss.data.item())
            if iters % par.plot_interval == 0 and iters > 0:
                hl.plot_loss(loss=loss_list, epoch=ep, iteration=iters, step=1, loss_name="NLL", loss_dir=par.model_dir)

            if iters % par.save_interval == 0:
                hl.save_model(model=mapNet_model, model_dir=par.model_dir, model_name="MapNet", train_iter=iters)

            if iters % par.test_interval == 0:
                evaluate_MapNet(par, test_iter=iters, test_ids=test_ids, test_data=avd)
        scheduler.step()



</pre>
        </div>
      </div>
    </div>
    <!-- 继续添加更多代码块，如下 -->
    <!-- code3.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
import os
import numpy as np


class ILNet(nn.Module):
    # Definition of the navigation network for imitation learning
    # Receives as input the mapnet state and egocentric observations, and outputs action costs.
    def __init__(self, par, map_in_embedding, map_orient, tvec_dim, nActions, use_ego_obsv, drop_rate=0.2):
        super(ILNet, self).__init__()
        self.map_in_embedding = map_in_embedding
        self.orientations = map_orient
        self.fc_dim = par.fc_dim
        # Small cnn to take the map and extract an embedding
        self.map_cnn = nn.Sequential(
            nn.Conv2d(in_channels=map_in_embedding, out_channels=par.conv_embedding, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(num_features=par.conv_embedding),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, stride=2)
        )
        self.fc_map = nn.Linear(13*13*8, par.fc_dim) # 13*13*8 is the output dim of the small cnn
        self.relu_map = nn.ReLU(inplace=True)
        self.drop_map = nn.Dropout(p=drop_rate)
        # Small cnn to take the position prediction and extract an embedding
        self.p_cnn = nn.Sequential(
            nn.Conv2d(in_channels=map_orient, out_channels=par.conv_embedding, kernel_size=3, stride=1, padding=0),
            nn.BatchNorm2d(num_features=par.conv_embedding),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc_p = nn.Linear(13*13*8, par.fc_dim) # 13*13*8 is the output dim of the small cnn
        self.relu_p = nn.ReLU(inplace=True)
        self.drop_p = nn.Dropout(p=drop_rate)
        # Extract an embedding from the target one-hot vector
        self.fc_t = nn.Linear(tvec_dim, par.fc_dim)
        self.relu_t = nn.ReLU(inplace=True)
        self.drop_t = nn.Dropout(p=drop_rate)
        state_items = 3
        # if we are using egocentric observations
        if use_ego_obsv:
            self.fc_ego = nn.Linear(512, par.fc_dim) # 512 is the output dim of resnet18
            self.relu_ego = nn.ReLU(inplace=True)
            self.drop_ego = nn.Dropout(p=drop_rate)
            state_items = 4
        # Pass the concatenated inputs into an LSTM
        self.lstm = nn.LSTM(input_size=par.fc_dim*state_items+1, hidden_size=par.fc_dim*state_items+1, num_layers=1)
        # The fc layer that maps from hidden state space to action space 
        self.fc_action = nn.Linear(par.fc_dim*state_items+1, nActions)
        self.hidden = None
        # Tried both L1 and MSE losses, both work almost the same
        self.cost_loss = nn.L1Loss() #nn.MSELoss()


    def init_hidden(self, batch_size, state_items):
        return (torch.zeros(1, batch_size, self.fc_dim*state_items+1).cuda(), 
                torch.zeros(1, batch_size, self.fc_dim*state_items+1).cuda())

    def build_loss(self, cost_pred, cost_gt, loss_weight):
        loss = loss_weight * self.cost_loss(cost_pred, cost_gt)
        return loss


    def forward(self, state, use_ego_obsv):
        if use_ego_obsv:
            (map_pred, p_pred, t_vec, collision, ego_obsv) = state
        else:
            (map_pred, p_pred, t_vec, collision) = state
        if len(map_pred.size()) == 4: # add the sequence dimension
            map_pred = map_pred.unsqueeze(1)
            p_pred = p_pred.unsqueeze(1)

        batch_size = map_pred.shape[0]
        seq_len = map_pred.shape[1]
        global_map_dim = (map_pred.shape[3], map_pred.shape[4])
        

        actual_map_embedding = map_pred.shape[2]
        if actual_map_embedding != self.map_in_embedding:
            print(f"维度不匹配警告: 期望map_embedding={self.map_in_embedding}, 实际={actual_map_embedding}")
            print(f"map_pred形状: {map_pred.shape}")
            if not hasattr(self, 'map_adapter'):
                print(f"创建维度适配器: {actual_map_embedding} -> {self.map_in_embedding}")
                self.map_adapter = nn.Conv2d(actual_map_embedding, self.map_in_embedding, 1).cuda()
            

            map_pred_reshaped = map_pred.view(batch_size*seq_len, actual_map_embedding, global_map_dim[0], global_map_dim[1])
            map_pred = self.map_adapter(map_pred_reshaped)
            map_pred = map_pred.view(batch_size, seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        
        ## get map embedding
        map_pred = map_pred.view(batch_size*seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        map_cnn_out = self.map_cnn(map_pred)
        map_out = self.fc_map(map_cnn_out.view(map_cnn_out.shape[0], -1))
        map_out = self.relu_map(map_out)
        map_out = self.drop_map(map_out)
        map_out = map_out.view(batch_size, seq_len, -1)
        ## get position embedding
        p_pred = p_pred.view(batch_size*seq_len, self.orientations, global_map_dim[0], global_map_dim[1])
        p_cnn_out = self.p_cnn(p_pred)
        p_out = self.fc_p(p_cnn_out.view(p_cnn_out.shape[0], -1))
        p_out = self.relu_p(p_out)
        p_out = self.drop_p(p_out)
        p_out = p_out.view(batch_size, seq_len, -1)
        ## get target vector embedding
        t_out = self.fc_t(t_vec)
        t_out = self.relu_t(t_out)
        t_out = self.drop_t(t_out)
        t_out = t_out.unsqueeze(1).repeat(1, seq_len, 1) # replicate the target vec for each step in the sequence
        collision = collision.view(batch_size, seq_len, 1)
        if use_ego_obsv:
            ## get egocentric observation embedding
            ego_obsv = ego_obsv.view(batch_size*seq_len, -1)
            ego_out = self.fc_ego(ego_obsv)
            ego_out = self.relu_ego(ego_out)
            ego_out = self.drop_ego(ego_out)
            ego_out = ego_out.view(batch_size, seq_len, -1)
            x = torch.cat((map_out, p_out, t_out, collision, ego_out), 2)
        else:
            x = torch.cat((map_out, p_out, t_out, collision), 2)
        x = x.permute(1,0,2)
        lstm_out, self.hidden = self.lstm(x, self.hidden)
        lstm_out = lstm_out.permute(1,0,2)
        action_costs = self.fc_action(lstm_out) # batch_size x seq_len x nActions
        return action_costs
    
    def get_features(self, state, use_ego_obsv):
        """
        获取状态特征（LSTM输出），用于对比学习
        """
        if use_ego_obsv:
            (map_pred, p_pred, t_vec, collision, ego_obsv) = state
        else:
            (map_pred, p_pred, t_vec, collision) = state
        if len(map_pred.size()) == 4: # add the sequence dimension
            map_pred = map_pred.unsqueeze(1)
            p_pred = p_pred.unsqueeze(1)

        batch_size = map_pred.shape[0]
        seq_len = map_pred.shape[1]
        global_map_dim = (map_pred.shape[3], map_pred.shape[4])
        
        actual_map_embedding = map_pred.shape[2]
        if actual_map_embedding != self.map_in_embedding:
            if not hasattr(self, 'map_adapter'):
                self.map_adapter = nn.Conv2d(actual_map_embedding, self.map_in_embedding, 1).cuda()
            map_pred_reshaped = map_pred.view(batch_size*seq_len, actual_map_embedding, global_map_dim[0], global_map_dim[1])
            map_pred = self.map_adapter(map_pred_reshaped)
            map_pred = map_pred.view(batch_size, seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        
        map_pred = map_pred.view(batch_size*seq_len, self.map_in_embedding, global_map_dim[0], global_map_dim[1])
        map_cnn_out = self.map_cnn(map_pred)
        map_out = self.fc_map(map_cnn_out.view(map_cnn_out.shape[0], -1))
        map_out = self.relu_map(map_out)
        map_out = self.drop_map(map_out)
        map_out = map_out.view(batch_size, seq_len, -1)
        p_pred = p_pred.view(batch_size*seq_len, self.orientations, global_map_dim[0], global_map_dim[1])
        p_cnn_out = self.p_cnn(p_pred)
        p_out = self.fc_p(p_cnn_out.view(p_cnn_out.shape[0], -1))
        p_out = self.relu_p(p_out)
        p_out = self.drop_p(p_out)
        p_out = p_out.view(batch_size, seq_len, -1)
        t_out = self.fc_t(t_vec)
        t_out = self.relu_t(t_out)
        t_out = self.drop_t(t_out)
        t_out = t_out.unsqueeze(1).repeat(1, seq_len, 1)
        collision = collision.view(batch_size, seq_len, 1)
        if use_ego_obsv:
            ego_obsv = ego_obsv.view(batch_size*seq_len, -1)
            ego_out = self.fc_ego(ego_obsv)
            ego_out = self.relu_ego(ego_out)
            ego_out = self.drop_ego(ego_out)
            ego_out = ego_out.view(batch_size, seq_len, -1)
            x = torch.cat((map_out, p_out, t_out, collision, ego_out), 2)
        else:
            x = torch.cat((map_out, p_out, t_out, collision), 2)
        x = x.permute(1,0,2)
        lstm_out, self.hidden = self.lstm(x, self.hidden)
        lstm_out = lstm_out.permute(1,0,2)
        return lstm_out # 返回特征而不是 action_costs
        




class Encoder(nn.Module): 
    # Feature extractor for the egocentric observations
    def __init__(self):
        super(Encoder, self).__init__()
        fnet = models.resnet18(pretrained=True)
        fnet.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,bias=False)
        self.img_features = nn.Sequential(*list(fnet.children())[:-1]) # get resnet18 except the last fc layer
    def forward(self, img_data):
        img_out = self.img_features(img_data) # batch_size x 512 x 1 x 1
        return img_out
</pre>
        </div>
      </div>
    </div>
    <!-- code4.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">train_AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import os
import numpy as np
import math
import random
from dataloader_perturbation_contrast import AVD_IL_Perturbed_Contrast
from dataloader import AVD_online
from IL_Net_wo_IPH import ILNet, Encoder
from mapNet import MapNet
from parameters_wo_IPH import Parameters_IL, ParametersMapNet
import helper as hl
import data_helper as dh
from itertools import chain
from test_NavNet import evaluate_NavNet
import time

print("CUDA is available:", torch.cuda.is_available())
print("CUDA device count:", torch.cuda.device_count())
if torch.cuda.is_available():
    print("Current CUDA device:", torch.cuda.current_device())
    print("CUDA device name:", torch.cuda.get_device_name(0))

# InfoNCE对比损失函数
class InfoNCELoss(nn.Module):
    def __init__(self, temperature=0.07):
        super(InfoNCELoss, self).__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss()
        
    def forward(self, anchor_features, positive_features, negative_features):
        """
        计算InfoNCE损失
        
        参数:
            anchor_features: 锚点特征 [batch_size, feature_dim]
            positive_features: 正样本特征 [batch_size, feature_dim]
            negative_features: 负样本特征 [batch_size, neg_count, feature_dim]
        
        返回:
            对比损失值
        """
        batch_size = anchor_features.size(0)
        neg_count = negative_features.size(1)
        feature_dim = anchor_features.size(1)
        
        # 归一化特征向量
        anchor_features = F.normalize(anchor_features, p=2, dim=1)
        positive_features = F.normalize(positive_features, p=2, dim=1)
        negative_features = F.normalize(negative_features.view(-1, feature_dim), p=2, dim=1).view(batch_size, neg_count, feature_dim)
        
        # 计算正样本的相似度 [batch_size, 1]
        pos_similarity = torch.bmm(anchor_features.unsqueeze(1), positive_features.unsqueeze(2)).squeeze(-1)
        
        # 计算负样本的相似度 [batch_size, neg_count]
        neg_similarity = torch.bmm(anchor_features.unsqueeze(1), 
                                  negative_features.transpose(1, 2)).squeeze(1)
        
     
        logits = torch.cat([pos_similarity, neg_similarity], dim=1) / self.temperature
        labels = torch.zeros(batch_size, dtype=torch.long).to(anchor_features.device)
        
        return self.criterion(logits, labels)

# 从增强数据集中获取小批量数据
def get_minibatch_with_contrast(batch_size, tvec_dim, seq_len, nActions, data, ex_ids, data_index):
    # 数据结构与原始相似，但包含原始轨迹和扰动轨迹两部分
    batch_data = []
    
    for k in range(batch_size):
        batch_data.append(data[ex_ids[data_index + k]])
    
    # 处理原始轨迹数据
    imgs_batch = torch.stack([item['original']['images'] for item in batch_data]).cuda()
    sseg_batch = torch.stack([item['original']['sseg'] for item in batch_data]).cuda()
    dets_batch = torch.stack([item['original']['dets'] for item in batch_data]).cuda()
    imgs_obsv_batch = torch.stack([item['original']['images_obsv'] for item in batch_data]).cuda()
    dets_obsv_batch = torch.stack([item['original']['dets_obsv'] for item in batch_data]).cuda()
    
    # 获取复杂的嵌套数据
    points2D_batch = [item['original']['points2D'] for item in batch_data]
    local3D_batch = [item['original']['local3D'] for item in batch_data]
    
    # 目标标签
    tvec_batch = torch.zeros(batch_size, tvec_dim).float().cuda()
    for k in range(batch_size):
        tvec_batch[k, batch_data[k]['original']['target_lbl']] = 1
    
   
    pose_gt_batch = np.array([item['original']['pose'] for item in batch_data])
    collisions_batch = torch.stack([item['original']['collisions'] for item in batch_data]).cuda()
    actions = [item['original']['actions'] for item in batch_data]
    costs_batch = torch.stack([item['original']['costs'] for item in batch_data]).cuda()
    image_names = [item['original']['images_names'] for item in batch_data]
    scenes = [item['original']['scene'] for item in batch_data]
    scales = [item['original']['scale'] for item in batch_data]
    
    # 处理扰动轨迹数据
    perturbed_imgs_batch = torch.stack([item['perturbed']['images'] for item in batch_data]).cuda()
    perturbed_sseg_batch = torch.stack([item['perturbed']['sseg'] for item in batch_data]).cuda()
    perturbed_dets_batch = torch.stack([item['perturbed']['dets'] for item in batch_data]).cuda()
    perturbed_imgs_obsv_batch = torch.stack([item['perturbed']['images_obsv'] for item in batch_data]).cuda()
    perturbed_dets_obsv_batch = torch.stack([item['perturbed']['dets_obsv'] for item in batch_data]).cuda()
    
    perturbed_points2D_batch = [item['perturbed']['points2D'] for item in batch_data]
    perturbed_local3D_batch = [item['perturbed']['local3D'] for item in batch_data]
    
    perturbed_pose_gt_batch = np.array([item['perturbed']['pose'] for item in batch_data])
    perturbed_collisions_batch = torch.stack([item['perturbed']['collisions'] for item in batch_data]).cuda()
    perturbed_actions = [item['perturbed']['actions'] for item in batch_data]
    perturbed_costs_batch = torch.stack([item['perturbed']['costs'] for item in batch_data]).cuda()
    
    # 对比学习信息
    contrast_info = {
        "perturb_points": [item['contrast_info']['perturb_point'] for item in batch_data],
        "is_positive": [item['contrast_info']['is_positive'] for item in batch_data],
        "similarity_scores": [item['contrast_info']['similarity_score'] for item in batch_data]
    }
    
    # 返回格式
    mapNet_batch = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch)
    IL_batch = (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, image_names, scenes, scales)
    
    perturbed_mapNet_batch = (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                             perturbed_sseg_batch, perturbed_dets_batch, perturbed_pose_gt_batch)
    perturbed_IL_batch = (perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch, tvec_batch, 
                         perturbed_collisions_batch, perturbed_actions, perturbed_costs_batch, 
                         image_names, scenes, scales)
    
    return mapNet_batch, IL_batch, perturbed_mapNet_batch, perturbed_IL_batch, contrast_info

def run_mapNet(parMapNet, mapNet, start_info, use_p_gt, pose_gt_batch):
    if use_p_gt:
        p_gt_batch = dh.build_p_gt(parMapNet, pose_gt_batch)
        p_, map_, confidence_ = mapNet(local_info=start_info, update_type=parMapNet.update_type, 
                                    input_flags=parMapNet.input_flags, p_gt=p_gt_batch)
        p_ = p_gt_batch.clone() # overwrite the predicted with the ground-truth location
    else:
        p_, map_, confidence_ = mapNet(local_info=start_info, update_type=parMapNet.update_type, input_flags=parMapNet.input_flags)
    return p_, map_

def extract_state_features(parIL, state, policy_net):
    """从策略网络提取状态特征，用于对比学习"""
    # 根据状态包含的元素初始化隐藏状态
    policy_net.hidden = policy_net.init_hidden(batch_size=state[0].size(0), state_items=len(state)-1)
    
    # 使用前向传播但不返回预测的成本，而是返回内部特征
    with torch.no_grad():
        state_features = policy_net.get_features(state, parIL.use_ego_obsv)
        
    return state_features

def select_minibatch(par, iters_done):
    sample = random.random()
    eps_threshold = par.EPS_END + (par.EPS_START-par.EPS_END) * math.exp(-1. * iters_done / par.EPS_DECAY)
    if sample > eps_threshold:
        return 0
    else:
        return 1

def unroll_policy(parIL, parMapNet, policy_net, mapNet, action_list, batch_size, seq_len, graphs):
    with torch.no_grad():
        nScenes = 4
        ind = np.random.randint(len(parIL.train_scene_list), size=nScenes)
        scene_list = np.asarray(parIL.train_scene_list)
        sel_scene = scene_list[ind]
        avd_dagger = AVD_online(par=parIL, nStartPos=batch_size/nScenes, 
                                        scene_list=sel_scene, action_list=action_list, graphs_dict=graphs)
        
        # 初始化所有要返回的数组
        imgs_batch = torch.zeros(batch_size, seq_len, 3, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        sseg_batch = torch.zeros(batch_size, seq_len, 1, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        dets_batch = torch.zeros(batch_size, seq_len, avd_dagger.dets_nClasses, avd_dagger.cropSize[1], avd_dagger.cropSize[0]).float().cuda()
        imgs_obsv_batch = torch.zeros(batch_size, seq_len, 3, avd_dagger.cropSizeObsv[1], avd_dagger.cropSizeObsv[0]).float().cuda()
        dets_obsv_batch = torch.zeros(batch_size, seq_len, 1, avd_dagger.cropSizeObsv[1], avd_dagger.cropSizeObsv[0]).float().cuda()
        tvec_batch = torch.zeros(batch_size, parIL.nTargets).float().cuda()
        pose_gt_batch = np.zeros((batch_size, seq_len, 3), dtype=np.float32)
        collisions_batch = torch.zeros(batch_size, seq_len).float().cuda()
        costs_batch = torch.zeros(batch_size, seq_len, len(action_list)).float().cuda()
        points2D_batch, local3D_batch = [], []
        image_names_batch, scene_batch, scale_batch, actions = [], [], [], []
        
        for i in range(len(avd_dagger)):
            ex = avd_dagger[i]
            img = ex["image"].unsqueeze(0)
            points2D_seq, local3D_seq = [], [] 
            points2D_seq.append(ex["points2D"])
            local3D_seq.append(ex["local3D"])
            sseg = ex["sseg"].unsqueeze(0)
            dets = ex['dets'].unsqueeze(0)
            mapNet_input_start = (img.cuda(), points2D_seq, local3D_seq, sseg.cuda(), dets.cuda())
            
            target_lbl = ex["target_lbl"]
            im_obsv = ex['image_obsv'].cuda()
            dets_obsv = ex['dets_obsv'].cuda()
            tvec = torch.zeros(1, parIL.nTargets).float().cuda()
            tvec[0,target_lbl] = 1
            image_name_seq = []
            image_name_seq.append(ex['image_name'])
            scene = ex['scene']
            scene_batch.append(scene)
            scale = ex['scale']
            scale_batch.append(scale)
            graph = avd_dagger.graphs_dict[target_lbl][scene]
            abs_pose_seq = np.zeros((seq_len, 3), dtype=np.float32)

            annotations, _, im_names_all, world_poses, directions = dh.load_scene_info(parIL.avd_root, scene)
            start_abs_pose = dh.get_image_poses(world_poses, directions, im_names_all, image_name_seq, scale)
            
            p_, confidence_, map_ = mapNet.forward_single_step(local_info=mapNet_input_start, t=0, 
                                                    input_flags=parMapNet.input_flags, update_type=parMapNet.update_type)
            collision_ = torch.tensor([0], dtype=torch.float32).cuda()
            if parIL.use_ego_obsv:
                enc_in = torch.cat((im_obsv, dets_obsv), 0).unsqueeze(0)
                ego_obsv_feat = ego_encoder(enc_in)
                state = (map_, p_, tvec, collision_, ego_obsv_feat)
            else:
                state = (map_, p_, tvec, collision_) 
            current_im = image_name_seq[0]

            imgs_batch[i,0,:,:,:] = img
            sseg_batch[i,0,:,:,:] = sseg
            dets_batch[i,0,:,:,:] = dets
            imgs_obsv_batch[i,0,:,:,:] = im_obsv
            dets_obsv_batch[i,0,:,:,:] = dets_obsv
            tvec_batch[i] = tvec
            collisions_batch[i,0] = collision_
            abs_pose_seq[0,:] = start_abs_pose
            cost = np.asarray(dh.get_state_action_cost(current_im, action_list, annotations, graph), dtype=np.float32)
            costs_batch[i,0,:] = torch.from_numpy(cost).float()

            policy_net.hidden = policy_net.init_hidden(batch_size=1, state_items=len(state)-1)
            for t in range(1, seq_len):
                pred_costs = policy_net(state, parIL.use_ego_obsv)
                pred_costs = pred_costs.view(-1).cpu().numpy()
                pred_label = np.argmin(pred_costs)
                pred_action = action_list[pred_label]
                actions.append(pred_action)

                next_im = avd_dagger.scene_annotations[scene][current_im][pred_action]
                if not(next_im==''):
                    collision = 0
                    batch_next, obsv_batch_next = avd_dagger.get_step_data(next_ims=[next_im], scenes=[scene], scales=[scale])
                    next_im_abs_pose = dh.get_image_poses(world_poses, directions, im_names_all, [next_im], scale)
                    if parIL.use_p_gt:
                        abs_poses = np.concatenate((start_abs_pose, next_im_abs_pose), axis=0)
                        rel_poses = dh.relative_poses(poses=abs_poses)
                        next_im_rel_pose = np.expand_dims(rel_poses[1,:], axis=0)
                        p_gt = dh.build_p_gt(parMapNet, pose_gt_batch=np.expand_dims(next_im_rel_pose, axis=1)).squeeze(1)
                        p_next,  confidence_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, input_flags=parMapNet.input_flags,
                                                                map_previous=state[0], p_given=p_gt, update_type=parMapNet.update_type)
                    else:
                        p_next, confidence_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, 
                                            input_flags=parMapNet.input_flags, map_previous=state[0], update_type=parMapNet.update_type)
                    if parIL.use_ego_obsv:
                        enc_in = torch.cat(obsv_batch_next, 1)
                        ego_obsv_feat = ego_encoder(enc_in)
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda(), ego_obsv_feat)
                    else:
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda())
                    current_im = next_im
                    
                    (imgs_next, points2D_next, local3D_next, sseg_next, dets_next) = batch_next
                    (imgs_obsv_next, dets_obsv_next) = obsv_batch_next
                    imgs_batch[i,t,:,:,:] = imgs_next
                    sseg_batch[i,t,:,:,:] = sseg_next
                    dets_batch[i,t,:,:,:] = dets_next
                    imgs_obsv_batch[i,t,:,:,:] = imgs_obsv_next
                    dets_obsv_batch[i,t,:,:,:] = dets_obsv_next
                    collisions_batch[i,t] = torch.tensor([collision], dtype=torch.float32)
                    abs_pose_seq[t,:] = next_im_abs_pose
                    cost = np.asarray(dh.get_state_action_cost(current_im, action_list, annotations, graph), dtype=np.float32)
                    costs_batch[i,t,:] = torch.from_numpy(cost).float()
                    image_name_seq.append(current_im)
                    points2D_seq.append(points2D_next[0])
                    local3D_seq.append(local3D_next[0])

                else:
                    collision = 1
                    if parIL.stop_on_collision:
                        break
                    if parIL.use_ego_obsv:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda(), state[4])
                    else:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda())
                    
                    imgs_batch[i,t,:,:,:] = imgs_batch[i,t-1,:,:,:]
                    sseg_batch[i,t,:,:,:] = sseg_batch[i,t-1,:,:,:]
                    dets_batch[i,t,:,:,:] = dets_batch[i,t-1,:,:,:]
                    imgs_obsv_batch[i,t,:,:,:] = imgs_obsv_batch[i,t-1,:,:,:]
                    dets_obsv_batch[i,t,:,:,:] = dets_obsv_batch[i,t-1,:,:,:]
                    collisions_batch[i,t] = torch.tensor([collision], dtype=torch.float32)
                    abs_pose_seq[t,:] = abs_pose_seq[t-1,:]
                    costs_batch[i,t,:] = costs_batch[i,t-1,:]
                    image_name_seq.append(current_im)
                    points2D_seq.append(points2D_seq[t-1])
                    local3D_seq.append(local3D_seq[t-1])

            pose_gt_batch[i] = dh.relative_poses(poses=abs_pose_seq)
            image_names_batch.append(image_name_seq)
            points2D_batch.append(points2D_seq)
            local3D_batch.append(local3D_seq)

        actions = np.asarray(actions)
        image_names_batch = np.asarray(image_names_batch)

        mapNet_batch = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch)
        IL_batch = (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, image_names_batch, scene_batch, scale_batch)
        return mapNet_batch, IL_batch

if __name__ == '__main__':
    # 配置参数
    parMapNet = ParametersMapNet()
    parIL = Parameters_IL()
    parIL.model_id = "IL_perturb_contrast"  # 设置新的模型ID
    parIL.model_dir = os.path.join(os.path.dirname(parIL.model_dir), f"{parIL.model_id}/")
    os.makedirs(parIL.model_dir, exist_ok=True)
    
    # 对比学习超参数
    contrast_weight = 0.1  # 对比损失的权重
    temperature = 0.07     # InfoNCE温度参数
    negative_samples = 32  # 从小批量中选择的负样本数量
    
    action_list = np.asarray(parMapNet.action_list)

    # 初始化模型
    policy_net = ILNet(parIL, parMapNet.map_embedding, parMapNet.orientations, parIL.nTargets, len(action_list), parIL.use_ego_obsv)
    policy_net.train()
    policy_net.cuda()

    # 加载预训练的MapNet模型
    state_model = hl.load_model(model_dir=parIL.mapNet_model_dir, model_name="MapNet",
                                test_iter=parIL.mapNet_iters, eval=not(parIL.finetune_mapNet))
    
    # 初始化优化器
    if parIL.finetune_mapNet:
        all_params = chain(policy_net.parameters(), state_model.parameters())
    else:
        all_params = policy_net.parameters()
    optimizer = optim.Adam(all_params, lr=parIL.lr_rate)
    scheduler = StepLR(optimizer, step_size=parIL.step_size, gamma=parIL.gamma)
    
    # 初始化InfoNCE损失
    contrast_criterion = InfoNCELoss(temperature=temperature)

    # 加载ego观察编码器（如果使用）
    if parIL.use_ego_obsv:
        ego_encoder = Encoder()
        ego_encoder.cuda()
        ego_encoder.eval()
    else:
        ego_encoder = None

    # 加载扰动对比学习的数据集
    print("Loading training episodes with perturbation and contrast...")
    img_encoder_path = os.path.join(os.path.dirname(os.path.dirname(parIL.model_dir)), 
                                   "Img_encoder.pth.tar")
    avd = AVD_IL_Perturbed_Contrast(
        par=parIL, 
        seq_len=parIL.seq_len, 
        nEpisodes=parIL.epi_per_scene,
        scene_list=parIL.train_scene_list, 
        action_list=action_list,
        perturb_prob=0.6,  # 60%的轨迹应用扰动
        similarity_threshold=0.6,
        img_encoder_path=img_encoder_path
    )

    # 加载验证数据集
    print("Loading validation episodes...")
    avd_test = AVD_online(par=parIL, nStartPos=10, scene_list=parIL.train_scene_list, action_list=action_list)
    
    test_ids = list(range(len(avd_test)))
    train_ids = list(range(len(avd)))
    
    # 保存参数和初始化日志
    hl.save_params(parIL, parIL.model_dir, name="IL")
    hl.save_params(parMapNet, parIL.model_dir, name="mapNet")
    log = open(parIL.model_dir + f"train_log_{parIL.model_id}.txt", 'w')
    
    nData = len(train_ids)
    iters_per_epoch = int(nData / float(parIL.batch_size))
    log.write(f"Iters_per_epoch: {iters_per_epoch}\n")
    print(f"Iters per epoch: {iters_per_epoch}")
    
    loss_list = []
    contrast_loss_list = []

    # 开始训练循环
    for ep in range(parIL.nEpochs):
        epoch_start_time = time.time()
        random.shuffle(train_ids)
        data_index = 0
        
        for i in range(iters_per_epoch):
            iters = i + ep * iters_per_epoch
            
            # 选择数据源：预采样数据或在线生成数据
            ch = select_minibatch(par=parIL, iters_done=iters)
            
            if ch:
                # 使用预处理的对比学习数据
                mapNet_batch, IL_batch, perturbed_mapNet_batch, perturbed_IL_batch, contrast_info = get_minibatch_with_contrast(
                    batch_size=parIL.batch_size,
                    tvec_dim=parIL.nTargets,
                    seq_len=parIL.seq_len,
                    nActions=len(action_list),
                    data=avd,
                    ex_ids=train_ids,
                    data_index=data_index
                )
                
                # 计算对比学习损失的标志
                use_contrast_loss = True
                
            else:
                # 使用在线生成的数据
                mapNet_batch, IL_batch = unroll_policy(parIL, parMapNet, policy_net, state_model,
                                                    action_list, batch_size=parIL.batch_size, 
                                                    seq_len=parIL.seq_len, graphs=avd.graphs_dict)
                
                # 在线数据不包含扰动和对比信息
                perturbed_mapNet_batch = None
                perturbed_IL_batch = None
                contrast_info = None
                use_contrast_loss = False
            
            data_index += parIL.batch_size
            
         
            (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch, pose_gt_batch) = mapNet_batch
            (imgs_obsv_batch, dets_obsv_batch, tvec_batch, collisions_batch, actions, costs_batch, 
             image_names, scenes, scales) = IL_batch
            
            # 1. 处理原始轨迹
            start_info = (imgs_batch, points2D_batch, local3D_batch, sseg_batch, dets_batch)
            if parIL.finetune_mapNet:
                p_, map_ = run_mapNet(parMapNet, state_model, start_info, parIL.use_p_gt, pose_gt_batch)
            else:
                with torch.no_grad():
                    p_, map_ = run_mapNet(parMapNet, state_model, start_info, parIL.use_p_gt, pose_gt_batch)
            
            if parIL.use_ego_obsv:
                with torch.no_grad():
                    enc_in = torch.cat((imgs_obsv_batch, dets_obsv_batch), 2)
                    enc_in = enc_in.view(parIL.batch_size*parIL.seq_len, 4, parIL.crop_size_obsv[1], parIL.crop_size_obsv[0])
                    ego_obsv_feat = ego_encoder(enc_in)
                    ego_obsv_feat = ego_obsv_feat.view(parIL.batch_size, parIL.seq_len, ego_obsv_feat.shape[1])
                state = (map_, p_, tvec_batch, collisions_batch, ego_obsv_feat)
            else: 
                state = (map_, p_, tvec_batch, collisions_batch)
            
            # 计算主要的导航损失
            policy_net.hidden = policy_net.init_hidden(parIL.batch_size, state_items=len(state)-1)
            pred_costs = policy_net(state, parIL.use_ego_obsv)
            nav_loss = policy_net.build_loss(cost_pred=pred_costs, cost_gt=costs_batch, loss_weight=parIL.loss_weight)
            
            # 初始化总损失
            total_loss = nav_loss
            contrast_loss = torch.tensor(0.0).cuda()
            perturbed_nav_loss = torch.tensor(0.0).cuda()
            
            # 2. 如果使用对比学习数据，计算额外的损失
            if use_contrast_loss and perturbed_mapNet_batch is not None:
                # 解包扰动数据
                (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                 perturbed_sseg_batch, perturbed_dets_batch, perturbed_pose_gt_batch) = perturbed_mapNet_batch
                (perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch, _, perturbed_collisions_batch, 
                 perturbed_actions, perturbed_costs_batch, _, _, _) = perturbed_IL_batch
                
                # 处理扰动轨迹
                perturbed_start_info = (perturbed_imgs_batch, perturbed_points2D_batch, perturbed_local3D_batch, 
                                       perturbed_sseg_batch, perturbed_dets_batch)
                if parIL.finetune_mapNet:
                    perturbed_p_, perturbed_map_ = run_mapNet(parMapNet, state_model, perturbed_start_info, 
                                                            parIL.use_p_gt, perturbed_pose_gt_batch)
                else:
                    with torch.no_grad():
                        perturbed_p_, perturbed_map_ = run_mapNet(parMapNet, state_model, perturbed_start_info, 
                                                                 parIL.use_p_gt, perturbed_pose_gt_batch)
                
                if parIL.use_ego_obsv:
                    with torch.no_grad():
                        perturbed_enc_in = torch.cat((perturbed_imgs_obsv_batch, perturbed_dets_obsv_batch), 2)
                        perturbed_enc_in = perturbed_enc_in.view(parIL.batch_size*parIL.seq_len, 4, 
                                                               parIL.crop_size_obsv[1], parIL.crop_size_obsv[0])
                        perturbed_ego_obsv_feat = ego_encoder(perturbed_enc_in)
                        perturbed_ego_obsv_feat = perturbed_ego_obsv_feat.view(parIL.batch_size, parIL.seq_len, 
                                                                        perturbed_ego_obsv_feat.shape[1])
                    perturbed_state = (perturbed_map_, perturbed_p_, tvec_batch, perturbed_collisions_batch, perturbed_ego_obsv_feat)
                else: 
                    perturbed_state = (perturbed_map_, perturbed_p_, tvec_batch, perturbed_collisions_batch)
                
                # 计算对比学习损失
                orig_features = extract_state_features(parIL, state, policy_net)
                perturbed_features = extract_state_features(parIL, perturbed_state, policy_net)
                
                valid_contrast_samples = 0
                for b in range(parIL.batch_size):
                    perturb_point = contrast_info['perturb_points'][b]
                    
                    if perturb_point > 0:
                        anchor_feat = orig_features[b, perturb_point+1].unsqueeze(0)
                        contrast_feat = perturbed_features[b, perturb_point+1].unsqueeze(0)
                        is_positive = contrast_info['is_positive'][b]
                        
                        neg_indices = [j for j in range(parIL.batch_size) if j != b]
                        if len(neg_indices) > negative_samples:
                            neg_indices = random.sample(neg_indices, negative_samples)
                        
                        if is_positive:
                            neg_feats = torch.stack([perturbed_features[j, perturb_point+1] for j in neg_indices])
                            pos_contrast_loss = contrast_criterion(anchor_feat, contrast_feat, neg_feats.unsqueeze(0))
                            contrast_loss += pos_contrast_loss
                        else:
                            if neg_indices:
                                pos_idx = random.choice(neg_indices)
                                pos_feat = orig_features[pos_idx, perturb_point+1].unsqueeze(0)
                                
                                all_neg_indices = neg_indices.copy()
                                if pos_idx in all_neg_indices:
                                    all_neg_indices.remove(pos_idx)
                                if len(all_neg_indices) > 0:
                                    other_neg_feats = torch.stack([perturbed_features[j, perturb_point+1] for j in all_neg_indices])
                                    neg_feats = torch.cat([contrast_feat, other_neg_feats], dim=0)
                                    
                                    neg_contrast_loss = contrast_criterion(anchor_feat, pos_feat, neg_feats.unsqueeze(0))
                                    contrast_loss += neg_contrast_loss
                                    valid_contrast_samples += 1
                
                if valid_contrast_samples > 0:
                    contrast_loss = contrast_loss / valid_contrast_samples
                
                # 计算扰动轨迹的导航损失
                policy_net.hidden = policy_net.init_hidden(parIL.batch_size, state_items=len(perturbed_state)-1)
                perturbed_pred_costs = policy_net(perturbed_state, parIL.use_ego_obsv)
                perturbed_nav_loss = policy_net.build_loss(
                    cost_pred=perturbed_pred_costs, 
                    cost_gt=perturbed_costs_batch, 
                    loss_weight=parIL.loss_weight
                )
                
                # 更新总损失
                total_loss = nav_loss + perturbed_nav_loss + contrast_weight * contrast_loss
            
           
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            # 输出、记录和可视化
            if iters % parIL.show_interval == 0:
                if use_contrast_loss:
                    log_msg = (f"Epoch: {ep} ITER: {iters} "
                              f"Nav_Loss: {nav_loss.item():.4f} "
                              f"Perturb_Loss: {perturbed_nav_loss.item():.4f} "
                              f"Contrast_Loss: {contrast_loss.item():.4f} "
                              f"Total_Loss: {total_loss.item():.4f} [Contrast]")
                else:
                    log_msg = (f"Epoch: {ep} ITER: {iters} "
                              f"Nav_Loss: {nav_loss.item():.4f} "
                              f"Total_Loss: {total_loss.item():.4f} [Online]")
                log.write(log_msg + "\n")
                print(log_msg)
                log.flush()
            
            if iters > 0:
                loss_list.append(total_loss.item())
                contrast_loss_list.append(contrast_loss.item())
                
            if iters % parIL.plot_interval == 0 and iters > 0:
                hl.plot_loss(loss=loss_list, epoch=ep, iteration=iters, 
                            step=1, loss_name="Total_Loss", loss_dir=parIL.model_dir)
                hl.plot_loss(loss=contrast_loss_list, epoch=ep, iteration=iters,
                            step=1, loss_name="Contrast_Loss", loss_dir=parIL.model_dir)
                
            if iters % parIL.save_interval == 0:
                hl.save_model(model=policy_net, model_dir=parIL.model_dir, 
                             model_name="ILNet", train_iter=iters)
                if parIL.finetune_mapNet:
                    hl.save_model(model=state_model, model_dir=parIL.model_dir, 
                                model_name="MapNet", train_iter=iters)
            
            if iters % parIL.test_interval == 0:
                evaluate_NavNet(parIL, parMapNet, mapNet=state_model, ego_encoder=ego_encoder, 
                               test_iter=iters, test_ids=test_ids, test_data=avd_test, 
                               action_list=action_list)
        
        
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch {ep} completed in {epoch_time:.2f} seconds. Updating learning rate...")
        scheduler.step()

    
</pre>
        </div>
      </div>
    </div>   
    <!-- code5.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">trajectory_perturbation.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>

import torch
from torch.utils.data import Dataset
import numpy as np
from tqdm import tqdm
import random
import os
import data_helper as dh
import torch.nn as nn
from torchvision.models import resnet18 as resnet18_img
from torchvision import transforms
from PIL import Image

class AVD_IL_Perturbed_Contrast(Dataset):
    """
    AVD类，用于预采样剧集和用于监督的行动成本，
    并添加扰动策略生成轨迹和对比学习功能
    """
    def __init__(self, par, seq_len, nEpisodes, scene_list, action_list, perturb_prob=0.8, 
                similarity_threshold=0.5, img_encoder_path=None):
        self.datasetPath = par.avd_root
        self.cropSize = par.crop_size
        self.cropSizeObsv = par.crop_size_obsv
        self.orig_res = par.orig_res
        self.normalize = True
        self.pixFormat = "NCHW"
        self.scene_list = scene_list
        self.n_episodes = nEpisodes # 5000
        self.seq_len = seq_len
        self.actions = action_list
        self.dets_nClasses = par.dets_nClasses
        
        # 扰动和对比学习参数
        self.perturb_prob = perturb_prob  # 轨迹被扰动的概率
        self.similarity_threshold = similarity_threshold  # 正负样本的划分阈值
        
        # 读取语义分割文件
        self.sseg_data = np.load(par.sseg_file_path, encoding='bytes', allow_pickle=True).item()
        
        self.detection_data, self.labels_to_cats, self.labels_to_index = dh.load_detections(par, self.scene_list)
        self.cat_dict = par.cat_dict  # 将类别名称定位到标签字典
        
        # 需要收集目标的图像名称
        self.targets_data = np.load(par.targets_file_path, encoding='bytes', allow_pickle=True).item()
        
        # 需要预先计算每个目标的场景图
        self.graphs_dict = dh.get_scene_target_graphs(self.datasetPath, self.cat_dict, self.targets_data, self.actions)
        
        # 图像编码器相关参数
        self.torch_device = 'cuda:' + str(par.TORCH_GPU_ID) if hasattr(par, 'TORCH_GPU_ID') and torch.cuda.device_count() > 0 else 'cpu'
        # self.transform_eval = transforms.Compose([
        #     transforms.ToTensor(), 
        #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        # ])
        self.feature_dim = par.img_embedding_dim if hasattr(par, 'img_embedding_dim') else 512

        # 加载图像编码器用于对比学习
        self.img_encoder = None
        if img_encoder_path is None:
            img_encoder_path = os.path.join(os.path.dirname(os.path.dirname(par.model_dir)), 
                                          "Img_encoder.pth.tar")
        self.img_encoder = self.load_img_encoder(self.feature_dim, img_encoder_path)
        
        # 采样episodes，包括原始轨迹和扰动轨迹
        self.sample_episodes()

    def load_img_encoder(self, feature_dim, ckpt_pth):
        """加载图像编码器（参考dataloder_perturbation.py）"""
        img_encoder = resnet18_img(num_classes=feature_dim)
        dim_mlp = img_encoder.fc.weight.shape[1]
        img_encoder.fc = nn.Sequential(nn.Linear(dim_mlp, dim_mlp), nn.ReLU(), img_encoder.fc)
        if not os.path.exists(ckpt_pth):
            print(f"警告: 编码器文件不存在: {ckpt_pth}")
            print("使用随机初始化的编码器")
        else:
            try:
                ckpt = torch.load(ckpt_pth, map_location='cpu')
                if 'state_dict' in ckpt:
                    state_dict = ckpt['state_dict']
                    if any(key.startswith('module.encoder_q.') for key in state_dict.keys()):
                        state_dict = {k[len('module.encoder_q.'):]: v for k, v in state_dict.items() if 'module.encoder_q.' in k}
                    elif any(key.startswith('module.') for key in state_dict.keys()):
                        state_dict = {k[len('module.'):]: v for k, v in state_dict.items() if k.startswith('module.')}
                else:
                    state_dict = ckpt
                img_encoder.load_state_dict(state_dict, strict=False)
                print(f"成功加载图像编码器: {ckpt_pth}")
            except Exception as e:
                print(f"加载编码器失败: {e}")
                print("使用随机初始化的编码器")
        img_encoder.eval().to(self.torch_device)
        return img_encoder

    def embed_image(self, img_tensor):
        """使用预训练编码器提取图像特征"""
        if self.img_encoder is None:
            return None
        with torch.no_grad():
            # 如果输入是numpy，转为tensor
            if isinstance(img_tensor, np.ndarray):
                # 如果是HWC，转为CHW
                if img_tensor.shape[0] != 3 and img_tensor.shape[-1] == 3:
                    img_tensor = np.transpose(img_tensor, (2, 0, 1))
                img_tensor = torch.from_numpy(img_tensor).float()
            # 确保输入形状正确 [C, H, W]
            if len(img_tensor.shape) == 3:
                img_tensor = img_tensor.unsqueeze(0)
            img_tensor = img_tensor.to(self.torch_device)
            features = self.img_encoder(img_tensor)
            features = nn.functional.normalize(features, dim=1)
            return features.cpu().numpy()

    def is_close(self, img1_features, img2_features, threshold=None):
        """计算两个图像特征的相似度，返回是否为正样本(True/False)和相似度"""
        if threshold is None:
            threshold = self.similarity_threshold
            
        if img1_features is None or img2_features is None:
            return False, 0.0
            
        # 计算余弦相似度
        img1_features = img1_features.flatten()
        img2_features = img2_features.flatten()
        similarity = np.dot(img1_features, img2_features) / (np.linalg.norm(img1_features) * np.linalg.norm(img2_features))
        
        return similarity > threshold, similarity

    def sample_episodes(self):
        """采样一系列episode，包含原始轨迹和扰动轨迹"""
        epi_id = 0
        im_paths, action_paths, cost_paths = {}, {}, {}
        scene_name, scene_scale, target_lbls = {}, {}, {}
        pose_paths, collisions = {}, {}
        
        # 扰动轨迹相关数据
        perturbed_im_paths, perturbed_action_paths = {}, {}
        perturbed_cost_paths, perturbed_pose_paths = {}, {}
        perturbed_collisions = {}
        similarity_scores = {}  # 存储扰动点处原图像和扰动后图像的相似度
        perturb_points = {}  # 存储扰动点位置
        is_positive_sample = {}  # 存储是否为正样本

        # 添加正负样本统计变量
        total_perturbed_episodes = 0
        total_positive_samples = 0
        total_negative_samples = 0
        total_no_perturbation = 0
        similarity_score_list = []
        scene_stats = {}  # 每个场景的统计信息

        for scene in tqdm(self.scene_list, desc="采样房间进度"):
            annotations, scale, im_names_all, world_poses, directions = dh.load_scene_info(self.datasetPath, scene)
            scene_epi_count = 0
            
            # 初始化场景统计
            scene_stats[scene] = {
                'total_episodes': 0,
                'perturbed_episodes': 0,
                'positive_samples': 0,
                'negative_samples': 0,
                'no_perturbation': 0,
                'similarity_scores': []
            }
            
            pbar = tqdm(total=self.n_episodes, desc=f"{scene}采样episode", leave=False)
            
            while scene_epi_count < self.n_episodes:
                # 为每个episode随机选一个场景图片作为智能体的出发点
                idx = np.random.randint(len(im_names_all), size=1)
                im_name_0 = im_names_all[idx[0]]
                
                # 找出当前场景中所有可能的目标类别
                candidates = dh.candidate_targets(scene, self.cat_dict, self.targets_data)
                idx_cat = np.random.randint(len(candidates), size=1)
                cat = candidates[idx_cat[0]]
                target_lbl = self.cat_dict[cat]
                graph = self.graphs_dict[target_lbl][scene]

                # 采样原始轨迹
                choice = np.random.randint(2, size=1)[0]
                im_seq, action_seq, cost_seq = [], [], []
                poses_seq, collision_seq = [], []
                im_seq.append(im_name_0)
                current_im = im_name_0
                
                cost_seq.append(dh.get_state_action_cost(current_im, self.actions, annotations, graph))
                poses_seq.append(dh.get_im_pose(im_names_all, current_im, world_poses, directions, scale))
                collision_seq.append(0)
                
                # 生成原始轨迹
                for i in range(1, self.seq_len):
                    if choice:  # 执行最佳动作
                        actions_cost = np.array(cost_seq[i-1])
                        min_cost = np.min(actions_cost)
                        min_ind = np.where(actions_cost==min_cost)[0]
                        if len(min_ind)==1:
                            sel_ind = min_ind[0]
                        else:
                            sel_ind = min_ind[np.random.randint(len(min_ind), size=1)[0]]
                        sel_action = self.actions[sel_ind]
                    else:  # 执行随机动作
                        sel_action = self.actions[np.random.randint(len(self.actions), size=1)[0]]
                    
                    next_im = annotations[current_im][sel_action]
                    if not(next_im==''):  # 动作有效
                        current_im = next_im
                        collision_seq.append(0)
                    else:  # 动作无效，撞墙
                        collision_seq.append(1)
                    
                    im_seq.append(current_im)
                    action_seq.append(sel_action)
                    poses_seq.append(dh.get_im_pose(im_names_all, current_im, world_poses, directions, scale))
                    cost_seq.append(dh.get_state_action_cost(current_im, self.actions, annotations, graph))
                
                # 存储原始轨迹
                im_paths[epi_id] = np.asarray(im_seq)
                action_paths[epi_id] = np.asarray(action_seq)
                cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                scene_name[epi_id] = scene
                scene_scale[epi_id] = scale
                target_lbls[epi_id] = target_lbl
                pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                
                # 更新场景统计
                scene_stats[scene]['total_episodes'] += 1
                
                # 是否生成扰动轨迹
                apply_perturbation = np.random.rand() < self.perturb_prob
                
                if apply_perturbation:
                    # 选择扰动点（避免第一个点）
                    valid_perturb_points = list(range(1, self.seq_len-1))
                    if len(valid_perturb_points) > 0:
                        perturb_idx = random.choice(valid_perturb_points)
                        perturb_points[epi_id] = perturb_idx
                        
                        # 开始创建扰动轨迹（复制原始轨迹直到扰动点）
                        perturb_im_seq = im_seq[:perturb_idx+1].copy()
                        perturb_action_seq = action_seq[:perturb_idx].copy()  # 动作序列比位置序列少一个
                        perturb_cost_seq = cost_seq[:perturb_idx+1].copy()
                        perturb_poses_seq = poses_seq[:perturb_idx+1].copy()
                        perturb_collision_seq = collision_seq[:perturb_idx+1].copy()
                        
                        # 在扰动点选择不同于原动作的随机动作
                        available_actions = [a for a in self.actions if a != action_seq[perturb_idx]]
                        if not available_actions:  # 如果只有一个动作（原动作），不扰动
                            available_actions = self.actions
                            
                        perturb_action = random.choice(available_actions)
                        perturb_action_seq.append(perturb_action)
                        
                        # 从扰动点开始重新生成轨迹
                        current_im = perturb_im_seq[perturb_idx]
                        next_im = annotations[current_im][perturb_action]
                        
                        # 获取扰动点的原图像特征和扰动后的图像特征（如果可用）
                        original_img = None
                        perturbed_img = None
                        
                        if next_im != '':  # 扰动动作有效
                            total_perturbed_episodes += 1
                            scene_stats[scene]['perturbed_episodes'] += 1
                            
                            # 获取扰动点原图像
                            original_img_data, _, _ = dh.getImageData(
                                self.datasetPath, im_seq[perturb_idx+1], 
                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
                                
                            # 获取扰动后的图像
                            perturbed_img_data, _, _ = dh.getImageData(
                                self.datasetPath, next_im,
                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
                                
                            # 使用编码器嵌入图像特征
                            original_feat = self.embed_image(original_img_data)
                            perturbed_feat = self.embed_image(perturbed_img_data)
                            
                            # 计算相似度和是否为正样本
                            is_pos, sim_score = self.is_close(original_feat, perturbed_feat)
                            is_positive_sample[epi_id] = is_pos
                            similarity_scores[epi_id] = sim_score
                            
                            # 更新统计信息
                            similarity_score_list.append(sim_score)
                            scene_stats[scene]['similarity_scores'].append(sim_score)
                            
                            if is_pos:
                                total_positive_samples += 1
                                scene_stats[scene]['positive_samples'] += 1
                            else:
                                total_negative_samples += 1
                                scene_stats[scene]['negative_samples'] += 1
                            
                            # 继续生成扰动轨迹
                            current_im = next_im
                            perturb_im_seq.append(current_im)
                            perturb_collision_seq.append(0)
                            
                            perturb_poses_seq.append(dh.get_im_pose(im_names_all, current_im, 
                                                    world_poses, directions, scale))
                            perturb_cost_seq.append(dh.get_state_action_cost(
                                current_im, self.actions, annotations, graph))
                            
                            # 继续生成剩余轨迹，使用最优动作
                            for i in range(perturb_idx+2, self.seq_len):
                                actions_cost = np.array(perturb_cost_seq[-1])
                                min_cost = np.min(actions_cost)
                                min_ind = np.where(actions_cost==min_cost)[0]
                                if len(min_ind)==1:
                                    sel_ind = min_ind[0]
                                else:
                                    sel_ind = min_ind[np.random.randint(len(min_ind), size=1)[0]]
                                sel_action = self.actions[sel_ind]
                                perturb_action_seq.append(sel_action)
                                
                                next_im = annotations[current_im][sel_action]
                                if not(next_im==''):  # 动作有效
                                    current_im = next_im
                                    perturb_collision_seq.append(0)
                                else:  # 动作无效，撞墙
                                    perturb_collision_seq.append(1)
                                
                                perturb_im_seq.append(current_im)
                                perturb_poses_seq.append(dh.get_im_pose(im_names_all, current_im, 
                                                        world_poses, directions, scale))
                                perturb_cost_seq.append(dh.get_state_action_cost(
                                    current_im, self.actions, annotations, graph))
                        else:
                            # 如果扰动动作无效，使用与原始轨迹相同的数据
                            is_positive_sample[epi_id] = False
                            similarity_scores[epi_id] = 0.0
                            total_no_perturbation += 1
                            scene_stats[scene]['no_perturbation'] += 1
                            perturb_im_seq = im_seq.copy()
                            perturb_action_seq = action_seq.copy()
                            perturb_cost_seq = cost_seq.copy()
                            perturb_poses_seq = poses_seq.copy()
                            perturb_collision_seq = collision_seq.copy()
                        
                        # 确保轨迹长度正确
                        if len(perturb_im_seq) < self.seq_len:
                            # 如果扰动轨迹短于预期长度，用最后一个状态填充
                            last_im = perturb_im_seq[-1]
                            last_pose = perturb_poses_seq[-1]
                            last_cost = perturb_cost_seq[-1]
                            last_collision = perturb_collision_seq[-1]
                            
                            for _ in range(self.seq_len - len(perturb_im_seq)):
                                perturb_im_seq.append(last_im)
                                perturb_poses_seq.append(last_pose)
                                perturb_cost_seq.append(last_cost)
                                perturb_collision_seq.append(last_collision)
                                if len(perturb_action_seq) < self.seq_len - 1:
                                    perturb_action_seq.append(self.actions[0])  # 添加一个默认动作
                        
                        # 存储扰动轨迹
                        perturbed_im_paths[epi_id] = np.asarray(perturb_im_seq)
                        perturbed_action_paths[epi_id] = np.asarray(perturb_action_seq)
                        perturbed_cost_paths[epi_id] = np.asarray(perturb_cost_seq, dtype=np.float32)
                        perturbed_pose_paths[epi_id] = np.asarray(perturb_poses_seq, dtype=np.float32)
                        perturbed_collisions[epi_id] = np.asarray(perturb_collision_seq, dtype=np.float32)
                    else:
                        # 如果没有有效的扰动点，使用原始轨迹
                        perturbed_im_paths[epi_id] = np.asarray(im_seq)
                        perturbed_action_paths[epi_id] = np.asarray(action_seq)
                        perturbed_cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                        perturbed_pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                        perturbed_collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                        perturb_points[epi_id] = -1  # 表示没有扰动点
                        is_positive_sample[epi_id] = False
                        similarity_scores[epi_id] = 0.0
                        total_no_perturbation += 1
                        scene_stats[scene]['no_perturbation'] += 1
                else:
                    # 如果不扰动，使用原始轨迹
                    perturbed_im_paths[epi_id] = np.asarray(im_seq)
                    perturbed_action_paths[epi_id] = np.asarray(action_seq)
                    perturbed_cost_paths[epi_id] = np.asarray(cost_seq, dtype=np.float32)
                    perturbed_pose_paths[epi_id] = np.asarray(poses_seq, dtype=np.float32)
                    perturbed_collisions[epi_id] = np.asarray(collision_seq, dtype=np.float32)
                    perturb_points[epi_id] = -1  # 表示没有扰动点
                    is_positive_sample[epi_id] = False
                    similarity_scores[epi_id] = 0.0
                    total_no_perturbation += 1
                    scene_stats[scene]['no_perturbation'] += 1
                
                epi_id += 1
                scene_epi_count += 1
                pbar.update(1)
            pbar.close()
        
        # 打印详细统计信息
        self.print_sampling_statistics(
            total_episodes=epi_id,
            total_perturbed_episodes=total_perturbed_episodes,
            total_positive_samples=total_positive_samples,
            total_negative_samples=total_negative_samples,
            total_no_perturbation=total_no_perturbation,
            similarity_score_list=similarity_score_list,
            scene_stats=scene_stats
        )
        
        # 保存统计信息到实例变量
        self.sampling_stats = {
            'total_episodes': epi_id,
            'total_perturbed_episodes': total_perturbed_episodes,
            'total_positive_samples': total_positive_samples,
            'total_negative_samples': total_negative_samples,
            'total_no_perturbation': total_no_perturbation,
            'similarity_scores': similarity_score_list,
            'scene_stats': scene_stats
        }
        
        # 保存所有采集的数据
        self.im_paths = im_paths
        self.action_paths = action_paths
        self.cost_paths = cost_paths
        self.scene_name = scene_name
        self.scene_scale = scene_scale
        self.target_lbls = target_lbls
        self.pose_paths = pose_paths
        self.collisions = collisions
        
        # 扰动轨迹数据
        self.perturbed_im_paths = perturbed_im_paths
        self.perturbed_action_paths = perturbed_action_paths
        self.perturbed_cost_paths = perturbed_cost_paths
        self.perturbed_pose_paths = perturbed_pose_paths
        self.perturbed_collisions = perturbed_collisions
        self.perturb_points = perturb_points
        self.is_positive_sample = is_positive_sample
        self.similarity_scores = similarity_scores
        
        print(f"总共采样 {epi_id} 个episodes")
        
    def print_sampling_statistics(self, total_episodes, total_perturbed_episodes, 
                                total_positive_samples, total_negative_samples, 
                                total_no_perturbation, similarity_score_list, scene_stats):
        """打印详细的采样统计信息"""
        print("\n" + "="*80)
        print("数据采集统计报告")
        print("="*80)
        
        # 总体统计
        print(f"总episodes数量: {total_episodes}")
        print(f"应用扰动的episodes: {total_perturbed_episodes} ({total_perturbed_episodes/total_episodes*100:.1f}%)")
        print(f"未扰动的episodes: {total_no_perturbation} ({total_no_perturbation/total_episodes*100:.1f}%)")
        print()
        
        # 正负样本统计
        if total_perturbed_episodes > 0:
            positive_ratio = total_positive_samples / total_perturbed_episodes * 100
            negative_ratio = total_negative_samples / total_perturbed_episodes * 100
            
            print("对比学习样本统计:")
            print(f"  正样本数量: {total_positive_samples} ({positive_ratio:.1f}%)")
            print(f"  负样本数量: {total_negative_samples} ({negative_ratio:.1f}%)")
            print(f"  正负样本比例: {total_positive_samples}:{total_negative_samples}")
            
            if len(similarity_score_list) > 0:
                avg_similarity = np.mean(similarity_score_list)
                std_similarity = np.std(similarity_score_list)
                min_similarity = np.min(similarity_score_list)
                max_similarity = np.max(similarity_score_list)
                
                print(f"  相似度统计:")
                print(f"    平均相似度: {avg_similarity:.3f}")
                print(f"    标准差: {std_similarity:.3f}")
                print(f"    最小值: {min_similarity:.3f}")
                print(f"    最大值: {max_similarity:.3f}")
                print(f"    阈值: {self.similarity_threshold}")
            print()
        
        # 按场景统计
        print("各场景统计:")
        print(f"{'场景名称':<15} {'总episodes':<10} {'扰动episodes':<12} {'正样本':<8} {'负样本':<8} {'正样本率':<10}")
        print("-" * 80)
        
        for scene, stats in scene_stats.items():
            if stats['perturbed_episodes'] > 0:
                pos_rate = stats['positive_samples'] / stats['perturbed_episodes'] * 100
            else:
                pos_rate = 0.0
                
            print(f"{scene:<15} {stats['total_episodes']:<10} {stats['perturbed_episodes']:<12} "
                  f"{stats['positive_samples']:<8} {stats['negative_samples']:<8} {pos_rate:<10.1f}%")
        
        print("="*80)
        
        # 相似度分布统计
        if len(similarity_score_list) > 0:
            print("\n相似度分布:")
            bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
            hist, _ = np.histogram(similarity_score_list, bins=bins)
            
            for i in range(len(bins)-1):
                count = hist[i]
                percentage = count / len(similarity_score_list) * 100
                print(f"  [{bins[i]:.1f}-{bins[i+1]:.1f}): {count} 个样本 ({percentage:.1f}%)")
        
        print("="*80 + "\n")

    def get_sampling_statistics(self):
        """返回采样统计信息，供外部调用"""
        return getattr(self, 'sampling_stats', None)

    def __len__(self):
        return len(self.im_paths)

    def __getitem__(self, index):
        # 创建两个item：原始轨迹和扰动轨迹
        item_original = {}
        item_perturbed = {}
        
        # 原始轨迹数据
        im_path = self.im_paths[index]
        action_path = self.action_paths[index]
        cost_path = self.cost_paths[index]
        scene = self.scene_name[index]
        scale = self.scene_scale[index]
        target_lbl = self.target_lbls[index]
        abs_poses = self.pose_paths[index]
        collision_seq = self.collisions[index]
        
        # 扰动轨迹数据
        perturbed_im_path = self.perturbed_im_paths[index]
        perturbed_action_path = self.perturbed_action_paths[index]
        perturbed_cost_path = self.perturbed_cost_paths[index]
        perturbed_abs_poses = self.perturbed_pose_paths[index]
        perturbed_collision_seq = self.perturbed_collisions[index]
        
        # 扰动信息
        perturb_point = self.perturb_points[index]
        is_positive = self.is_positive_sample[index]
        similarity_score = self.similarity_scores[index]
        
        scene_seg = self.sseg_data[scene.encode()]  # 将字符串转换为字节
        scene_dets = self.detection_data[scene]

        # 处理原始轨迹
        imgs = np.zeros((self.seq_len, 3, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        imgs_obsv = np.zeros((self.seq_len, 3, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        ssegs = np.zeros((self.seq_len, 1, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        dets = np.zeros((self.seq_len, self.dets_nClasses, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        dets_obsv = np.zeros((self.seq_len, 1, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        points2D, local3D = [], []
        
        for i in range(len(im_path)):
            im_name = im_path[i]
            imgData, points2D_step, local3D_step = dh.getImageData(self.datasetPath, im_name, 
                                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
            imgs[i,:,:,:] = imgData
            points2D.append(points2D_step)
            local3D.append(local3D_step)
            imgs_obsv[i,:,:,:] = dh.getImageData(self.datasetPath, im_name, scene, self.cropSizeObsv, 
                                            self.orig_res, self.pixFormat, self.normalize, get3d=False)
            # 加载语义分割、检测掩码和以自我为中心的检测观察
            ssegs[i,:,:,:] = dh.get_sseg(im_name, scene_seg, self.cropSize)
            dets[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSize, self.dets_nClasses, self.labels_to_index)
            dets_obsv[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSizeObsv, 1, self.labels_to_index)

        # 计算相对位姿
        rel_poses = dh.relative_poses(poses=abs_poses)

        # 填充原始轨迹item
        item_original["images"] = torch.from_numpy(imgs).float()
        item_original["images_names"] = im_path
        item_original["points2D"] = points2D
        item_original["local3D"] = local3D
        item_original["actions"] = action_path
        item_original["costs"] = torch.from_numpy(cost_path).float()
        item_original["target_lbl"] = target_lbl
        item_original["pose"] = rel_poses
        item_original["abs_pose"] = abs_poses
        item_original["collisions"] = torch.from_numpy(collision_seq).float()
        item_original["scene"] = scene
        item_original["scale"] = scale
        item_original['sseg'] = torch.from_numpy(ssegs).float()
        item_original['dets'] = torch.from_numpy(dets).float()
        item_original['images_obsv'] = torch.from_numpy(imgs_obsv).float()
        item_original['dets_obsv'] = torch.from_numpy(dets_obsv).float()
        
        # 处理扰动轨迹
        perturbed_imgs = np.zeros((self.seq_len, 3, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_imgs_obsv = np.zeros((self.seq_len, 3, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        perturbed_ssegs = np.zeros((self.seq_len, 1, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_dets = np.zeros((self.seq_len, self.dets_nClasses, self.cropSize[1], self.cropSize[0]), dtype=np.float32)
        perturbed_dets_obsv = np.zeros((self.seq_len, 1, self.cropSizeObsv[1], self.cropSizeObsv[0]), dtype=np.float32)
        perturbed_points2D, perturbed_local3D = [], []
        
        for i in range(len(perturbed_im_path)):
            im_name = perturbed_im_path[i]
            imgData, points2D_step, local3D_step = dh.getImageData(self.datasetPath, im_name, 
                                                scene, self.cropSize, self.orig_res, self.pixFormat, self.normalize)
            perturbed_imgs[i,:,:,:] = imgData
            perturbed_points2D.append(points2D_step)
            perturbed_local3D.append(local3D_step)
            perturbed_imgs_obsv[i,:,:,:] = dh.getImageData(self.datasetPath, im_name, scene, self.cropSizeObsv, 
                                                    self.orig_res, self.pixFormat, self.normalize, get3d=False)
            # 加载语义分割、检测掩码和以自我为中心的检测观察
            perturbed_ssegs[i,:,:,:] = dh.get_sseg(im_name, scene_seg, self.cropSize)
            perturbed_dets[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSize, self.dets_nClasses, self.labels_to_index)
            perturbed_dets_obsv[i,:,:,:] = dh.get_det_mask(im_name, scene_dets, self.cropSizeObsv, 1, self.labels_to_index)

        # 计算扰动轨迹的相对位姿
        perturbed_rel_poses = dh.relative_poses(poses=perturbed_abs_poses)
        
        # 填充扰动轨迹item
        item_perturbed["images"] = torch.from_numpy(perturbed_imgs).float()
        item_perturbed["images_names"] = perturbed_im_path
        item_perturbed["points2D"] = perturbed_points2D
        item_perturbed["local3D"] = perturbed_local3D
        item_perturbed["actions"] = perturbed_action_path
        item_perturbed["costs"] = torch.from_numpy(perturbed_cost_path).float()
        item_perturbed["target_lbl"] = target_lbl
        item_perturbed["pose"] = perturbed_rel_poses
        item_perturbed["abs_pose"] = perturbed_abs_poses
        item_perturbed["collisions"] = torch.from_numpy(perturbed_collision_seq).float()
        item_perturbed["scene"] = scene
        item_perturbed["scale"] = scale
        item_perturbed['sseg'] = torch.from_numpy(perturbed_ssegs).float()
        item_perturbed['dets'] = torch.from_numpy(perturbed_dets).float()
        item_perturbed['images_obsv'] = torch.from_numpy(perturbed_imgs_obsv).float()
        item_perturbed['dets_obsv'] = torch.from_numpy(perturbed_dets_obsv).float()
        
        # 组合信息
        contrast_info = {
            "perturb_point": perturb_point,
            "is_positive": is_positive,
            "similarity_score": similarity_score
        }
        
        return {
            "original": item_original,
            "perturbed": item_perturbed,
            "contrast_info": contrast_info
        }

</pre>     
        </div>
      </div>
    </div>
    <!-- code6.py -->
    <div class="col-12 col-md-6 mb-4">
      <div class="card">
        <div class="card-header">test_AOS.py</div>
        <div class="card-body" style="max-height:400px; overflow:auto; font-family:monospace; background:#f8f9fa;">
<pre>
import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import numpy as np
import random
from dataloader import AVD_online
from mapNet import MapNet
from IL_Net import Encoder
from parameters import ParametersMapNet, Parameters_IL
import helper as hl
import data_helper as dh
import networkx as nx
import pickle
def softmax(x):
	scoreMatExp = np.exp(np.asarray(x))
	return scoreMatExp / scoreMatExp.sum(0)


def prepare_mapNet_input(ex):
    img = ex["image"]
    points2D = ex["points2D"]
    local3D = ex["local3D"]
    sseg = ex["sseg"]
    dets = ex['dets']
    # test_batch_size=1
    imgs_batch = img.unsqueeze(0)
    sseg_batch = sseg.unsqueeze(0)
    dets_batch = dets.unsqueeze(0)
    points2D_batch, local3D_batch = [], [] # add another dimension for the batch
    points2D_batch.append(points2D)
    local3D_batch.append(local3D)
    return (imgs_batch.cuda(), points2D_batch, local3D_batch, sseg_batch.cuda(), dets_batch.cuda())


def evaluate_NavNet(parIL, parMapNet, mapNet, ego_encoder, test_iter, test_ids, test_data, action_list):
    print("\nRunning validation on NavNet!")
    with torch.no_grad():
        policy_net = hl.load_model(model_dir=parIL.model_dir, model_name="ILNet", test_iter=test_iter)
        acc, epi_length, path_ratio = 0, 0, 0
        spl_sum, apl_sum = 0, 0  
        episode_results, episode_count = {}, 0 # store predictions
        for i in test_ids:
            test_ex = test_data[i]
            # Get all info for the starting position
            mapNet_input_start = prepare_mapNet_input(ex=test_ex)
            target_lbl = test_ex["target_lbl"]
            im_obsv = test_ex['image_obsv'].cuda()
            dets_obsv = test_ex['dets_obsv'].cuda()
            tvec = torch.zeros(1, parIL.nTargets).float().cuda()
            tvec[0,target_lbl] = 1
            # We need to keep other info to allow us to do the steps later
            image_name, scene, scale = [], [], []
            image_name.append(test_ex['image_name'])
            scene.append(test_ex['scene'])
            scale.append(test_ex['scale'])
            shortest_path_length = test_ex['path_length']

            if parIL.use_p_gt:
                # get the ground-truth pose, which is the relative pose with respect to the first image
                info, annotations, _ = dh.load_scene_info(parIL.avd_root, scene[0])
                im_names_all = info['image_name'] # info 0 # list of image names in the scene
                im_names_all = np.hstack(im_names_all) # flatten the array
                start_abs_pose = dh.get_image_poses(info, im_names_all, image_name, scale[0]) # init pose of the episode # 1 x 3  

            
            # p_, confidence_, map_ = mapNet.forward_single_step(local_info=mapNet_input_start, t=0, 
            #                                         input_flags=parMapNet.input_flags, update_type=parMapNet.update_type)
            p_, map_ = mapNet.forward_single_step(local_info=mapNet_input_start, t=0, 
                                                    input_flags=parMapNet.input_flags, update_type=parMapNet.update_type)
            collision_ = torch.tensor([0], dtype=torch.float32).cuda() # collision indicator is 0
            if parIL.use_ego_obsv:
                enc_in = torch.cat((im_obsv, dets_obsv), 0).unsqueeze(0)
                ego_obsv_feat = ego_encoder(enc_in) # 1 x 512 x 1 x 1
                state = (map_, p_, tvec, collision_, ego_obsv_feat)
            else:
                state = (map_, p_, tvec, collision_) 
            current_im = image_name[0]

            done=0
            image_seq, action_seq = [], []
            image_seq.append(current_im)
            policy_net.hidden = policy_net.init_hidden(batch_size=1, state_items=len(state)-1)
            for t in range(1, parIL.max_steps+1):
                pred_costs = policy_net(state, parIL.use_ego_obsv) # apply policy for single step
                pred_costs = pred_costs.view(-1).cpu().numpy()
                # choose the action with a certain prob
                pred_probs = softmax(-pred_costs)
                pred_label = np.random.choice(len(action_list), 1, p=pred_probs)[0]
                pred_action = action_list[pred_label]

                # get the next image, check collision and goal
                next_im = test_data.scene_annotations[scene[0]][current_im][pred_action]
                if next_im=='':
                    image_seq.append(current_im)
                else:
                    image_seq.append(next_im)
                action_seq.append(pred_action)
                #print(t, current_im, pred_action, next_im)#会在终端显示输出信息
                if not(next_im==''): # not collision case
                    collision = 0
                    # check for goal
                    path_dist = len(nx.shortest_path(test_data.graphs_dict[target_lbl][scene[0]], next_im, "goal")) - 2
                    if path_dist <= parIL.steps_from_goal: # GOAL!
                        acc += 1
                        epi_length += t
                        path_ratio += t/float(shortest_path_length) # ratio of estimated path towards shortest path
                        # 计算SPL和APL指标
                        spl_sum += shortest_path_length / max(t, shortest_path_length)  # SPL累积
                        apl_sum += t  # APL累积（成功场景的路径长度）
                        done=1
                        break
                    # get next state from mapNet
                    batch_next, obsv_batch_next = test_data.get_step_data(next_ims=[next_im], scenes=scene, scales=scale)
                    if parIL.use_p_gt:
                        next_im_abs_pose = dh.get_image_poses(info, im_names_all, [next_im], scale[0])
                        abs_poses = np.concatenate((start_abs_pose, next_im_abs_pose), axis=0)
                        rel_poses = dh.relative_poses(poses=abs_poses)
                        next_im_rel_pose = np.expand_dims(rel_poses[1,:], axis=0)
                        p_gt = dh.build_p_gt(parMapNet, pose_gt_batch=np.expand_dims(next_im_rel_pose, axis=1)).squeeze(1)
                        # p_next, confidence_, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, input_flags=parMapNet.input_flags,
                        #                                         map_previous=state[0], p_given=p_gt, update_type=parMapNet.update_type)
                        p_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, input_flags=parMapNet.input_flags,
                                                                map_previous=state[0], p_given=p_gt, update_type=parMapNet.update_type)
                    else:
                        # p_next, confidence_, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, 
                        #                     input_flags=parMapNet.input_flags, map_previous=state[0], update_type=parMapNet.update_type)
                        p_next, map_next = mapNet.forward_single_step(local_info=batch_next, t=t, 
                                            input_flags=parMapNet.input_flags, map_previous=state[0], update_type=parMapNet.update_type)
                    if parIL.use_ego_obsv:
                        enc_in = torch.cat(obsv_batch_next, 1)
                        ego_obsv_feat = ego_encoder(enc_in) # b x 512 x 1 x 1
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda(), ego_obsv_feat)
                    else:
                        state = (map_next, p_next, tvec, torch.tensor([collision], dtype=torch.float32).cuda())
                    current_im = next_im

                else: # collision case
                    collision = 1
                    if parIL.stop_on_collision:
                        break
                    if parIL.use_ego_obsv:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda(), state[4])
                    else:
                        state = (state[0], state[1], state[2], torch.tensor([collision], dtype=torch.float32).cuda())
                
            episode_results[episode_count] = (image_seq, action_seq, parIL.lbl_to_cat[target_lbl], done)
            episode_count+=1
        # store the episodes
        #episode_results_path = parIL.model_dir+'episode_results_eval_'+str(test_iter)+'.pkl'
        scene_name_for_file = "_".join(parIL.predefined_test_scenes)
        episode_results_path = parIL.model_dir+f'episode_results_eval_{scene_name_for_file}_{test_iter}.pkl'
        
        with open(episode_results_path, 'wb') as f:
            pickle.dump(episode_results, f)
        
        success_rate = acc / float(len(test_ids))
        if acc > 0:
            mean_epi_length = epi_length / float(acc)
            avg_path_length_ratio = path_ratio / float(acc)
            apl = apl_sum / float(acc)  # 平均路径长度：APL = 成功场景总路径长度 / 成功场景数
        else:
            mean_epi_length = 0
            avg_path_length_ratio = 0
            apl = 0
        
        spl = spl_sum / float(len(test_ids))  # 路径长度加权成功率：SPL = (所有场景的成功加权值之和) / 总场景数
        
        print("Test iter:", test_iter, "Success rate:", success_rate)
        print("Mean epi length:", mean_epi_length, "Avg path length ratio:", avg_path_length_ratio)
        print("SPL (Success weighted by Path Length):", spl)
        print("APL (Average Path Length):", apl)



if __name__ == '__main__':
    parMapNet = ParametersMapNet()
    parIL = Parameters_IL()
    action_list = np.asarray(parMapNet.action_list)

    if parIL.use_predefined_test_set:#True
      
        avd = AVD_online(par=parIL, nStartPos=0, scene_list=parIL.predefined_test_scenes, 
                                                action_list=action_list, init_configs=parIL.predefined_confs_file)
    else:
        # 从 AVD_online 类中随机抽样起始位置和目标
        avd = AVD_online(par=parIL, nStartPos=10, scene_list=["Home_001_1"], action_list=action_list)

    test_ids = list(range(len(avd)))

    # 需要加载经过训练的 MapNet
    if parIL.finetune_mapNet: # 选择是否使用微调的 mapNet 模型
        mapNet_model = hl.load_model(model_dir=parIL.model_dir, model_name="MapNet", test_iter=parIL.test_iters)
        print(f"加载的MapNet模型路径: {parIL.model_dir}, 迭代次数: {parIL.test_iters}")
    else:
        mapNet_model = hl.load_model(model_dir=parIL.mapNet_model_dir, model_name="MapNet", test_iter=parIL.mapNet_iters)
        print(f"加载的MapNet模型路径: {parIL.mapNet_model_dir}, 迭代次数: {parIL.mapNet_iters}")
    
    if parIL.use_ego_obsv:
        ego_encoder = Encoder()
        ego_encoder.cuda()
        ego_encoder.eval()
    else:
        ego_encoder = None

    evaluate_NavNet(parIL, parMapNet, mapNet_model, ego_encoder, test_iter=parIL.test_iters, 
                                            test_ids=test_ids, test_data=avd, action_list=action_list)

</pre>     
        </div>
      </div>
    </div> 
          
</section>

  <!-- 页脚 -->
  <footer class="bg-light text-center py-3">
    &copy; 2025 Yuhao Wang<br>
    This website is licensed under a
    <a rel="license"
       href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
  </footer>
</body>
</html>
















